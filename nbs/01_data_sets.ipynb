{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data sets\n",
    "Parsing and discovery, loading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp data_sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "#| hide \n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *\n",
    "from fastcore.basics import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import os\n",
    "import re\n",
    "import logging\n",
    "import warnings\n",
    "import numpy as np\n",
    "import polars as pl\n",
    "from pathlib import Path\n",
    "from copy import deepcopy\n",
    "from enum import Enum, auto\n",
    "from functools import partial\n",
    "from collections import defaultdict\n",
    "from typing import Dict, List, Tuple\n",
    "from pisces.mads_olsen_support import *\n",
    "from typing import DefaultDict, Iterable\n",
    "from scipy.ndimage import gaussian_filter1d\n",
    "from pisces.utils import determine_header_rows_and_delimiter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data set discovery using Prefix Trees\n",
    "\n",
    "Data sets are discovered based on being folders within the provided data set root directory which contain subdirectories that start with `cleaned_`.  \n",
    "\n",
    "Once the data sets are discovered, we take the `cleaned_<feature>` subdirectories and use the `<feature>` as the feature name. \n",
    "\n",
    "Then we take the files within the `cleaned_<feature>` subdirectories and discover the ids that data set has for that feature. These do not need to be the same across features, hence all of our data getters might also return `None`.\n",
    "\n",
    "Automagic ID discovery is done using a prefix tree, which is a data structure that allows for efficient searching of strings based on their prefixes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class SimplifiablePrefixTree:\n",
    "    \"\"\"\n",
    "    A standard prefix tree with the ability to \"simplify\" itself by combining nodes with only one child.\n",
    "    These also have the ability to \"flatten\" themselves, which means to convert all nodes at and below a certain depth into leaves on the most recent ancestor of that depth.\n",
    "    \"\"\"\n",
    "    def __init__(self, delimiter: str = \"\", # The delimiter to use when splitting words into characters. If empty, the words are treated as sequences of characters.\n",
    "                 key: str = \"\", # The key of the current node in its parent's `.children` dictionary. If empty, the node is (likely) the root of the tree.\n",
    "                 ):\n",
    "        \"\"\"\n",
    "        key : str\n",
    "            The key of the current node in its parent's `.children` dictionary. If empty, the node is (likely) the root of the tree.\n",
    "        children : Dict[str, SimplifiablePrefixTree]\n",
    "            The children of the current node, stored in a dictionary with the keys being the children's keys.\n",
    "        is_end_of_word : bool\n",
    "            Whether the current node is the end of a word. Basically, is this a leaf node?\n",
    "        delimiter : str\n",
    "            The delimiter to use when splitting words into characters. If empty, the words are treated as sequences of characters.\n",
    "        print_spacer : str\n",
    "            The string to use to indent the printed tree.\n",
    "        \"\"\"\n",
    "        self.key = key\n",
    "        self.children: Dict[str, SimplifiablePrefixTree] = {}\n",
    "        self.is_end_of_word = False\n",
    "        self.delimiter = delimiter\n",
    "        self.print_spacer = \"++\"\n",
    "    \n",
    "    def chars_from(self, word: str):\n",
    "        \"\"\"\n",
    "        Splits a word into characters, using the `delimiter` attribute as the delimiter.\n",
    "        \"\"\"\n",
    "        return word.split(self.delimiter) if self.delimiter else word\n",
    "\n",
    "    def insert(self, word: str):\n",
    "        \"\"\"\n",
    "        Inserts a word into the tree. If the word is already in the tree, nothing happens.\n",
    "        \"\"\"\n",
    "        node = self\n",
    "        for char in self.chars_from(word):\n",
    "            if char not in node.children:\n",
    "                node.children[char] = SimplifiablePrefixTree(self.delimiter, key=char)\n",
    "            node = node.children[char]\n",
    "        node.is_end_of_word = True\n",
    "\n",
    "    def search(self, word: str) -> bool:\n",
    "        \"\"\"\n",
    "        Searches for a word in the tree.\n",
    "        \"\"\"\n",
    "        node = self\n",
    "        for char in self.chars_from(word):\n",
    "            if char not in node.children:\n",
    "                return False\n",
    "            node = node.children[char]\n",
    "        return node.is_end_of_word\n",
    "    \n",
    "    def simplified(self) -> 'SimplifiablePrefixTree':\n",
    "        \"\"\"\n",
    "        Returns a simplified copy of the tree. The original tree is not modified.\n",
    "        \"\"\"\n",
    "        self_copy = deepcopy(self)\n",
    "        return self_copy.simplify()\n",
    "    \n",
    "    def simplify(self):\n",
    "        \"\"\"\n",
    "        Simplifies the tree in place.\n",
    "        \"\"\"\n",
    "        if len(self.children) == 1 and not self.is_end_of_word:\n",
    "            child_key = list(self.children.keys())[0]\n",
    "            self.key += child_key\n",
    "            self.children = self.children[child_key].children\n",
    "            self.simplify()\n",
    "        else:\n",
    "            current_keys = list(self.children.keys())\n",
    "            for key in current_keys:\n",
    "                child = self.children.pop(key)\n",
    "                child.simplify()\n",
    "                self.children[child.key] = child\n",
    "        return self\n",
    "    \n",
    "    def reversed(self) -> 'SimplifiablePrefixTree':\n",
    "        \"\"\"\n",
    "        Returns a reversed copy of the tree, except with with `node.key` reversed versus the node in `self.children`. The original tree is not modified.\n",
    "        \"\"\"\n",
    "        rev_self = SimplifiablePrefixTree(self.delimiter, key=self.key[::-1])\n",
    "        rev_self.children = {k[::-1]: v.reversed() for k, v in self.children.items()}\n",
    "        return rev_self\n",
    "    \n",
    "    def flattened(self, max_depth: int = 1) -> 'SimplifiablePrefixTree':\n",
    "        \"\"\"\n",
    "        Returns a Tree identical to `self` up to the given depth, but with all nodes at + below `max_depth` converted into leaves on the most recent ancestor of depth `max_depth - 1`.\n",
    "        \"\"\"\n",
    "        flat_self = SimplifiablePrefixTree(self.delimiter, key=self.key)\n",
    "        if max_depth == 0:\n",
    "            if not self.is_end_of_word:\n",
    "                warnings.warn(f\"max_depth is 0, but {self.key} is not a leaf.\")\n",
    "            return flat_self\n",
    "        if max_depth == 1:\n",
    "            for k, v in self.children.items():\n",
    "                if v.is_end_of_word:\n",
    "                    flat_self.children[k] = SimplifiablePrefixTree(self.delimiter, key=k)\n",
    "                else:\n",
    "                    # flattened_children = v._pushdown()\n",
    "                    for flattened_child in v._pushdown():\n",
    "                        flat_self.children[flattened_child.key] = flattened_child\n",
    "        else:\n",
    "            for k, v in self.children.items():\n",
    "                flat_self.children[k] = v.flattened(max_depth - 1)\n",
    "        return flat_self\n",
    "    \n",
    "    def _pushdown(self) -> List['SimplifiablePrefixTree']:\n",
    "        \"\"\"\n",
    "        Returns a list corresponding to the children of `self`, with `self.key` prefixed to each child's key.\n",
    "        \"\"\"\n",
    "        pushed_down = [\n",
    "            c\n",
    "            for k in self.children.values()\n",
    "            for c in k._pushdown()\n",
    "        ]\n",
    "        for i in range(len(pushed_down)):\n",
    "            pushed_down[i].key = self.key + self.delimiter + pushed_down[i].key\n",
    "\n",
    "        if not pushed_down:\n",
    "            return [SimplifiablePrefixTree(self.delimiter, key=self.key)]\n",
    "        else:\n",
    "            return pushed_down\n",
    "            \n",
    "\n",
    "    def __str__(self):\n",
    "        # prints .children recursively with indentation\n",
    "        return self.key + \"\\n\" + self.print_tree()\n",
    "\n",
    "    def print_tree(self, indent=0) -> str:\n",
    "        result = \"\"\n",
    "        for key, child in self.children.items():\n",
    "            result +=  self.print_spacer * indent + \"( \" + child.key + \"\\n\"\n",
    "            result += SimplifiablePrefixTree.print_tree(child, indent + 1)\n",
    "        return result\n",
    "\n",
    "\n",
    "class IdExtractor(SimplifiablePrefixTree):\n",
    "    \"\"\"\n",
    "    Class extending the prefix trees that incorporates the algorithm for extracting IDs from a list of file names. The algorithm is somewhat oblique, so it's better to just use the `extract_ids` method versus trying to use the prfix trees directly at the call site.\n",
    "    \n",
    "    The algorithm is based on the assumption that the IDs are the same across all file names, but that the file names may have different suffixes. The algorithm reverses the file names, inserts them into the tree, and then simplifes and flattens that tree in order to find the IDs as leaves of that simplified tree.\n",
    "\n",
    "    1. Insert the file name string into the tree, but with each string **reversed**.\n",
    "    2. Simplify the tree, combining nodes with only one child.\n",
    "    3. There may be unexpected suffix matches for these IDs, so we flatten the tree to depth 1, meaning all children of the root are combined to make leaves.\n",
    "    4. The leaves are the IDs we want to extract. However, we must reverse these leaf keys to get the original IDs, since we reversed the file names in step 1.\n",
    "\n",
    "    TODO:\n",
    "    * If we want to find IDs for files with differing prefixes instead, we should instead insert the file names NOT reversed and then NOT reverse in the last step.\n",
    "\n",
    "    * To handle IDs that appear in the middle of file names, we can use both methods to come up with a list of potential IDs based on prefix and suffix, then figure out the \"intersection\" of those lists. (Maybe using another prefix tree?)\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, delimiter: str = \"\", key: str = \"\"):\n",
    "        super().__init__(delimiter, key)\n",
    "\n",
    "    def extract_ids(self, \n",
    "                    files: List[str], \n",
    "                    id_template: str | None,\n",
    "                    id_symbol: str) -> List[str]:\n",
    "        \"\"\"\n",
    "        Extracts IDs from a list of file names. If an ID template is provided, the algorithm will use that to extract the IDs. If not, the algorithm will extract the IDs based on the assumption that the files share a common structure.\n",
    "        When providing an ID template, do it as follows: `\"prefix\" + id_symbol + \"suffix\"`. Either the prefix or suffix can be empty, but the `id_symbol` part must be present.\n",
    "        An id_symbol is used to separate the prefix and suffix from the ID. \n",
    "        \"\"\"\n",
    "        if len(files) == 0:\n",
    "            raise ValueError(\"Please provide at least one file name to extract IDs\")\n",
    "\n",
    "        if len(files) == 1:\n",
    "            if not id_template:\n",
    "                raise ValueError(\"Please provide an ID template if you only have one file name.\")\n",
    "            else:\n",
    "                file = files[0]\n",
    "                prefix, suffix = id_template.split(id_symbol)\n",
    "                id_str = file.replace(prefix, \"\").replace(suffix, \"\")\n",
    "                return [id_str]\n",
    "\n",
    "        \n",
    "        if not id_template:\n",
    "            for file in files:\n",
    "                self.insert(file[::-1])\n",
    "            return sorted([\n",
    "                c.key for c in self\n",
    "                    .prefix_flattened()\n",
    "                    .children\n",
    "                    .values()\n",
    "            ])\n",
    "        else:\n",
    "            ids = []\n",
    "            for file in files:\n",
    "                prefix, suffix = id_template.split(id_symbol)\n",
    "                id_str = file.replace(prefix, \"\").replace(suffix, \"\")\n",
    "                ids.append(id_str)\n",
    "            \n",
    "            return sorted(ids)\n",
    "    \n",
    "    def prefix_flattened(self) -> 'IdExtractor':\n",
    "        return self.simplified().flattened(1).reversed()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "entries = [\n",
    "    '3XYZabc12',\n",
    "]\n",
    "\n",
    "expected_ids = sorted([\n",
    "    '3XYZ',\n",
    "])\n",
    "\n",
    "id_extractor = IdExtractor()\n",
    "ids = id_extractor.extract_ids(entries, id_template=\"<<ID>>abc12\", id_symbol=\"<<ID>>\")\n",
    "assert ids == expected_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "entries = [\n",
    "    '3XYZabc12',\n",
    "]\n",
    "\n",
    "expected_ids = sorted([\n",
    "    '3XYZ',\n",
    "])\n",
    "\n",
    "id_extractor = IdExtractor()\n",
    "ids = id_extractor.extract_ids(entries, id_template=\"ID_HEREabc12\", id_symbol=\"ID_HERE\")\n",
    "\n",
    "assert ids == expected_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "entries = [\n",
    "    '3XYZabc12',\n",
    "    '3XY&abc12',\n",
    "    '3XYAabc12',\n",
    "    '3XYBabc12',\n",
    "    'MMVQabc12',\n",
    "    'NMVQabc12',\n",
    "]\n",
    "\n",
    "expected_ids = sorted([\n",
    "    '3XYZ',\n",
    "    '3XY&',\n",
    "    '3XYA',\n",
    "    '3XYB',\n",
    "    'MMVQ',\n",
    "    'NMVQ',\n",
    "])\n",
    "\n",
    "id_extractor = IdExtractor()\n",
    "ids = id_extractor.extract_ids(entries, id_template=\"<<ID>>abc12\", id_symbol=\"<<ID>>\")\n",
    "for i, (expected, actual) in enumerate(zip(expected_ids, ids)):\n",
    "    assert expected == actual, f\"Expected {expected}, but got {actual} at index {i}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#| hide\n",
    "# DataSetObject"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "LOG_LEVEL = logging.INFO\n",
    "\n",
    "class DataSetObject:\n",
    "    FEATURE_PREFIX = \"cleaned_\"\n",
    "\n",
    "    # Set up logging\n",
    "    logger = logging.getLogger(__name__)\n",
    "    logger.setLevel(LOG_LEVEL)\n",
    "    formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "    console_handler = logging.StreamHandler()\n",
    "    console_handler.setFormatter(formatter)\n",
    "    logger.addHandler(console_handler)\n",
    "\n",
    "    def __init__(self, name: str, path: Path):\n",
    "        self.name = name\n",
    "        self.path = path\n",
    "        self.ids: List[str] = []\n",
    "\n",
    "        # keeps track of the files for each feature and user\n",
    "        self._feature_map: DefaultDict[str, Dict[str, str]] = defaultdict(dict)\n",
    "        self._feature_cache: DefaultDict[str, Dict[str, pl.DataFrame]] = defaultdict(dict)\n",
    "    \n",
    "    @property\n",
    "    def features(self) -> List[str]:\n",
    "        return list(self._feature_map.keys())\n",
    "    \n",
    "    def __str__(self):\n",
    "        return f\"{self.name}: {self.path}\"\n",
    "\n",
    "    def get_feature_data(self, feature: str, id: str) -> pl.DataFrame | None:\n",
    "        if feature not in self.features:\n",
    "            warnings.warn(f\"Feature {feature} not found in {self.name}. Returning None.\")\n",
    "            return None\n",
    "        if id not in self.ids:\n",
    "            warnings.warn(f\"ID {id} not found in {self.name}\")\n",
    "            return None\n",
    "        if (df := self._feature_cache[feature].get(id)) is None:\n",
    "            file = self.get_filename(feature, id)\n",
    "            if not file:\n",
    "                return None\n",
    "            self.logger.debug(f\"Loading {file}\")\n",
    "            try:\n",
    "                n_rows, delimiter = determine_header_rows_and_delimiter(file)\n",
    "                # self.logger.debug(f\"n_rows: {n_rows}, delimiter: {delimiter}\")\n",
    "                df = pl.read_csv(file, has_header=True if n_rows > 0 else False,\n",
    "                                 skip_rows=max(n_rows-1, 0), \n",
    "                                 separator=delimiter)\n",
    "            except Exception as e:\n",
    "                warnings.warn(f\"Error reading {file}:\\n{e}\")\n",
    "                return None\n",
    "            # sort by time when loading\n",
    "            df.sort(df.columns[0])\n",
    "            self._feature_cache[feature][id] = df\n",
    "        return df\n",
    "\n",
    "    def get_filename(self, feature: str, id: str) -> Path | None:\n",
    "        feature_ids = self._feature_map.get(feature)\n",
    "        if feature_ids is None:\n",
    "            # raise ValueError(f\"Feature {feature_ids} not found in {self.name}\")\n",
    "            print(f\"Feature {feature_ids} not found in {self.name}\")\n",
    "            return None\n",
    "        file = feature_ids.get(id)\n",
    "        if file is None:\n",
    "            # raise ValueError\n",
    "            print(f\"ID {id} not found in {self.name}\")\n",
    "            return None\n",
    "        return self.get_feature_path(feature)\\\n",
    "            .joinpath(file)\n",
    "    \n",
    "    def get_feature_path(self, feature: str) -> Path:\n",
    "        return self.path.joinpath(self.FEATURE_PREFIX + feature)\n",
    "    \n",
    "    def _extract_ids(self, files: List[str],\n",
    "                     id_template: str | None,\n",
    "                     id_symbol: str) -> List[str]:\n",
    "        return IdExtractor().extract_ids(files, id_template, id_symbol)\n",
    "    \n",
    "    def add_feature_files(self, feature: str, files: Iterable[str],\n",
    "                          id_template: str | None, id_symbol: str):\n",
    "        if feature not in self.features:\n",
    "            self.logger.debug(f\"Adding feature {feature} to {self.name}\")\n",
    "            self._feature_map[feature] = {}\n",
    "        # use a set for automatic deduping\n",
    "        deduped_ids = set(self.ids)\n",
    "        extracted_ids = sorted(self._extract_ids(files, id_template, id_symbol))\n",
    "        files = sorted(list(files))\n",
    "        # print('# extracted_ids:', len(extracted_ids))\n",
    "        for id, file in zip(extracted_ids, files):\n",
    "            # print('adding data for id:', id, 'file:', file)\n",
    "            self._feature_map[feature][id] = file\n",
    "            # set.add only adds the value if it's not already in the set\n",
    "            deduped_ids.add(id)\n",
    "        self.ids = sorted(list(deduped_ids))\n",
    "    \n",
    "    def get_feature_files(self, feature: str) -> Dict[str, str]:\n",
    "        return {k: v for k, v in self._feature_map[feature].items()}\n",
    "    \n",
    "    def get_id_files(self, id: str) -> Dict[str, str]:\n",
    "        return {k: v[id] for k, v in self._feature_map.items()}\n",
    "    \n",
    "    def load_feature_data(self, feature: str | None, id: str | None) -> Dict[str, np.ndarray]:\n",
    "        if feature not in self.features:\n",
    "            raise ValueError(f\"Feature {feature} not found in {self.name}\")\n",
    "    \n",
    "    @classmethod\n",
    "    def find_data_sets(cls, \n",
    "                       root: str | Path,\n",
    "                       ) -> Dict[str, 'DataSetObject']:\n",
    "        root = str(root).replace(\"\\\\\", \"/\") # Use consistent separators\n",
    "\n",
    "        feature_dir_regex = rf\".*/(.+)/{cls.FEATURE_PREFIX}(.+)/?\"\n",
    "\n",
    "        data_sets: Dict[str, DataSetObject] = {}\n",
    "        for root_dir, dirs, files in os.walk(root, followlinks=True):\n",
    "            normalized_root_dir = root_dir.replace(\"\\\\\", \"/\")\n",
    "            if (root_match := re.match(feature_dir_regex, normalized_root_dir)):\n",
    "                data_set_name = root_match.group(1)\n",
    "                feature_name = root_match.group(2)\n",
    "                if (data_set := data_sets.get(data_set_name)) is None:\n",
    "                    data_set = DataSetObject(data_set_name, Path(root_dir).parent)\n",
    "                    data_set._feature_map[feature_name] = {}\n",
    "                    data_sets[data_set.name] = data_set\n",
    "                else:\n",
    "                    data_sets[data_set_name]._feature_map[feature_name] = {}\n",
    "        return data_sets\n",
    "\n",
    "    def parse_data_sets(self, \n",
    "                        ignore_startswith: List=[\".\"], # Ignore files starting with these strings \n",
    "                        ignore_endswith: List=[\".tmp\"], # Ignore files ending with these strings \n",
    "                        id_templates: Dict[str, str] | str | None=None, # The template for extracting IDs from the file names. A template per feature can be provided as a dictionary \n",
    "                        id_symbol: str=\"<<ID>>\",\n",
    "                        ):\n",
    "        for feature in self.features:\n",
    "            feature_path = self.get_feature_path(feature)\n",
    "            if not feature_path.exists():\n",
    "                warnings.warn(f\"Feature path {feature_path} not found.\")\n",
    "                continue\n",
    "            files = [f.name for f in feature_path.iterdir() if f.is_file()]\n",
    "            relevant_files = []\n",
    "            for f in files:\n",
    "                ignore_start = any(f.startswith(prefix) for prefix in ignore_startswith)\n",
    "                ignore_end = any(f.endswith(suffix) for suffix in ignore_endswith)\n",
    "                if ignore_start or ignore_end:\n",
    "                    continue\n",
    "                relevant_files.append(f)\n",
    "            if isinstance(id_templates, dict):\n",
    "                id_template = id_templates[feature]\n",
    "            else:\n",
    "                id_template = id_templates\n",
    "            self.add_feature_files(feature, relevant_files, id_template, id_symbol)\n",
    "\n",
    "    def find_overlapping_time_section(\n",
    "        self,\n",
    "        features: List[str], # List of features included in the calculation, typically a combination of input and output features\n",
    "        id: str, # Subject id to process\n",
    "        ) -> Tuple[int, int]:\n",
    "        '''\n",
    "        Find common time interval when there's data for all features\n",
    "        '''\n",
    "        max_start = None\n",
    "        min_end = None\n",
    "        for feature in features:\n",
    "            data = self.get_feature_data(feature, id)\n",
    "            time = data[:, 0]\n",
    "            if max_start is None:\n",
    "                max_start = time.min()\n",
    "            else:\n",
    "                max_start = max([max_start, time.min()])\n",
    "            if min_end is None:\n",
    "                min_end = time.max()\n",
    "            else:\n",
    "                min_end = min([min_end, time.max()])\n",
    "        return (max_start, min_end)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#| hide\n",
    "# Handle labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def psg_to_sleep_wake(psg: pl.DataFrame) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    * map all positive classes to 1 (sleep)\n",
    "    * retain all 0 (wake) and -1 (mask) classes\n",
    "    \"\"\"\n",
    "    return np.where(psg[:, 1] > 0, 1, psg[:, 1])\n",
    "\n",
    "def to_WLDM(x: float, N4: bool=True) -> int:\n",
    "    \"\"\"\n",
    "    Map sleep stages to wake, light, deep, and REM sleep.\n",
    "    Retain masked values. If N4 stage is not present,\n",
    "    PSG=4 is mapped to REM. Otherwise it is mapped to deep sleep.\n",
    "    \"\"\"\n",
    "    if x < 0:\n",
    "        return -1\n",
    "    if x == 0:\n",
    "        return 0\n",
    "    if x < 3:\n",
    "        return 1\n",
    "    rem_value = 5 if N4 else 4\n",
    "    if x < rem_value:\n",
    "        return 2\n",
    "    return 3\n",
    "\n",
    "vec_to_WLDM = np.vectorize(to_WLDM)\n",
    "\n",
    "def psg_to_WLDM(psg: pl.DataFrame, N4: bool = True) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    * map all positive classes as follows:\n",
    "    If N4 is True:\n",
    "        - 1, 2 => 1 (light sleep)\n",
    "        - 3, 4 => 2 (deep sleep)\n",
    "        - 5 => 3 (REM)\n",
    "    If N4 is False:\n",
    "        - 1, 2 => 1 (light sleep)\n",
    "        - 3 => 2 (deep sleep)\n",
    "        - 5 => 3 (REM)\n",
    "    * retain all 0 (wake) and -1 (mask) classes\n",
    "    \"\"\"\n",
    "    return vec_to_WLDM(psg[:, 1].to_numpy(), N4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# Test find_data_sets\n",
    "## Create a temporary directory with some files\n",
    "## The directory structure should be as follows:\n",
    "## root\n",
    "## ├── data_set_1\n",
    "## │   ├── cleaned_feature_1\n",
    "## │   │   ├── id_001_feat1.csv\n",
    "## │   │   ├── id_002_feat1.csv\n",
    "## │   │   ├── id_003_feat1.csv\n",
    "## │   │   ├── analyze_data.py\n",
    "## │   │   └── _preliminary_analysis.csv\n",
    "## │   └── cleaned_feature_2\n",
    "## │       ├── id_001_feat2.csv\n",
    "## │       ├── id_002_feat2.csv\n",
    "## │       ├── id_003_feat2.csv\n",
    "## │       ├── analyze_data_another_way.py\n",
    "## │       └── .other_data.csv\n",
    "## └── data_set_2\n",
    "##     ├── cleaned_feature_3\n",
    "##     │   ├── id_001_feat3.csv\n",
    "##     │   ├── id_002_feat3.csv\n",
    "##     │   ├── id_003_feat3.csv\n",
    "##     │   ├── id_004_feat3.csv\n",
    "##     │   └── combined_data.tmp\n",
    "##     └── cleaned_feature_4\n",
    "##         ├── id_001_feat4.csv\n",
    "##         ├── id_002_feat4.csv\n",
    "##         ├── id_003_feat4.csv\n",
    "##         └── id_004_feat4.csv\n",
    "import tempfile\n",
    "\n",
    "def create_temporary_directory_structure():\n",
    "    temp_dir = tempfile.mkdtemp()\n",
    "    data_set_1 = Path(temp_dir).joinpath(\"data_set_1\")\n",
    "    data_set_1.mkdir()\n",
    "    data_set_2 = Path(temp_dir).joinpath(\"data_set_2\")\n",
    "    data_set_2.mkdir()\n",
    "\n",
    "    cleaned_feature_1 = data_set_1.joinpath(\"cleaned_feature_1\")\n",
    "    cleaned_feature_1.mkdir()\n",
    "    cleaned_feature_2 = data_set_1.joinpath(\"cleaned_feature_2\")\n",
    "    cleaned_feature_2.mkdir()\n",
    "\n",
    "    cleaned_feature_3 = data_set_2.joinpath(\"cleaned_feature_3\")\n",
    "    cleaned_feature_3.mkdir()\n",
    "    cleaned_feature_4 = data_set_2.joinpath(\"cleaned_feature_4\")\n",
    "    cleaned_feature_4.mkdir()\n",
    "\n",
    "    # Create feature files\n",
    "    for feature in  range(1, 5):\n",
    "        for i in range(1, 4):\n",
    "                if feature == 1:\n",
    "                    cleaned_feature_1.joinpath(f\"id_00{i}_feat{feature}.csv\").touch()\n",
    "                if feature == 2:\n",
    "                    cleaned_feature_2.joinpath(f\"id_00{i}_feat{feature}.csv\").touch()\n",
    "                if feature == 3:\n",
    "                    cleaned_feature_3.joinpath(f\"id_00{i}_feat{feature}.csv\").touch()\n",
    "                if feature == 4:\n",
    "                    cleaned_feature_4.joinpath(f\"id_00{i}_feat{feature}.csv\").touch()\n",
    "    # Add extra ids to 3 and 4\n",
    "    cleaned_feature_3.joinpath(f\"id_004_feat3.csv\").touch()\n",
    "    cleaned_feature_4.joinpath(f\"id_004_feat4.csv\").touch()\n",
    "    # Create other files\n",
    "    cleaned_feature_1.joinpath(\"analyze_data.py\").touch()\n",
    "    cleaned_feature_1.joinpath(\"_preliminary_analysis.csv\").touch()\n",
    "    cleaned_feature_2.joinpath(\".analyze_data_another_way.py\").touch()\n",
    "    cleaned_feature_2.joinpath(\".other_data.csv\").touch()\n",
    "    cleaned_feature_3.joinpath(\"combined_data.tmp\").touch()\n",
    "\n",
    "    return temp_dir\n",
    "# Test converting a PSG to different sleep stages\n",
    "data = pl.read_csv(\"../mock_data_sets/data_set_1/cleaned_psg/id001_cleaned_psg.out\",\n",
    "                   has_header=False, separator=\" \")\n",
    "# PSG to sleep/wake\n",
    "sleep_wake = pl.Series(\"sleep_wake\", psg_to_sleep_wake(data))\n",
    "sleep_wake_data = data.with_columns([sleep_wake])\n",
    "masked_values = sleep_wake_data.filter(sleep_wake_data[\"column_2\"] == -1)\n",
    "assert np.all(masked_values[\"sleep_wake\"].to_numpy() == -1)\n",
    "awake_values = sleep_wake_data.filter(sleep_wake_data[\"column_2\"] == 0)\n",
    "assert np.all(awake_values[\"sleep_wake\"].to_numpy() == 0)\n",
    "sleep_values = sleep_wake_data.filter(sleep_wake_data[\"column_2\"] > 0)\n",
    "assert np.all(sleep_values[\"sleep_wake\"].to_numpy() == 1)\n",
    "# PSG to WLDM (with N4)\n",
    "wldm = pl.Series(\"wldm\", psg_to_WLDM(data))\n",
    "wldm_data = data.with_columns([wldm])\n",
    "masked_values = wldm_data.filter(wldm_data[\"column_2\"] == -1)\n",
    "assert np.all(masked_values[\"wldm\"].to_numpy() == -1)\n",
    "awake_values = wldm_data.filter(wldm_data[\"column_2\"] == 0)\n",
    "assert np.all(awake_values[\"wldm\"].to_numpy() == 0)\n",
    "light_values = wldm_data.filter((wldm_data[\"column_2\"] == 1) | (wldm_data[\"column_2\"] == 2))\n",
    "assert np.all(light_values[\"wldm\"].to_numpy() == 1)\n",
    "deep_values = wldm_data.filter((wldm_data[\"column_2\"] == 3) | (wldm_data[\"column_2\"] == 4))\n",
    "assert np.all(deep_values[\"wldm\"].to_numpy() == 2)\n",
    "rem_values = wldm_data.filter(wldm_data[\"column_2\"] == 5)\n",
    "assert np.all(rem_values[\"wldm\"].to_numpy() == 3)\n",
    "# PSG to WLDM (without N4)\n",
    "# convert all 5s to 4s in column_2\n",
    "new_column_2 = wldm_data[\"column_2\"].map_elements(lambda x: 4 if x == 5 else x, pl.Int32)\n",
    "wldm_data = wldm_data.with_columns([new_column_2])\n",
    "wldm = pl.Series(\"wldm\", psg_to_WLDM(data, N4=False))\n",
    "wldm_data = data.with_columns([wldm])\n",
    "masked_values = wldm_data.filter(wldm_data[\"column_2\"] == -1)\n",
    "assert np.all(masked_values[\"wldm\"].to_numpy() == -1)\n",
    "awake_values = wldm_data.filter(wldm_data[\"column_2\"] == 0)\n",
    "assert np.all(awake_values[\"wldm\"].to_numpy() == 0)\n",
    "light_values = wldm_data.filter((wldm_data[\"column_2\"] == 1) | (wldm_data[\"column_2\"] == 2))\n",
    "assert np.all(light_values[\"wldm\"].to_numpy() == 1)\n",
    "deep_values = wldm_data.filter((wldm_data[\"column_2\"] == 3))\n",
    "assert np.all(deep_values[\"wldm\"].to_numpy() == 2)\n",
    "rem_values = wldm_data.filter(wldm_data[\"column_2\"] == 4)\n",
    "assert np.all(rem_values[\"wldm\"].to_numpy() == 3)\n",
    "# Test find and parse data sets\n",
    "temp_dir = create_temporary_directory_structure()\n",
    "id_templates = {\n",
    "     'feature_1': 'id_<<ID>>_feat1.csv',\n",
    "    'feature_2': 'id_<<ID>>_feat2.csv',\n",
    "    'feature_3': 'id_<<ID>>_feat3.csv',\n",
    "    'feature_4': 'id_<<ID>>_feat4.csv',\n",
    "}\n",
    "data_sets = DataSetObject.find_data_sets(temp_dir)\n",
    "\n",
    "for data_set in data_sets.values():\n",
    "    data_set.parse_data_sets(ignore_startswith=[\".\", \"_\"], \n",
    "                             ignore_endswith=[\".tmp\", \".py\"], \n",
    "                             id_templates=id_templates)\n",
    "assert len(data_sets) == 2\n",
    "\n",
    "assert len(data_sets['data_set_1'].features) == 2\n",
    "assert len(data_sets['data_set_2'].features) == 2\n",
    "assert data_sets['data_set_1'].ids == ['001', '002', '003']\n",
    "assert data_sets['data_set_2'].ids == ['001', '002', '003', '004']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#| hide\n",
    "# DataProcessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class ModelOutputType(Enum):\n",
    "    SLEEP_WAKE = auto()\n",
    "    WAKE_LIGHT_DEEP_REM = auto()\n",
    "\n",
    "class PSGType(Enum):\n",
    "    NO_N4 = auto()\n",
    "    HAS_N4 = auto()\n",
    "\n",
    "class ModelInput:\n",
    "    def __init__(self,\n",
    "                 input_features: List[str] | str,\n",
    "                 input_sampling_hz: int | float, # Sampling rate of the input data (1/s)\n",
    "                 ):\n",
    "        # input_features\n",
    "        if isinstance(input_features, str):\n",
    "            input_features = [input_features]\n",
    "        self.input_features = input_features\n",
    "        # input_sampling_hz\n",
    "        if not isinstance(input_sampling_hz, (int, float)):\n",
    "            raise ValueError(\"input_sampling_hz must be an int or a float\")\n",
    "        else:\n",
    "            if input_sampling_hz <= 0:\n",
    "                raise ValueError(\"input_sampling_hz must be greater than 0\")\n",
    "        self.input_sampling_hz = float(input_sampling_hz)\n",
    "\n",
    "class ModelInput1D(ModelInput):\n",
    "    def __init__(self,\n",
    "                 input_features: List[str] | str,\n",
    "                 input_sampling_hz: int | float, # Sampling rate of the input data (1/s)\n",
    "                 input_window_time: int | float, # Window size (in seconds) for the input data. Window will be centered around the time point for which the model is making a prediction\n",
    "                 ):\n",
    "        super().__init__(input_features, input_sampling_hz)\n",
    "        # input_window_time\n",
    "        if not isinstance(input_window_time, (int, float)):\n",
    "            raise ValueError(\"input_window_time must be an int or a float\")\n",
    "        else:\n",
    "            if input_window_time <= 0:\n",
    "                raise ValueError(\"input_window_time must be greater than 0\")\n",
    "\n",
    "        self.input_window_time = float(input_window_time)\n",
    "        # Number of samples for the input window of a single feature\n",
    "        self.input_window_samples = int(self.input_window_time * self.input_sampling_hz)\n",
    "        ## force it to be odd to have perfectly centered window\n",
    "        if self.input_window_samples % 2 == 0:\n",
    "            self.input_window_samples += 1\n",
    "        # Dimension of the input data for the model\n",
    "        self.model_input_dimension = int(len(input_features) * self. input_window_samples)\n",
    "\n",
    "class ModelInputSpectrogram(ModelInput):\n",
    "    def __init__(self,\n",
    "                 input_features: List[str] | str,\n",
    "                 input_sampling_hz: int | float, # Sampling rate of the input data (1/s)\n",
    "                 spectrogram_preprocessing_config: Dict=MO_PREPROCESSING_CONFIG, # Steps in the preprocessing pipeline for getting a spectrogram from acceleration\n",
    "                 ):\n",
    "        super().__init__(input_features, input_sampling_hz)\n",
    "        self.input_sampling_hz = float(input_sampling_hz)\n",
    "        self.spectrogram_preprocessing_config = spectrogram_preprocessing_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def get_sample_weights(y: np.ndarray) -> np.ndarray:\n",
    "     \"\"\"\n",
    "     Calculate sample weights based on the distribution of classes in the data.\n",
    "     Doesn't count masked values (-1) in the class distribution.\n",
    "     \"\"\"\n",
    "     # Filter out -1 values\n",
    "     valid_y = y[y != -1]\n",
    "     # Calculate class counts for valid labels only\n",
    "     class_counts = np.bincount(valid_y)\n",
    "     class_weights = np.where(class_counts > 0, class_counts.sum() / class_counts, 0)\n",
    "     # Map valid class weights to corresponding samples in y\n",
    "     sample_weights = np.zeros_like(y, dtype=float)\n",
    "     for class_index, weight in enumerate(class_weights):\n",
    "          sample_weights[y == class_index] = weight\n",
    "     # Masked values (-1) in y will have a weight of 0\n",
    "     return sample_weights\n",
    "\n",
    "\n",
    "def mask_psg_from_accel(psg: np.ndarray, accel: np.ndarray, \n",
    "                        psg_epoch: int = 30,\n",
    "                        accel_sample_rate: float | None = None,\n",
    "                        min_epoch_fraction_covered: float = 0.5\n",
    "                        ) -> np.ndarray:\n",
    "\n",
    "    acc_last_index = 0\n",
    "    acc_next_index = acc_last_index\n",
    "    acc_last_time = accel[acc_last_index, 0]\n",
    "    acc_next_time = acc_last_time\n",
    "\n",
    "    # at least this fraction of 1 epoch must be covered\n",
    "    # both in terms of time (no gap longer than 0.5 epochs)\n",
    "    # and in terms of expected number of samples in that time.\n",
    "    min_epoch_covered = min_epoch_fraction_covered * psg_epoch\n",
    "    if accel_sample_rate is None:\n",
    "        # median sample step size, if none provided\n",
    "        # median to not take into account gaps!\n",
    "        accel_sample_rate = np.median(np.diff(accel[:, 0]))\n",
    "    min_samples_per = min_epoch_covered / accel_sample_rate\n",
    "\n",
    "    psg_gap_indices = []\n",
    "\n",
    "    for (psg_index, psg_sample) in enumerate(psg):\n",
    "        epoch_ends = psg_sample[0] + psg_epoch\n",
    "\n",
    "        # find the last timestamp inside the epoch\n",
    "        while (acc_next_time <= epoch_ends and acc_next_index < len(accel)):\n",
    "            acc_next_time = accel[acc_next_index, 0]\n",
    "            acc_next_index += 1\n",
    "        \n",
    "        # 1. check for lots of missing time\n",
    "        # 2. check for very low sampling rate\n",
    "        if ((acc_next_time - acc_last_time) < min_epoch_covered) \\\n",
    "            or (acc_next_index - acc_last_index < min_samples_per):\n",
    "            psg_gap_indices.append(psg_index)\n",
    "        \n",
    "        # set up for next iteration\n",
    "        acc_last_time = acc_next_time\n",
    "        acc_last_index = acc_next_index\n",
    "    \n",
    "    psg[np.array(psg_gap_indices), 1] = -1\n",
    "\n",
    "    return psg\n",
    "\n",
    "\n",
    "def apply_gausian_filter(df: pl.DataFrame, sigma: float = 1.0, overwrite: bool = False) -> pl.DataFrame:\n",
    "    data_columns = df.columns[1:]  # Adjust this to match your data column indices\n",
    "    # Apply Gaussian smoothing to each data column\n",
    "    for col in data_columns:\n",
    "        new_col_name = f\"{col}_smoothed\" if not overwrite else col\n",
    "        df = df.with_columns(\n",
    "            pl.Series(gaussian_filter1d(df[col].to_numpy(), sigma)).alias(new_col_name)\n",
    "        )\n",
    "    return df\n",
    "\n",
    "\n",
    "def fill_gaps_in_accelerometer_data(acc: pl.DataFrame, smooth: bool = False, final_sampling_rate_hz: int | None = None) -> np.ndarray:\n",
    "    # median sampling rate (to account for missing data)\n",
    "    sampling_period_s = acc[acc.columns[0]].diff().median() # 1 / sampling_rate_hz\n",
    "    \n",
    "    # Step 0: Save the original 'timestamp' column as 'timestamp_raw'\n",
    "    acc_resampled = acc.with_columns(acc[acc.columns[0]].alias('timestamp'))\n",
    "\n",
    "    #TODO: Check non int sampling rates\n",
    "    # if isinstance(final_sampling_rate_hz, int):\n",
    "    final_rate_sec = 1 / final_sampling_rate_hz\n",
    "    print(f\"resampling to {final_sampling_rate_hz}Hz ({final_rate_sec:0.5f}s) from {int(1/sampling_period_s)} Hz ({sampling_period_s:0.5f}s)\")\n",
    "    # make a new data frame with the new timestamps\n",
    "    # do this using linear interpolation\n",
    "\n",
    "    median_time = acc_resampled['timestamp'].to_numpy()\n",
    "    final_timestamps = np.arange(median_time.min(), median_time.max() + final_rate_sec, final_rate_sec)\n",
    "    median_data = acc_resampled[:, 1:4].to_numpy()\n",
    "    new_data = np.zeros((final_timestamps.shape[0], median_data.shape[1]))\n",
    "    for i in range(median_data.shape[1]):\n",
    "        new_data[:, i] = np.interp(final_timestamps, median_time, median_data[:, i])\n",
    "    acc_resampled = pl.DataFrame({\n",
    "        'timestamp': final_timestamps, \n",
    "        **{\n",
    "            acc_resampled.columns[i+1]: new_data[:, i] \n",
    "            for i in range(new_data.shape[1])\n",
    "        }})\n",
    "\n",
    "    if smooth:\n",
    "        acc_resampled = apply_gausian_filter(acc_resampled, overwrite=True)\n",
    "\n",
    "    return acc_resampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class DataProcessor:\n",
    "    def __init__(self,\n",
    "                 data_set: DataSetObject,\n",
    "                 model_input: ModelInput,\n",
    "                 output_feature: str='psg',\n",
    "                 output_type: ModelOutputType=ModelOutputType.WAKE_LIGHT_DEEP_REM,\n",
    "                 psg_type: PSGType=PSGType.NO_N4,\n",
    "                 ):\n",
    "        self.data_set = data_set\n",
    "        self.input_features = model_input.input_features\n",
    "        self.output_feature = output_feature\n",
    "        self.output_type = output_type\n",
    "        self.psg_type = psg_type\n",
    "        self.model_input = model_input\n",
    "\n",
    "        if self.is_1D:\n",
    "            self.input_window_time = model_input.input_window_time\n",
    "            self.input_sampling_hz = model_input.input_sampling_hz\n",
    "            self.input_window_samples = model_input.input_window_samples\n",
    "            self.model_input_dimension = model_input.model_input_dimension\n",
    "        elif self.is_spectrogram:\n",
    "            self.input_sampling_hz = model_input.input_sampling_hz\n",
    "            self.spectrogram_preprocessing_config = model_input.spectrogram_preprocessing_config\n",
    "\n",
    "    @property\n",
    "    def is_1D(self):\n",
    "        return isinstance(self.model_input, ModelInput1D)\n",
    "\n",
    "    @property\n",
    "    def is_spectrogram(self):\n",
    "        return isinstance(self.model_input, ModelInputSpectrogram)\n",
    "\n",
    "    def get_labels(self, id: str, start: int, end: int,\n",
    "                   output_feature: str) -> pl.DataFrame | None:\n",
    "        data = self.data_set.get_feature_data(output_feature, id)\n",
    "        data = data.filter(data[:, 0] >= start)\n",
    "        data = data.filter(data[:, 0] <= end)\n",
    "\n",
    "        # Mask PSG data based on accelerometer data if present\n",
    "        if \"accelerometer\" in self.input_features:\n",
    "            accelerometer = self.data_set.get_feature_data('accelerometer', id)\n",
    "            data = mask_psg_from_accel(data, accelerometer, \n",
    "                                       accel_sample_rate=self.input_sampling_hz)\n",
    "\n",
    "        if self.output_feature == 'psg':\n",
    "            if self.output_type == ModelOutputType.SLEEP_WAKE:\n",
    "                y = psg_to_sleep_wake(data)\n",
    "            elif self.output_type == ModelOutputType.WAKE_LIGHT_DEEP_REM:\n",
    "                N4 = self.psg_type == PSGType.HAS_N4\n",
    "                y = psg_to_WLDM(data, N4)\n",
    "            else:\n",
    "                raise ValueError(f\"Output type {self.output_type} not supported\")\n",
    "        else:\n",
    "            raise ValueError(f\"Output feature {output_feature} not supported\")\n",
    "\n",
    "        labels = pl.DataFrame({\n",
    "            'time': data[:, 0],\n",
    "            'label': y,\n",
    "        })\n",
    "\n",
    "        return labels\n",
    "\n",
    "    def get_1D_X_for_feature(self, interpolation_timestamps: np.ndarray, \n",
    "                             epoch_times: np.ndarray, feature_times: np.ndarray, \n",
    "                             feature_values: np.ndarray) -> np.ndarray:\n",
    "            interpolation = np.interp(interpolation_timestamps, feature_times, feature_values)\n",
    "            X_feature = []\n",
    "            for t in epoch_times:\n",
    "                t_idx = np.argmin(np.abs(interpolation_timestamps - t))\n",
    "                # Window centered around t with half `window_samples` on each side\n",
    "                window_idx_start = t_idx - self.input_window_samples // 2\n",
    "                window_idx_end = t_idx + self.input_window_samples // 2 + 1\n",
    "                window_data = interpolation[window_idx_start:window_idx_end]\n",
    "                # reshape into (1, window_size)\n",
    "                window_data = window_data.reshape(1, -1)\n",
    "                X_feature.append(window_data)\n",
    "            # create a numpy array of shape (n_samples, window_size)\n",
    "            X_feature = np.vstack(X_feature)\n",
    "            return X_feature\n",
    "\n",
    "    def get_1D_X_y(self, id: str) -> Tuple[np.ndarray, np.ndarray] | None:\n",
    "        # Find overlapping time section\n",
    "        all_features = self.input_features + [self.output_feature]\n",
    "        max_start, min_end = self.data_set.find_overlapping_time_section(all_features, id)\n",
    "        # Get labels\n",
    "        labels = self.get_labels(id, max_start, min_end, self.output_feature)\n",
    "        label_times = labels[:, 0]\n",
    "        epoch_start = label_times.min() + self.input_window_time / 2.0\n",
    "        epoch_end = label_times.max() - self.input_window_time / 2.0\n",
    "        filtered_labels = labels.filter(labels[:, 0] >= epoch_start)\n",
    "        filtered_labels = filtered_labels.filter(filtered_labels[:, 0] <= epoch_end)\n",
    "        epoch_times = filtered_labels[:, 0]\n",
    "        # Get input data\n",
    "        interpolation_timestamps = np.arange(max_start, \n",
    "                                             min_end + 1.0/self.input_sampling_hz,\n",
    "                                             1.0/self.input_sampling_hz,)\n",
    "        # Interpolate all data to the same time points\n",
    "        interpolated_features = []\n",
    "        for feature in self.input_features:\n",
    "            data = self.data_set.get_feature_data(feature, id)\n",
    "            feature_times = data[:, 0]\n",
    "            for i in range(1, data.shape[1]):\n",
    "                feature_values = data[:, i]\n",
    "                X_feature = self.get_1D_X_for_feature(interpolation_timestamps, \n",
    "                                                        epoch_times, feature_times, \n",
    "                                                        feature_values)\n",
    "                interpolated_features.append(X_feature)\n",
    "        # Concatenate input features alongside the first dimension\n",
    "        X = np.concatenate(interpolated_features, axis=1)\n",
    "        y = filtered_labels[:, 1].to_numpy()\n",
    "        return X, y\n",
    "    \n",
    "    def accelerometer_to_spectrogram(self, accelerometer: pl.DataFrame) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Implementation by Mads Olsen at https://github.com/MADSOLSEN/SleepStagePrediction\n",
    "        with minor modifications.\n",
    "        \"\"\"\n",
    "        if isinstance(accelerometer, pl.DataFrame):\n",
    "            acc = accelerometer.to_numpy()\n",
    "        else:\n",
    "            raise ValueError(\"accelerometer must be a polars DataFrame\")\n",
    "\n",
    "        x_ = acc[:, 1]\n",
    "        y_ = acc[:, 2]\n",
    "        z_ = acc[:, 3]\n",
    "\n",
    "        for step in self.spectrogram_preprocessing_config[\"preprocessing\"]:\n",
    "            fn = eval(step[\"type\"])  # convert string version to function in environment\n",
    "            fn_args = partial(\n",
    "                fn, **step[\"args\"]\n",
    "            )  # fill in the args given, which must be everything besides numerical input\n",
    "\n",
    "            # apply\n",
    "            x_ = fn_args(x_)\n",
    "            y_ = fn_args(y_)\n",
    "            z_ = fn_args(z_)\n",
    "\n",
    "        spec = x_ + y_ + z_\n",
    "        spec /= 3.0\n",
    "\n",
    "        return spec\n",
    "\n",
    "    def mirror_spectrogram(self, spectrogram: np.ndarray) -> np.ndarray:\n",
    "        # We will copy the spectrogram to both channels, flipping it on channel 1\n",
    "        input_shape = (1, *MO_UNET_CONFIG['input_shape'])\n",
    "        inputs_len = input_shape[1]\n",
    "\n",
    "        mirrored = np.zeros(shape=input_shape, dtype=np.float32)\n",
    "        # We must do some careful work with indices to not overflow arrays\n",
    "        spec = spectrogram[:inputs_len].astype(np.float32) # protect agains spec.len > input_shape\n",
    "\n",
    "        #! careful, order matters here. We first trim spec to make sure it'll fit into inputs,\n",
    "        # then compute the new length which we KNOW is <= inputs_len\n",
    "        spec_len = spec.shape[0]\n",
    "        # THEN we assign only as much inputs as spec covers\n",
    "        mirrored[0, : spec_len, :, 0] = spec # protect agains spec_len < input_shape\n",
    "        mirrored[0, : spec_len, :, 1] = spec[:, ::-1]\n",
    "\n",
    "        return mirrored\n",
    "\n",
    "    def get_spectrogram_X_y(self, id: str) -> Tuple[np.ndarray, np.ndarray] | None:\n",
    "        # Find overlapping time section\n",
    "        all_features = self.input_features + [self.output_feature]\n",
    "        max_start, min_end = self.data_set.find_overlapping_time_section(all_features, id)\n",
    "\n",
    "        if self.input_features != ['accelerometer']:\n",
    "            raise ValueError(\"Spectrogram input only supported for accelerometer data\")\n",
    "\n",
    "        accelerometer = self.data_set.get_feature_data('accelerometer', id)\n",
    "        # Use only the overlapping time section\n",
    "        accelerometer = accelerometer.filter(accelerometer[:, 0] >= max_start)\n",
    "        accelerometer = accelerometer.filter(accelerometer[:, 0] <= min_end)\n",
    "        # Fill gaps in accelerometer data\n",
    "        accelerometer = fill_gaps_in_accelerometer_data(accelerometer, smooth=False, \n",
    "                                                        final_sampling_rate_hz=self.input_sampling_hz)\n",
    "        # Get spectrogram (mirrored)\n",
    "        spectrogram = self.accelerometer_to_spectrogram(accelerometer)\n",
    "        X = self.mirror_spectrogram(spectrogram)\n",
    "\n",
    "        # Get labels\n",
    "        labels = self.get_labels(id, max_start, min_end, self.output_feature)\n",
    "        y = labels[:, 1].to_numpy()\n",
    "        # Match labels to model output\n",
    "        if len(y) < N_OUT:\n",
    "            y = np.pad(y, (0, N_OUT - len(y)), constant_values=-1)\n",
    "        elif len(y) > N_OUT:\n",
    "            y = y[:N_OUT]\n",
    "\n",
    "        return X, y \n",
    "\n",
    "    def preprocess_data_for_subject(self, id: str) -> Tuple[np.ndarray, np.ndarray] | None:\n",
    "        if self.is_1D:\n",
    "            return self.get_1D_X_y(id)\n",
    "        elif self.is_spectrogram:\n",
    "            return self.get_spectrogram_X_y(id)\n",
    "        else:\n",
    "            raise ValueError(\"ModelInput type not supported\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# Test we get the correct shape of inputs and outputs\n",
    "mock_data_set_1_path = Path(\"../mock_data_sets/data_set_1\")\n",
    "data_sets = DataSetObject.find_data_sets(mock_data_set_1_path)\n",
    "id_templates = {\n",
    "    'accelerometer': '<<ID>>_cleaned_motion.out',\n",
    "    'activity': '<<ID>>_cleaned_counts.out',\n",
    "    'heartrate': '<<ID>>_cleaned_hr.out',\n",
    "    'psg': '<<ID>>_cleaned_psg.out',\n",
    "}\n",
    "for data_set in data_sets.values():\n",
    "    data_set.parse_data_sets(id_templates=id_templates)\n",
    "# Test getting X and y for a subject for 1D input\n",
    "## one feature\n",
    "input_sampling_hz = 1.0\n",
    "input_window_time = 60\n",
    "input_features = ['activity']\n",
    "data_processor = DataProcessor(data_set, \n",
    "                               ModelInput1D(input_features,\n",
    "                                            input_sampling_hz, \n",
    "                                            input_window_time),)\n",
    "X_1D, y_1D = data_processor.get_1D_X_y(data_set.ids[0])\n",
    "assert X_1D.shape[0] == y_1D.shape[0]\n",
    "assert X_1D.shape[1] == int(input_window_time * input_sampling_hz + 1) * len(data_processor.input_features)\n",
    "## two features\n",
    "input_features = ['activity', 'heartrate']\n",
    "data_processor = DataProcessor(data_set, \n",
    "                               ModelInput1D(input_features,\n",
    "                                            input_sampling_hz, \n",
    "                                            input_window_time),)\n",
    "X_1D, y_1D = data_processor.get_1D_X_y(data_set.ids[0])\n",
    "assert X_1D.shape[0] == y_1D.shape[0]\n",
    "assert X_1D.shape[1] == int(input_window_time * input_sampling_hz + 1) * len(data_processor.input_features)\n",
    "# three features including accelerometer\n",
    "input_features = ['activity', 'heartrate', 'accelerometer']\n",
    "data_processor = DataProcessor(data_set, \n",
    "                               ModelInput1D(input_features,\n",
    "                                            input_sampling_hz, \n",
    "                                            input_window_time),)\n",
    "X_1D, y_1D = data_processor.get_1D_X_y(data_set.ids[0])\n",
    "assert X_1D.shape[0] == y_1D.shape[0]\n",
    "assert X_1D.shape[1] == int(input_window_time * input_sampling_hz + 1) * (len(data_processor.input_features) + 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#| hide\n",
    "# Create mock data set for testing and documentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# not exported\n",
    "def _create_mock_data():\n",
    "    np.random.seed(42) # by Deep Thought\n",
    "    mock_data_location = \"../mock_data_sets\"\n",
    "    if not os.path.exists(mock_data_location):\n",
    "        os.makedirs(mock_data_location)\n",
    "    total_time_hrs = 1.0\n",
    "    total_subjects = 3\n",
    "    total_data_sets = 2\n",
    "    accelerometer_sampling_hz = 1.0 \n",
    "    activity_dt_seconds = 15.0\n",
    "    max_activity = 50\n",
    "    psg_dt_seconds = 30.0\n",
    "    for data_set in range(total_data_sets):\n",
    "        data_set_path = f\"{mock_data_location}/data_set_{data_set}\"\n",
    "        if not os.path.exists(data_set_path):\n",
    "            os.makedirs(data_set_path)\n",
    "        # Accelerometer data\n",
    "        accelerometer_path = f\"{data_set_path}/cleaned_accelerometer\"\n",
    "        if not os.path.exists(accelerometer_path):\n",
    "            os.makedirs(accelerometer_path)\n",
    "        accelerometer_time = np.arange(0, total_time_hrs * 3600, 1.0 / accelerometer_sampling_hz)\n",
    "        for i in range(total_subjects):\n",
    "            accelerometer_data = np.random.randn(len(accelerometer_time), 3)\n",
    "            accelerometer = pl.DataFrame({\n",
    "                'timestamp': accelerometer_time,\n",
    "                'x': accelerometer_data[:, 0],\n",
    "                'y': accelerometer_data[:, 1],\n",
    "                'z': accelerometer_data[:, 2],\n",
    "            })\n",
    "            subject_path = f\"{accelerometer_path}/id00{i}_cleaned_motion.out\"\n",
    "            accelerometer.write_csv(subject_path, include_header=False, separator=' ')\n",
    "        # Activity data\n",
    "        activity_path = f\"{data_set_path}/cleaned_activity\"\n",
    "        if not os.path.exists(activity_path):\n",
    "            os.makedirs(activity_path)\n",
    "        activity_time = np.arange(0, total_time_hrs * 3600, activity_dt_seconds)\n",
    "        for i in range(total_subjects):\n",
    "            activity_data = np.random.randint(0, max_activity, len(activity_time))\n",
    "            activity = pl.DataFrame({\n",
    "                'timestamp': activity_time,\n",
    "                'activity': activity_data,\n",
    "            })\n",
    "            subject_path = f\"{activity_path}/id00{i}_cleaned_counts.out\"\n",
    "            activity.write_csv(subject_path, include_header=False, separator=' ')\n",
    "        # Heart rate data\n",
    "        hr_path = f\"{data_set_path}/cleaned_heartrate\"\n",
    "        if not os.path.exists(hr_path):\n",
    "            os.makedirs(hr_path)\n",
    "        ## Irregular sampling rate\n",
    "        hr_time = np.random.choice(accelerometer_time, len(activity_time), replace=False)\n",
    "        for i in range(total_subjects):\n",
    "            hr_data = np.random.randint(60, 120, len(activity_time))\n",
    "            hr = pl.DataFrame({\n",
    "                'timestamp': hr_time,\n",
    "                'hr': hr_data,\n",
    "            })\n",
    "            subject_path = f\"{hr_path}/id00{i}_cleaned_hr.out\"\n",
    "            hr.write_csv(subject_path, include_header=False, separator=' ')\n",
    "        # PSG data\n",
    "        psg_path = f\"{data_set_path}/cleaned_psg\"\n",
    "        if not os.path.exists(psg_path):\n",
    "            os.makedirs(psg_path)\n",
    "        psg_time = np.arange(0, total_time_hrs * 3600, psg_dt_seconds)\n",
    "        for i in range(total_subjects):\n",
    "            psg_data = np.random.randint(-1, 5, len(psg_time))\n",
    "            psg = pl.DataFrame({\n",
    "                'timestamp': psg_time,\n",
    "                'stage': psg_data,\n",
    "            })\n",
    "            subject_path = f\"{psg_path}/id00{i}_cleaned_psg.out\"\n",
    "            psg.write_csv(subject_path, include_header=False, separator=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev\n",
    "nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
