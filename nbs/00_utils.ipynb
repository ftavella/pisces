{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# utils\n",
    "\n",
    "> Module of utility functions used throughout the package, for things like common preprocessing steps many models are likely to use, or useful for multiple points of analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp utils "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide \n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "import csv\n",
    "import os\n",
    "import time\n",
    "from enum import Enum, auto\n",
    "from typing import Dict, List, Optional, Tuple, Union\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CSV utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def determine_header_rows_and_delimiter(\n",
    "    filename: Path | str\n",
    ") -> Tuple[Optional[int], Optional[str]]:\n",
    "    \"\"\"\n",
    "    Given a filename pointing at a CSV files, decides:\n",
    "     * how many header lines there are (based on first line starting with a digit)\n",
    "     * the delimiter-- right now tries whitespace and comma\n",
    "\n",
    "    Returns one of:\n",
    "     - (number of header rows, column delimiter),\n",
    "     - (number of header rows, None) if the delimiter could not be inferred,\n",
    "     - (None, None) if CSV has no numerical rows,\n",
    "\n",
    "    :param filename: CSV Path or filepath literal\n",
    "    :return: header and delimiter information, if possible.\n",
    "    \"\"\"\n",
    "    MAX_ROWS = 100  # if you don't have data within first 100 lines, exit.\n",
    "    header_row_count = 0\n",
    "    with open(filename) as f:\n",
    "        header_found = False\n",
    "        line = \"\"  # modified in while below\n",
    "\n",
    "        # Search over lines until we find one starting with a digit\n",
    "        while not header_found:\n",
    "            line = f.readline()\n",
    "            if line == \"\":  # last line of file reached\n",
    "                return None, None\n",
    "            line = line.strip()\n",
    "            try:\n",
    "                int(line[0])\n",
    "                header_found = True\n",
    "            except ValueError:\n",
    "                header_row_count += 1\n",
    "            except IndexError:\n",
    "                header_row_count += 1\n",
    "\n",
    "            # guard against infinite loop\n",
    "            if header_row_count >= MAX_ROWS:\n",
    "                return None, None\n",
    "\n",
    "        # Now try splitting that first line of data\n",
    "        delim_guesses = [\" \", \",\", \", \"]\n",
    "\n",
    "        for guess in delim_guesses:\n",
    "            try:\n",
    "                comps = line.split(guess)  # whitespace separated?\n",
    "                float(comps[0])\n",
    "                return header_row_count, guess\n",
    "            except ValueError:\n",
    "                continue\n",
    "        return header_row_count, None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activity counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "class ActivityCountAlgorithm(Enum):\n",
    "    te_Lindert_et_al = 0\n",
    "    ActiGraphOfficial = 1\n",
    "    ADS = 2\n",
    "\n",
    "\n",
    "def build_activity_counts(\n",
    "    data,\n",
    "    axis: int = 3,\n",
    "    prefix: str = \"\",\n",
    "    algorithm: ActivityCountAlgorithm = ActivityCountAlgorithm.ADS\n",
    ") -> Tuple[np.ndarray, np.ndarray]:\n",
    "    if algorithm == ActivityCountAlgorithm.ActiGraphOfficial:\n",
    "        return build_ActiGraph_official(data)\n",
    "    if algorithm == ActivityCountAlgorithm.ADS:\n",
    "        return build_ADS(data)\n",
    "    if algorithm == ActivityCountAlgorithm.te_Lindert_et_al:\n",
    "        return build_activity_counts_te_Lindert_et_al(data, axis, prefix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "\n",
    "def build_ADS(\n",
    "    time_xyz: np.ndarray,\n",
    "    sampling_hz: float = 50.0,\n",
    "    bin_size_seconds: float = 15,\n",
    "    prefix: str = \"\",\n",
    ") -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"ADS algorithm for activity counts, developed by Arcascope with support from the NHRC.\n",
    "\n",
    "    Parameters\n",
    "    ---\n",
    "     - `time_xyz`: numpy array with shape (N_samples, 4) where the 4 coordinates are: [time, x, y, z] \n",
    "     - `sampling_hz`: `float` sampling frequency of thetime_xyz \n",
    "    \"\"\"\n",
    "    data_shape_error = ValueError(\n",
    "            f\"`time_xyz` must have shape (N_samples, 4) but has shape {time_xyz.shape}\"\n",
    "        )\n",
    "    try:\n",
    "        assert (len(time_xyz.shape) == 2 and time_xyz.shape[1] == 4)\n",
    "    except AssertionError:\n",
    "        raise data_shape_error\n",
    "\n",
    "    time_data_raw = time_xyz[:, 0]\n",
    "    x_accel = time_xyz[:, 1]\n",
    "    y_accel = time_xyz[:, 2]\n",
    "    z_accel = time_xyz[:, 3]\n",
    "\n",
    "    # Interpolate to sampling Hz\n",
    "    time_values = np.arange(\n",
    "        np.amin(time_data_raw), np.amax(time_data_raw), 1 / sampling_hz\n",
    "    )\n",
    "    # Must do each coordinate separately\n",
    "    x_data = np.interp(time_values, time_data_raw, x_accel)\n",
    "    y_data = np.interp(time_values, time_data_raw, y_accel)\n",
    "    z_data = np.interp(time_values, time_data_raw, z_accel)\n",
    "\n",
    "    # Calculate \"amplitude\" = timeseries of 2-norm of (x, y, z)\n",
    "    amplitude = np.linalg.norm(np.array([x_data, y_data, z_data]), axis = 0)\n",
    "\n",
    "    abs_amplitude_deriv = np.abs(np.diff(amplitude))\n",
    "    abs_amplitude_deriv = np.insert(abs_amplitude_deriv, 0, 0)\n",
    "\n",
    "    # Binning step\n",
    "    # Sum abs_amplitude_deriv in time-based windows\n",
    "    # ex: bin_size_seconds = 15\n",
    "    # Step from first to last time by 15 seconds\n",
    "    time_counts = np.arange(\n",
    "        np.amin(time_data_raw), np.amax(time_data_raw), bin_size_seconds\n",
    "    )\n",
    "\n",
    "    # Convert time at 50 hz to \"# of 15 second windows past start\"\n",
    "    bin_values = (time_values - time_values[0]).astype(int) // bin_size_seconds\n",
    "    sums_in_bins = np.bincount(bin_values, abs_amplitude_deriv)\n",
    "    sums_in_bins[sums_in_bins <= 0.05 * max(sums_in_bins)] = 0.0\n",
    "    return time_counts, sums_in_bins\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from scipy.signal import butter, filtfilt\n",
    "\n",
    "def build_activity_counts_te_Lindert_et_al(\n",
    "    time_xyz, axis: int = 3, prefix: str = \"\"\n",
    ") -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"Implementation of the reverse-engineered activity count algorithm from\n",
    "    te Lindert BH, Van Someren EJ. Sleep. 2013\n",
    "    Sleep estimates using microelectromechanical systems (MEMS). \n",
    "    doi: 10.5665/sleep.2648\n",
    "    \n",
    "    :param time_xyz: `np.ndarray` loaded from timestamped triaxial accelerometer CSV. Shape (N, 4)\n",
    "    :return: (time, activity counts with 15 second epoch)\n",
    "    \"\"\"\n",
    "\n",
    "    # a helper function to calculate max over 2 epochs\n",
    "    def max2epochs(data, fs, epoch):\n",
    "        data = data.flatten()\n",
    "\n",
    "        seconds = int(np.floor(np.shape(data)[0] / fs))\n",
    "        data = np.abs(data)\n",
    "        data = data[0 : int(seconds * fs)]\n",
    "\n",
    "        data = data.reshape(fs, seconds, order=\"F\").copy()\n",
    "\n",
    "        data = data.max(0)\n",
    "        data = data.flatten()\n",
    "        N = np.shape(data)[0]\n",
    "        num_epochs = int(np.floor(N / epoch))\n",
    "        data = data[0 : (num_epochs * epoch)]\n",
    "\n",
    "        data = data.reshape(epoch, num_epochs, order=\"F\").copy()\n",
    "        epoch_data = np.sum(data, axis=0)\n",
    "        epoch_data = epoch_data.flatten()\n",
    "\n",
    "        return epoch_data\n",
    "    \n",
    "    fs = 50\n",
    "    time = np.arange(np.amin(time_xyz[:, 0]), np.amax(time_xyz[:, 0]), 1.0 / fs)\n",
    "    z_data = np.interp(time, time_xyz[:, 0], time_xyz[:, axis])\n",
    "\n",
    "    cf_low = 3\n",
    "    cf_hi = 11\n",
    "    order = 5\n",
    "    w1 = cf_low / (fs / 2)\n",
    "    w2 = cf_hi / (fs / 2)\n",
    "    pass_band = [w1, w2]\n",
    "    b, a = butter(order, pass_band, \"bandpass\")\n",
    "\n",
    "    z_filt = filtfilt(b, a, z_data)\n",
    "    z_filt = np.abs(z_filt)\n",
    "\n",
    "    top_edge = 5\n",
    "    bottom_edge = 0\n",
    "    number_of_bins = 128\n",
    "\n",
    "    bin_edges = np.linspace(bottom_edge, top_edge, number_of_bins + 1)\n",
    "    binned = np.digitize(z_filt, bin_edges)\n",
    "    epoch = 15\n",
    "    counts = max2epochs(binned, fs, epoch)\n",
    "    counts = (counts - 18) * 3.07\n",
    "    counts[counts < 0] = 0\n",
    "\n",
    "    time_counts = np.linspace(np.min(time_xyz[:, 0]), max(time_xyz[:, 0]), np.shape(counts)[0])\n",
    "    time_counts = np.expand_dims(time_counts, axis=1)\n",
    "    counts = np.expand_dims(counts, axis=1)\n",
    "\n",
    "    return time_counts, counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from agcounts.extract import get_counts\n",
    "\n",
    "def build_ActiGraph_official(time_xyz, axis: int = 3) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    freq = 50\n",
    "    counts = get_counts(time_xyz[:, 1:], freq=freq, epoch=15)[:, axis - 1]\n",
    "    times = np.linspace(time_xyz[0, 0], time_xyz[-1, 0], len(counts))\n",
    "\n",
    "    return times, counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from typing import List\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def plot_scores_CDF(scores: List[float], ax: plt.Axes = None):\n",
    "    \"\"\"Plot the cumulative dist function (CDF) of the scores.\"\"\"\n",
    "    # plt.figure(figsize=(20, 10))\n",
    "    if ax is None:\n",
    "        _, ax = plt.subplots()\n",
    "    ax.set_xlim(0, 1)\n",
    "    _ = ax.hist(scores,\n",
    "                cumulative=True,\n",
    "                density=True,\n",
    "                bins=100)\n",
    "\n",
    "\n",
    "def plot_scores_PDF(scores: List[float], ax: plt.Axes = None):\n",
    "    \"\"\"Plot the probability dist function (PDF) of the scores.\"\"\"\n",
    "    ax_ = ax\n",
    "    if ax is None:\n",
    "        _, ax_ = plt.subplots()\n",
    "    ax_.set_xlim(0, 1)\n",
    "    _ = ax_.hist(scores, bins=20)\n",
    "\n",
    "    # plot the mean as a vertical 'tab:orange' line\n",
    "    ax_.axvline(np.mean(scores), color='tab:orange', linestyle='--', label=f\"Mean: {np.mean(scores):.3f}\")\n",
    "    if ax is None:\n",
    "        ax_.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def constant_interp(\n",
    "    x: np.ndarray, xp: np.ndarray, yp: np.ndarray, side: str = \"right\"\n",
    ") -> np.ndarray:\n",
    "    # constant interpolation, from https://stackoverflow.com/a/39929401/3856731\n",
    "    indices = np.searchsorted(xp, x, side=side)\n",
    "    y2 = np.concatenate(([0], yp))\n",
    "\n",
    "    return y2[indices]\n",
    "\n",
    "def avg_steps(\n",
    "    xs: List[List[float]], ys: List[List[float]]\n",
    ") -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"Computes average of step functions.\n",
    "\n",
    "    Each ys[j] is thought of as a right-continuous step function given by\n",
    "\n",
    "    `ys[j](x) = xs[j][i]`\n",
    "    for\n",
    "    `xs[j][i] <= x < xs[j][i+1]`\n",
    "\n",
    "    This function returns two NumPy arrays, `(inputs, outputs)`, giving the pointwise average\n",
    "    (see below) of these functions, one for inputs and one for outputs.\n",
    "    These output arrays can be considered to give another step function.\n",
    "\n",
    "    For a list of functions `[f_1, f_2, ..., f_n]`, their pointwise average\n",
    "    is the function `f_bar` defined by\n",
    "\n",
    "    `f_bar(x) = (1/n)(f_1(x) + f_2(x) + ... + f_n(x))`\n",
    "\n",
    "    Returns\n",
    "    ---\n",
    "    `inputs`: `np.ndaray`\n",
    "        The union of all elements of all vectors in `xs`; this is the mutual domain\n",
    "        of the average function.\n",
    "    `outputs`: `np.ndarray`\n",
    "        The pointwise average of the `ys[j]`s, considered as step functions extended\n",
    "        to the full real line by assuming constant values for `x < min(xs[j])`\n",
    "        or `x > max(xs[j])`\n",
    "    \"\"\"\n",
    "    all_xs = []\n",
    "\n",
    "    # Start by removing extraneous dims\n",
    "    xs = [np.squeeze(x) for x in xs]\n",
    "    ys = [np.squeeze(y) for y in ys]\n",
    "\n",
    "    for j in range(len(xs)):\n",
    "        x = xs[j]\n",
    "        y = ys[j]\n",
    "        # union all x-values\n",
    "        all_xs += list(x)\n",
    "\n",
    "        # ensure array values are sorted\n",
    "        x_sort = np.argsort(x)\n",
    "        xs[j] = x[x_sort]\n",
    "        ys[j] = y[x_sort]\n",
    "\n",
    "    all_xs = list(set(all_xs))\n",
    "    all_xs.sort()\n",
    "\n",
    "    all_xs = np.array(all_xs)\n",
    "\n",
    "    # Holds constant-interpolated step fns as rows (axis 0).\n",
    "    # We \"evaluate\" ys[j] for every x-value in `all_xs`\n",
    "    # Easy to average via np.mean(all_curves, axis=0)\n",
    "    all_curves = np.zeros((len(xs), len(all_xs)))\n",
    "\n",
    "    for j, (x, y) in enumerate(zip(xs, ys)):\n",
    "        x, y = np.array(x), np.array(y)\n",
    "        all_curves[j] = constant_interp(all_xs, x, y, side=\"right\")\n",
    "\n",
    "    avg_curve = np.mean(all_curves, axis=0)\n",
    "\n",
    "    return all_xs, avg_curve\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "from typing import List\n",
    "\n",
    "from sklearn.metrics import auc as auc_score\n",
    "\n",
    "def add_rocs(fprs: List[np.ndarray],\n",
    "             tprs: List[np.ndarray],\n",
    "             x_class: str = \"SLEEP\",\n",
    "             y_class: str = \"WAKE\", \n",
    "             min_auc: float = 0.0,\n",
    "             avg_curve_color: str = \"tab:blue\",\n",
    "             specific_curve_color: str = \"tab:orange\",\n",
    "             roc_group_name: str = \"\", \n",
    "             ax: plt.Axes | None = None):\n",
    "    \"\"\"\n",
    "    Adds ROC curves to the given plot, or makes a new plot if ax is None.\n",
    "\n",
    "    if ax is None, we are making a new plot. We do additional formatting\n",
    "    in this case, such as adding the legend and showing the plot. \n",
    "    \n",
    "    When `ax` is provided, we expect the call site to do formatting.\n",
    "    \"\"\"\n",
    "    # don't overwrite ax, this lets us use the None info later on \n",
    "    # to automatically show the legend and do other formatting, \n",
    "    # which otherwise we'd expect the call site to peform on `ax`\n",
    "    resolved_ax = ax if ax is not None else plt.subplots()[1]\n",
    "    aucs = np.array([\n",
    "        auc_score(fpr, tpr)\n",
    "        for fpr, tpr in zip(fprs, tprs)\n",
    "    ])\n",
    "\n",
    "    all_fprs, avg_curve = avg_steps(\n",
    "            xs=[list(fprs[i]) for i in range(len(aucs)) if aucs[i] > min_auc],\n",
    "            ys=[list(tprs[i]) for i in range(len(aucs)) if aucs[i] > min_auc],\n",
    "        )\n",
    "\n",
    "    avg_auc = np.mean(aucs[aucs > min_auc])\n",
    "\n",
    "    resolved_ax.step(\n",
    "        all_fprs,\n",
    "        avg_curve,\n",
    "        c=avg_curve_color,\n",
    "        where=\"post\",\n",
    "        label=f\"{roc_group_name + ' ' * bool(roc_group_name)}All splits avg ROC-AUC: {avg_auc:0.3f}\",\n",
    "    )\n",
    "    for roc in zip(fprs, tprs):\n",
    "        resolved_ax.step(roc[0], roc[1], c=specific_curve_color, alpha=0.2, where=\"post\")\n",
    "    resolved_ax.plot([0, 1], [0, 1], \"-.\", c=\"black\")\n",
    "\n",
    "    resolved_ax.set_ylabel(f\"Fraction of {y_class} scored as {y_class}\")\n",
    "    resolved_ax.set_xlabel(f\"Fraction of {x_class} scored as {y_class}\")\n",
    "\n",
    "    resolved_ax.spines[\"top\"].set_visible(False)\n",
    "    resolved_ax.spines[\"right\"].set_visible(False)\n",
    "\n",
    "    if ax is None:\n",
    "        # show the legend if we are making a new plot\n",
    "        # otherwise, the call site might want to make their own legend, leave it.\n",
    "        resolved_ax.legend()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "import warnings\n",
    "\n",
    "\n",
    "def pad_to_hat(y: np.ndarray, y_hat: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Adds zeros to the end of y to match the length of y_hat.\n",
    "\n",
    "    Useful when the inputs had to be padded with zeros to match shape requirements for dense layers.\n",
    "    \"\"\"\n",
    "    pad = y_hat.shape[-1] - y.shape[-1]\n",
    "    if pad < 0:\n",
    "        warnings.warn(f\"y_hat is shorter than y by {pad} elements, returning y unchanged\")\n",
    "        return y\n",
    "    y_padded = np.pad(y, (0, pad), constant_values=0)\n",
    "    return y_padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "from typing import Callable\n",
    "\n",
    "\n",
    "def mae_func(\n",
    "    func: Callable[[np.ndarray], float],\n",
    "    trues: List[np.ndarray],\n",
    "    preds: List[np.ndarray],\n",
    ") -> float:\n",
    "    \"\"\"Computes Mean Absolute Error (MAE) for the numerical function `func` on the given lists.\n",
    "\n",
    "    This function is useful for computing MAE of statistical functions giving a single float\n",
    "    for every NumPy array.\n",
    "\n",
    "    Parameters\n",
    "    ---\n",
    "    `func`: callable `(np.ndarray) -> float`\n",
    "        The statistic we are computing for truth/prediction arrays. It is called on each element\n",
    "        of the lists of NumPy arrays, then MAE of the resulting statistic lists is computed.\n",
    "    `trues`: `list` of `np.ndarray`\n",
    "        The \"True\" labels, eg. This function is symmetric in `trues` and `preds`, and isn't specific\n",
    "        to classifiers, so the argument names are just mnemonics.\n",
    "    `preds`: `list` of `np.ndarray`\n",
    "        The \"Predicted\" labels, eg.\n",
    "\n",
    "    Returns\n",
    "    ---\n",
    "    MAE of `func` applied to elements of `trues` and `preds`.\n",
    "    \"\"\"\n",
    "    assert len(trues) == len(preds)\n",
    "\n",
    "    # aes = (A)bsolute (E)rror(S)\n",
    "    # We will take the mean of this list for Mean Absolute Error\n",
    "    aes = list(\n",
    "        map(lambda ab: abs(ab[0] - ab[1]), zip(map(func, trues), map(func, preds)))\n",
    "    )\n",
    "\n",
    "    return sum(aes) / len(aes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sleep metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "from functools import partial\n",
    "\n",
    "\n",
    "class Constants:\n",
    "    # WAKE_THRESHOLD = 0.3  # These values were used for scikit-learn 0.20.3, See:\n",
    "    # REM_THRESHOLD = 0.35  # https://scikit-learn.org/stable/whats_new.html#version-0-21-0\n",
    "    WAKE_THRESHOLD = 0.5  #\n",
    "    REM_THRESHOLD = 0.35\n",
    "\n",
    "    DEFAULT_EPOCH_DURATION_IN_SECONDS = 30\n",
    "    SECONDS_PER_MINUTE = 60\n",
    "    SECONDS_PER_DAY = 3600 * 24\n",
    "    SECONDS_PER_HOUR = 3600\n",
    "    VERBOSE = True\n",
    "\n",
    "\n",
    "class SleepMetricsCalculator:\n",
    "    @staticmethod\n",
    "    def get_tst(labels, epoch_seconds: float | None = 30.0):\n",
    "        tst = np.sum(labels > 0)\n",
    "        epoch_seconds = (\n",
    "            epoch_seconds\n",
    "            if epoch_seconds is not None\n",
    "            else Constants.DEFAULT_EPOCH_DURATION_IN_SECONDS\n",
    "        )\n",
    "        return tst * epoch_seconds / Constants.SECONDS_PER_MINUTE\n",
    "\n",
    "    @staticmethod\n",
    "    def get_wake_after_sleep_onset(labels, epoch_seconds: float | None = 30.0):\n",
    "        select = labels >= 0\n",
    "        labels = labels[select]\n",
    "        sleep_indices = np.argwhere(labels > 0)\n",
    "\n",
    "        epoch_seconds = (\n",
    "            epoch_seconds\n",
    "            if epoch_seconds is not None\n",
    "            else Constants.DEFAULT_EPOCH_DURATION_IN_SECONDS\n",
    "        )\n",
    "        if np.shape(sleep_indices)[0] > 0:\n",
    "            sol_index = np.amin(sleep_indices)\n",
    "            indices_where_wake_occurred = np.where(labels == 0)\n",
    "\n",
    "            waso_indices = np.where(indices_where_wake_occurred > sol_index)\n",
    "            waso_indices = waso_indices[1]\n",
    "            number_waso_indices = np.shape(waso_indices)[0]\n",
    "            return number_waso_indices * epoch_seconds / Constants.SECONDS_PER_MINUTE\n",
    "        else:\n",
    "            # print(\"*\" * 10 + \"get_wake_after_sleep_onset\" + \"*\" * 10)\n",
    "            # print(labels)\n",
    "            return len(labels) * epoch_seconds / Constants.SECONDS_PER_MINUTE\n",
    "\n",
    "    @staticmethod\n",
    "    def get_sleep_efficiency(labels):\n",
    "        sleep_indices = np.where(labels > 0)\n",
    "        sleep_efficiency = float(np.shape(sleep_indices)[1]) / float(\n",
    "            np.shape(labels)[0]\n",
    "        )\n",
    "        return sleep_efficiency\n",
    "\n",
    "    @staticmethod\n",
    "    def get_sleep_onset_latency(labels, epoch_seconds: Optional[float]):\n",
    "        sleep_indices = np.argwhere(labels > 0)\n",
    "        epoch_seconds = (\n",
    "            epoch_seconds\n",
    "            if epoch_seconds is not None\n",
    "            else Constants.DEFAULT_EPOCH_DURATION_IN_SECONDS\n",
    "        )\n",
    "        if np.shape(sleep_indices)[0] > 0:\n",
    "            return np.amin(sleep_indices) * epoch_seconds / Constants.SECONDS_PER_MINUTE\n",
    "        else:\n",
    "            return len(labels) * epoch_seconds / Constants.SECONDS_PER_MINUTE\n",
    "\n",
    "    @staticmethod\n",
    "    def get_time_in_rem(labels, epoch_seconds: Optional[float]):\n",
    "        rem_epoch_indices = np.where(labels == 2)\n",
    "        rem_time = np.shape(rem_epoch_indices)[1]\n",
    "        return rem_time * epoch_seconds / Constants.SECONDS_PER_MINUTE\n",
    "\n",
    "    @staticmethod\n",
    "    def get_time_in_nrem(labels, epoch_seconds: Optional[float]):\n",
    "        rem_epoch_indices = np.where(labels == 1)\n",
    "        rem_time = np.shape(rem_epoch_indices)[1]\n",
    "        return rem_time * epoch_seconds / Constants.SECONDS_PER_MINUTE\n",
    "\n",
    "    @classmethod\n",
    "    def report_mae_tst_waso(\n",
    "        cls,\n",
    "        y_pred_y_true: List[Tuple[np.ndarray, np.ndarray]],\n",
    "        sleep_acc: float = 0.93,\n",
    "        epoch_seconds: Optional[float] = 30,\n",
    "    ) -> Dict[str, float]:\n",
    "        res = {\"mae_tst_minutes\": [], \"mae_waso_minutes\": []}\n",
    "        preds = []\n",
    "        trues = []\n",
    "        for pred, true in y_pred_y_true:\n",
    "            fprs, tprs, thresholds = roc_curve(true, pred)\n",
    "            threshold = thresholds[np.argmax(fprs <= (1 - sleep_acc))]\n",
    "            preds.append(pred >= threshold)\n",
    "            trues.append(true)\n",
    "\n",
    "        tst_func = partial(cls.get_tst, epoch_seconds=epoch_seconds)\n",
    "        waso_func = partial(cls.get_wake_after_sleep_onset, epoch_seconds=epoch_seconds)\n",
    "        res[\"mae_tst_minutes\"] = mae_func(tst_func, trues=trues, preds=preds)\n",
    "        res[\"mae_waso_minutes\"] = mae_func(waso_func, trues=trues, preds=preds)\n",
    "\n",
    "        return res\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split analysis tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from sklearn.metrics import roc_auc_score, roc_curve, cohen_kappa_score\n",
    "\n",
    "WASA_THRESHOLD = 0.93\n",
    "BALANCE_WEIGHTS = True\n",
    "\n",
    "def split_analysis(y, y_hat_sleep_proba, sleep_accuracy: float = WASA_THRESHOLD, balancing: bool = BALANCE_WEIGHTS):\n",
    "\n",
    "    y_flat = y.reshape(-1,)\n",
    "    n_sleep = np.sum(y_flat > 0)\n",
    "    n_wake = np.sum(y_flat == 0)\n",
    "    N = n_sleep + n_wake\n",
    "\n",
    "    balancing_weights_ignore_mask = np.where(y_flat > 0, N / n_sleep, N / n_wake) \\\n",
    "        if balancing else np.ones_like(y_flat)\n",
    "    balancing_weights_ignore_mask /= np.sum(balancing_weights_ignore_mask) # sums to 1.0\n",
    "\n",
    "    # adjust y to match the lenght of y_hat, which was padded to fit model constraints\n",
    "    y_padded = pad_to_hat(y_flat, y_hat_sleep_proba)\n",
    "    # make a mask to ignore the padded values, so they aren't counted against us\n",
    "    mask = pad_to_hat(balancing_weights_ignore_mask, y_hat_sleep_proba)\n",
    "\n",
    "    # also ignore any unscored or missing values.\n",
    "    y_to_score = pad_to_hat(y_flat >= 0, y_hat_sleep_proba)\n",
    "    mask *= y_to_score\n",
    "    # roc_auc will complain if -1 is in y_padded\n",
    "    y_padded *= y_to_score \n",
    "\n",
    "    # ROC analysis\n",
    "    fprs, tprs, thresholds = roc_curve(y_padded, y_hat_sleep_proba, sample_weight=mask)\n",
    "\n",
    "    # Sleep accuracy = (n sleep correct) / (n sleep) = TP/AP = TPR\n",
    "    wasa_threshold = thresholds[np.sum(tprs <= sleep_accuracy)]\n",
    "    y_guess = y_hat_sleep_proba > wasa_threshold\n",
    "\n",
    "    # # WASA X\n",
    "    guess_right = y_guess == y_padded\n",
    "    y_wake = y_padded == 0\n",
    "    wake_accuracy = np.sum(y_wake * guess_right * y_to_score) / np.sum(n_wake)\n",
    "     \n",
    "    return {\n",
    "        \"y_padded\": y_padded,\n",
    "        \"y_hat\": y_hat_sleep_proba,\n",
    "        \"mask\": mask,\n",
    "        \"kappa\": cohen_kappa_score(y_padded, y_guess, sample_weight=mask),\n",
    "        \"auc\": roc_auc_score(y_padded, y_hat_sleep_proba, sample_weight=mask),\n",
    "        \"roc_curve\": {\"tprs\": tprs,\n",
    "                      \"fprs\": fprs,\n",
    "                      \"thresholds\": thresholds\n",
    "        }, \n",
    "        f\"wasa{int(100 * sleep_accuracy)}_threshold\": wasa_threshold,\n",
    "        f\"wasa{int(100 * sleep_accuracy)}\": wake_accuracy, \n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev\n",
    "nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
