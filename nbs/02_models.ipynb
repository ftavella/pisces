{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data sets and models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide \n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *\n",
    "from fastcore.basics import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import sys\n",
    "import abc\n",
    "import keras\n",
    "import warnings\n",
    "import numpy as np\n",
    "import polars as pl\n",
    "from tqdm import tqdm\n",
    "import multiprocessing\n",
    "from io import StringIO\n",
    "from typing import Type\n",
    "from itertools import repeat\n",
    "from scipy.special import softmax\n",
    "from typing import Dict, List, Tuple\n",
    "from sklearn.pipeline import Pipeline\n",
    "from pisces.mads_olsen_support import *\n",
    "from pisces.data_sets import DataSetObject\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.model_selection import LeaveOneOut\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "from pisces.data_sets import DataSetObject, ModelInput1D, ModelInputSpectrogram, ModelOutputType, DataProcessor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifier models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class SleepWakeClassifier(abc.ABC):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    @abc.abstractmethod\n",
    "    def get_needed_X_y(self, data_set: DataSetObject, id: str) -> Tuple[np.ndarray, np.ndarray] | None:\n",
    "        pass\n",
    "    def train(self, examples_X: List[pl.DataFrame] = [], examples_y: List[pl.DataFrame] = [], \n",
    "              pairs_Xy: List[Tuple[pl.DataFrame, pl.DataFrame]] = [], \n",
    "              epochs: int = 10, batch_size: int = 32):\n",
    "        pass\n",
    "    def predict(self, sample_X: np.ndarray | pl.DataFrame) -> np.ndarray:\n",
    "        pass\n",
    "    def predict_probabilities(self, sample_X: np.ndarray | pl.DataFrame) -> np.ndarray:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SGD Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class SGDLogisticRegression(SleepWakeClassifier):\n",
    "    \"\"\"Uses Sk-Learn's `SGDCLassifier` to train a logistic regression model. The SGD aspect allows for online learning, or custom training regimes through the `partial_fit` method.\n",
    "     \n",
    "    The model is trained with a balanced class weight, and uses L1 regularization. The input data is scaled with a `StandardScaler` before being passed to the model.\n",
    "    \"\"\"\n",
    "    def __init__(self, \n",
    "                 data_processor: DataProcessor, \n",
    "                 lr: float = 0.15, \n",
    "                 epochs: int = 100,):\n",
    "        self.model = SGDClassifier(loss='log_loss',\n",
    "                                   learning_rate='adaptive',\n",
    "                                   penalty='l1',\n",
    "                                   eta0=lr,\n",
    "                                   class_weight='balanced',\n",
    "                                   max_iter=epochs,\n",
    "                                   warm_start=True,\n",
    "                                   verbose=1)\n",
    "        self.scaler = StandardScaler()\n",
    "        self.pipeline = Pipeline([('scaler', self.scaler), ('model', self.model)])\n",
    "        if not isinstance(data_processor.model_input, ModelInput1D):\n",
    "            raise ValueError(\"Model input must be set to 1D on the data processor\")\n",
    "        if not data_processor.output_type == ModelOutputType.SLEEP_WAKE:\n",
    "            raise ValueError(\"Model output must be set to SleepWake on the data processor\")\n",
    "        self.data_processor = data_processor\n",
    "\n",
    "    def get_needed_X_y(self, id: str) -> Tuple[np.ndarray, np.ndarray] | None:\n",
    "        return self.data_processor.get_1D_X_y(id)\n",
    "\n",
    "    def train(self, \n",
    "              examples_X: List[np.ndarray]=[], \n",
    "              examples_y: List[np.ndarray]=[], \n",
    "              pairs_Xy: List[Tuple[np.ndarray, np.ndarray]]=[],\n",
    "              epochs: int = 10,\n",
    "              ):\n",
    "        \"\"\"\n",
    "        Assumes data is already preprocessed using `get_needed_X_y` \n",
    "        and ready to be passed to the model.\n",
    "\n",
    "        Returns the loss history of the model.\n",
    "        \"\"\"\n",
    "        if (examples_X and not examples_y) or (examples_y and not examples_X):\n",
    "            raise ValueError(\"If providing examples, must provide both X and y\")\n",
    "        else:\n",
    "            if examples_X and examples_y:\n",
    "                assert len(examples_X) == len(examples_y)\n",
    "        if pairs_Xy:\n",
    "            assert not examples_X\n",
    "            examples_X = [pair[0] for pair in pairs_Xy]\n",
    "            examples_y = [pair[1] for pair in pairs_Xy]\n",
    "\n",
    "\n",
    "        Xs = np.concatenate(examples_X, axis=0)\n",
    "        ys = np.concatenate(examples_y, axis=0)\n",
    "\n",
    "        selector = ys >= 0\n",
    "        Xs = Xs[selector]\n",
    "        ys = ys[selector]\n",
    "\n",
    "        loss_list = []\n",
    "        old_stdout = sys.stdout\n",
    "        sys.stdout = mystdout = StringIO()\n",
    "        self.pipeline.fit(Xs, ys) # Fit the model\n",
    "        sys.stdout = old_stdout\n",
    "        loss_history = mystdout.getvalue()\n",
    "        # Get loss\n",
    "        try:\n",
    "            for line in loss_history.split('\\n'):\n",
    "                if(len(line.split(\"loss: \")) == 1):\n",
    "                    continue\n",
    "                loss_list.append(float(line.split(\"loss: \")[-1]))\n",
    "        except:\n",
    "            warnings.warn(\"Failed to fetch loss history. Returning empty list.\")\n",
    "\n",
    "        return loss_list\n",
    "    \n",
    "    def _input_preprocessing(self, X: np.ndarray) -> np.ndarray:\n",
    "        return self.scaler.transform(X)\n",
    "    \n",
    "    def predict(self, sample_X: np.ndarray | pl.DataFrame) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Assumes data is already preprocessed using `get_needed_X_y`\n",
    "        \"\"\"\n",
    "        return self.model.predict(self._input_preprocessing(sample_X))\n",
    "    \n",
    "    def predict_probabilities(self, sample_X: np.ndarray | pl.DataFrame) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Assumes data is already preprocessed using `get_needed_X_y`\n",
    "        \"\"\"\n",
    "        return self.model.predict_proba(self._input_preprocessing(sample_X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mads Olsen et all classifier\n",
    "\n",
    "We have downloaded the saved model weights from a [research repository from Mads Olsen's group](https://github.com/MADSOLSEN/SleepStagePrediction), and converted those into a saved Keras model to remove the need to re-define all of the layers. This conversion process is shown in `../analyses/convert_mads_olsen_model_to_keras.ipynb`.\n",
    "\n",
    "Thus, we have a TensorFlow model that we can run inference on, and we could train it if we wanted to.\n",
    "\n",
    "For simplicity, we are just going to run inference. One twist of our method is that the classifier is expecting two high-resolution spectrograms for inputs:\n",
    "1. 3-axis Accelerometer data\n",
    "2. PPG (photoplethysmogram) data\n",
    "\n",
    "Based on visually inspecting examples from the paper, we are going to hack together an input by flipping the accelerometer data along the frequencies axis. The paper images seem to show a similarity between high-frequency accelerometer data and low-frequency PPG data. Surprisingly, this seems to work well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\tavel\\Desktop\\Internship-Arcascope\\2024_internship\\.venv\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\core.py:187: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#| export\n",
    "class MOResUNetPretrained(SleepWakeClassifier):\n",
    "    tf_model = load_saved_keras()\n",
    "    config = MO_PREPROCESSING_CONFIG\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        sampling_hz: int = FS,\n",
    "        tf_model: keras.Model = None,\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Initialize the MOResUNetPretrained classifier.\n",
    "\n",
    "        Args:\n",
    "            sampling_hz (int, optional): The sampling frequency in Hz. Defaults to FS.\n",
    "            tf_model (keras.Model, optional): The TensorFlow model to use. Defaults to None, in which case the model is loaded from disk.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.sampling_hz = sampling_hz\n",
    "        self._tf_model = tf_model\n",
    "\n",
    "    @property\n",
    "    def tf_model(self) -> keras.Model:\n",
    "        if self._tf_model is None:\n",
    "            self._tf_model = load_saved_keras()\n",
    "        return self._tf_model\n",
    "\n",
    "    def prepare_set_for_training(self, \n",
    "                                 data_processor: DataProcessor, \n",
    "                                 ids: List[str],\n",
    "                                 max_workers: int | None = None \n",
    "                                 ) -> List[Tuple[np.ndarray, np.ndarray] | None]:\n",
    "        \"\"\"\n",
    "        Prepare the data set for training.\n",
    "\n",
    "        Args:\n",
    "            data_set (DataSetObject): The data set to prepare for training.\n",
    "            ids (List[str], optional): The IDs to prepare. Defaults to None.\n",
    "            max_workers (int, optional): The number of workers to use for parallel processing. Defaults to None, which uses all available cores. Setting to a negative number leaves that many cores unused. For example, if my machine has 4 cores and I set max_workers to -1, then 3 = 4 - 1 cores will be used; if max_workers=-3 then 1 = 4 - 3 cores are used.\n",
    "\n",
    "        Returns:\n",
    "            List[Tuple[np.ndarray, np.ndarray] | None]: A list of tuples, where each tuple is the result of `get_needed_X_y` for a given ID. An empty list indicates an error occurred during processing.\n",
    "        \"\"\"\n",
    "        if not isinstance(data_processor.model_input, ModelInputSpectrogram):\n",
    "            raise ValueError(\"Model input must be set to Spectrogram on the data processor\")\n",
    "\n",
    "        results = []\n",
    "        \n",
    "        # Get the number of available CPU cores\n",
    "        num_cores = multiprocessing.cpu_count()\n",
    "        workers_to_use = max_workers if max_workers is not None else num_cores\n",
    "        if (workers_to_use > num_cores):\n",
    "            warnings.warn(f\"Attempting to use {max_workers} but only have {num_cores}. Running with {num_cores} workers.\")\n",
    "            workers_to_use = num_cores\n",
    "        if workers_to_use <= 0:\n",
    "            workers_to_use = num_cores + max_workers\n",
    "        if workers_to_use < 1:\n",
    "            # do this check second, NOT with elif, to verify we're still in a valid state\n",
    "            raise ValueError(f\"With `max_workers` == {max_workers}, we end up with max_workers + num_cores ({max_workers} + {num_cores}) which is less than 1. This is an error.\")\n",
    "\n",
    "        print(f\"Using {workers_to_use} of {num_cores} cores ({int(100 * workers_to_use / num_cores)}%) for parallel preprocessing.\")\n",
    "        print(f\"This can cause memory or heat issues if  is too high; if you run into problems, call prepare_set_for_training() again with max_workers = -1, going more negative if needed. (See the docstring for more info.)\")\n",
    "        # Create a pool of workers\n",
    "        with ProcessPoolExecutor(max_workers=workers_to_use) as executor:\n",
    "            results = list(\n",
    "                tqdm(\n",
    "                    executor.map(\n",
    "                        self.get_needed_X_y,\n",
    "                        repeat(data_processor),\n",
    "                        ids,\n",
    "                    ), total=len(ids), desc=\"Preparing data...\"\n",
    "                ))\n",
    "\n",
    "        return results\n",
    "\n",
    "    def get_needed_X_y(self, data_processor: DataProcessor, id: str) -> Tuple[np.ndarray, np.ndarray] | None:\n",
    "        return data_processor.get_spectrogram_X_y(id)\n",
    "\n",
    "    def train(self, \n",
    "              examples_X: List[pl.DataFrame] = [], \n",
    "              examples_y: List[pl.DataFrame] = [], \n",
    "              pairs_Xy: List[Tuple[pl.DataFrame, pl.DataFrame]] = [], \n",
    "              lr: float = 1e-5, validation_split: float = 0.1,\n",
    "              epochs: int = 10, batch_size: int = 1,):\n",
    "        \"\"\"\n",
    "        Trains the associated Keras model.\n",
    "        \"\"\"\n",
    "        if examples_X or examples_y:\n",
    "            assert len(examples_X) == len(examples_y)\n",
    "        if pairs_Xy:\n",
    "            assert not examples_X\n",
    "\n",
    "        training = []\n",
    "        training_iterator = iter(pairs_Xy) if pairs_Xy else zip(examples_X, examples_y)\n",
    "        for X, y in training_iterator:\n",
    "            try:\n",
    "                y_reshaped = np.pad(\n",
    "                    y.reshape(1, -1), \n",
    "                    pad_width=[\n",
    "                        (0, 0), # axis 0, no padding\n",
    "                        (0, N_OUT - y.shape[0]), # axis 1, pad to N_OUT from mads_olsen_support\n",
    "                    ],\n",
    "                    mode='constant', \n",
    "                    constant_values=0) \n",
    "                sample_weights = y_reshaped >= 0\n",
    "                training.append((X, y_reshaped, sample_weights))\n",
    "            except Exception as e:\n",
    "                print(f\"Error folding or trimming data: {e}\")\n",
    "                continue\n",
    "\n",
    "        Xs = [X for X, _, _ in training]\n",
    "        ys = [y for _, y, _ in training]\n",
    "        weights = [w for _, _, w in training]\n",
    "        Xs_c = np.concatenate(Xs, axis=0)\n",
    "        ys_c = np.concatenate(ys, axis=0)\n",
    "        weights = np.concatenate(weights, axis=0)\n",
    "\n",
    "        self.tf_model.compile(\n",
    "            optimizer=keras.optimizers.RMSprop(learning_rate=lr), \n",
    "            loss=keras.losses.SparseCategoricalCrossentropy(),\n",
    "            metrics=[keras.metrics.SparseCategoricalAccuracy()],\n",
    "            weighted_metrics=[])\n",
    "\n",
    "        fit_result = self.tf_model.fit(\n",
    "            Xs_c, ys_c * weights, batch_size=batch_size, epochs=epochs,\n",
    "            sample_weight=weights, validation_split=validation_split,)\n",
    "\n",
    "        return fit_result\n",
    "\n",
    "    def predict(self, sample_X: np.ndarray | pl.DataFrame) -> np.ndarray:\n",
    "        return np.argmax(self.predict_probabilities(sample_X), axis=1)\n",
    "\n",
    "    def predict_probabilities(self, sample_X: np.ndarray | pl.DataFrame) -> np.ndarray:\n",
    "        if isinstance(sample_X, pl.DataFrame):\n",
    "            sample_X = sample_X.to_numpy()\n",
    "        return softmax(self._evaluate_tf_model(sample_X)[0], axis=1)\n",
    "\n",
    "    def _evaluate_tf_model(self, inputs: np.ndarray) -> np.ndarray:\n",
    "        inputs = inputs.astype(np.float32)\n",
    "        preds = self.tf_model.predict(inputs)\n",
    "        return preds\n",
    "\n",
    "    def evaluate_data_set(self, \n",
    "                          data_processor: DataProcessor, \n",
    "                          exclude: List[str] = [], \n",
    "                          max_workers: int = None) -> Tuple[Dict[str, dict], list]:\n",
    "        data_set = data_processor.data_set\n",
    "        filtered_ids = [id for id in data_set.ids if id not in exclude]\n",
    "        # Prepare the data\n",
    "        print(\"Preprocessing data...\")\n",
    "        mo_preprocessed_data = [\n",
    "            (d, i) \n",
    "            for (d, i) in zip(\n",
    "                self.prepare_set_for_training(data_processor, filtered_ids, max_workers=max_workers),\n",
    "                filtered_ids) \n",
    "            if d is not None\n",
    "        ]\n",
    "\n",
    "        print(\"Evaluating data set...\")\n",
    "        evaluations: Dict[str, dict] = {}\n",
    "        for _, ((X, y_true), id) in tqdm(enumerate(mo_preprocessed_data)):\n",
    "            y_prob = self.predict_probabilities(X)\n",
    "            m = keras.metrics.SparseCategoricalAccuracy()\n",
    "            # Remove masked values\n",
    "            selector = y_true >= 0\n",
    "            y_true_filtered = y_true[selector]\n",
    "            y_prob_filtered = y_prob[selector]\n",
    "            # Calculate sample weights\n",
    "            unique, counts = np.unique(y_true_filtered, return_counts=True)\n",
    "            class_weights = dict(zip(unique, counts))\n",
    "            inv_class_weights = {k: 1.0 / v for k, v in class_weights.items()}\n",
    "            min_weight = min(inv_class_weights.values())\n",
    "            normalized_weights = {k: v / min_weight for k, v in inv_class_weights.items()}\n",
    "            sample_weights = np.array([normalized_weights[class_id] for class_id in y_true_filtered])\n",
    "            # Sparse categorical accuracy\n",
    "            y_true_reshaped = y_true_filtered.reshape(-1, 1)\n",
    "            m.update_state(y_true_reshaped, y_prob_filtered, sample_weight=sample_weights)\n",
    "            accuracy = m.result().numpy()\n",
    "            evaluations[id] = {\n",
    "                'sparse_categorical_accuracy': accuracy,\n",
    "            }\n",
    "\n",
    "        return evaluations, mo_preprocessed_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class SplitMaker:\n",
    "    def split(self, ids: List[str]) -> Tuple[List[int], List[int]]:\n",
    "        raise NotImplementedError\n",
    "    \n",
    "class LeaveOneOutSplitter(SplitMaker):\n",
    "    def split(self, ids: List[str]) -> Tuple[List[int], List[int]]:\n",
    "        loo = LeaveOneOut()\n",
    "        return loo.split(ids)\n",
    "\n",
    "\n",
    "def run_split(train_indices, \n",
    "              preprocessed_data_set: List[Tuple[np.ndarray, np.ndarray]], \n",
    "              swc: SleepWakeClassifier,\n",
    "              epochs: int) -> SleepWakeClassifier:\n",
    "    training_pairs = [\n",
    "        preprocessed_data_set[i][0]\n",
    "        for i in train_indices\n",
    "        if preprocessed_data_set[i][0] is not None\n",
    "    ]\n",
    "    result = swc.train(pairs_Xy=training_pairs, epochs=epochs)\n",
    "\n",
    "    return swc, result\n",
    "\n",
    "\n",
    "def run_splits(split_maker: SplitMaker, \n",
    "               data_processor: DataProcessor, \n",
    "               swc_class: Type[SleepWakeClassifier], \n",
    "               epochs: int,\n",
    "               exclude: List[str] = [],\n",
    "               ) -> Tuple[\n",
    "                   List[SleepWakeClassifier], \n",
    "                   List[np.ndarray], \n",
    "                   List[List[List[int]]] \n",
    "                   ]:\n",
    "    split_models: List[swc_class] = []\n",
    "    test_indices = []\n",
    "    split_results = []\n",
    "    splits = []\n",
    "\n",
    "    swc = swc_class(data_processor, epochs=epochs)\n",
    "\n",
    "    ids_to_split = [id for id in data_processor.data_set.ids if id not in exclude]\n",
    "    tqdm_message_preprocess = f\"Preparing data for {len(ids_to_split)} IDs\"\n",
    "    preprocessed_data = [(swc.get_needed_X_y(id), id) for id in tqdm(ids_to_split, desc=tqdm_message_preprocess)]\n",
    "\n",
    "    tqdm_message_train = f\"Training {len(ids_to_split)} splits\"\n",
    "    all_splits = split_maker.split(ids_to_split)\n",
    "    for train_index, test_index in tqdm(all_splits, desc=tqdm_message_train, total=len(ids_to_split)):\n",
    "        if preprocessed_data[test_index[0]][0] is None:\n",
    "            continue\n",
    "        model, result = run_split(train_indices=train_index, \n",
    "                                  preprocessed_data_set=preprocessed_data, \n",
    "                                  swc=swc_class(data_processor, epochs=epochs), \n",
    "                                  epochs=epochs)\n",
    "        split_models.append(model)\n",
    "        split_results.append(result)\n",
    "        test_indices.append(test_index[0])\n",
    "        splits.append([train_index, test_index])\n",
    "    \n",
    "    return split_models, split_results, preprocessed_data, splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev\n",
    "nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
