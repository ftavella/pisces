{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data sets and models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide \n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *\n",
    "from fastcore.basics import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-13 16:14:40.596948: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-08-13 16:14:40.604644: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:479] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-08-13 16:14:40.616260: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:10575] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-08-13 16:14:40.616276: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1442] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-08-13 16:14:40.623557: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-08-13 16:14:41.061482: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "/home/eric/miniconda3/envs/pisces2/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "#| export\n",
    "import sys\n",
    "import abc\n",
    "import keras\n",
    "import warnings\n",
    "import numpy as np\n",
    "import polars as pl\n",
    "from enum import Enum\n",
    "from tqdm import tqdm\n",
    "import multiprocessing\n",
    "from io import StringIO\n",
    "from typing import Type\n",
    "from itertools import repeat\n",
    "from scipy.special import softmax\n",
    "from fastcore.basics import patch_to\n",
    "from typing import Dict, List, Tuple\n",
    "from sklearn.pipeline import Pipeline\n",
    "from pisces.mads_olsen_support import *\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.model_selection import LeaveOneOut\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from pisces.mads_olsen_support import load_saved_keras\n",
    "from pisces.data_sets import ModelInput1D, ModelInputSpectrogram, ModelOutputType, DataProcessor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifier models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class SleepWakeClassifier:\n",
    "    \"\"\" Abstract class for sleep/wake classifiers. \n",
    "    \"\"\"\n",
    "    def __init__(self, model=None, data_processor=None,\n",
    "                 scaler_pipeline_name: str='scaler', \n",
    "                 model_pipeline_name: str='model'):\n",
    "        self.model = model\n",
    "        self.scaler_pipeline_name = scaler_pipeline_name\n",
    "        self.model_pipeline_name = model_pipeline_name\n",
    "        self.pipeline = Pipeline([(scaler_pipeline_name, StandardScaler()), \n",
    "                                  (model_pipeline_name, self.model)])\n",
    "        self.data_processor = data_processor\n",
    "\n",
    "    def _input_preprocessing(self, X: np.ndarray) -> np.ndarray:\n",
    "        return self.pipeline.named_steps[self.scaler_pipeline_name].transform(X)\n",
    "\n",
    "    def predict(self, sample_X: np.ndarray | pl.DataFrame) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Assumes data is already preprocessed using `get_needed_X_y`\n",
    "        \"\"\"\n",
    "        return self.pipeline.predict(self._input_preprocessing(sample_X))\n",
    "\n",
    "    def predict_probabilities(self, sample_X: np.ndarray | pl.DataFrame) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Assumes data is already preprocessed using `get_needed_X_y`\n",
    "        \"\"\"\n",
    "        model = self.pipeline.named_steps[self.model_pipeline_name]\n",
    "        if hasattr(model, 'predict_proba'):\n",
    "            return model.predict_proba(self._input_preprocessing(sample_X))\n",
    "        elif hasattr(model, 'decision_function'):\n",
    "            warnings.warn(\"Model does not have `predict_proba`. Using `decision_function` instead.\")\n",
    "            binary_decision = model.decision_function(self._input_preprocessing(sample_X))\n",
    "            prediction = binary_decision > 0\n",
    "            return np.vstack([1 - prediction, prediction]).T\n",
    "\n",
    "        else:\n",
    "            raise ValueError(\"Model must have either `predict_proba` or `decision_function`\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def train_pipeline(classifier: SleepWakeClassifier,\n",
    "                   examples_X: List[np.ndarray]=[],\n",
    "                   examples_y: List[np.ndarray]=[],\n",
    "                   pairs_Xy: List[Tuple[np.ndarray, np.ndarray]]=[],\n",
    "                   **train_kwargs) -> List[float]:\n",
    "    \"\"\"\n",
    "    Assumes data is already preprocessed using `get_needed_X_y` \n",
    "    and ready to be passed to the classifier.\n",
    "\n",
    "    Returns the loss history of the model.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    classifier : SleepWakeClassifier\n",
    "        The classifier object.\n",
    "    examples_X : List[np.ndarray]\n",
    "        List of input examples. If non-empty, then `examples_y` must also be provided and must have the same length.\n",
    "    examples_y : List[np.ndarray]\n",
    "        List of target labels. If non-empty, then `examples_X` must also be provided and must have the same length.\n",
    "    pairs_Xy : List[Tuple[np.ndarray, np.ndarray]]\n",
    "        List of input-target pairs. If non-empty, then `examples_X` and `examples_y` must not be provided.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    List[float]\n",
    "        The loss history of the model.\n",
    "    \"\"\"\n",
    "    if pairs_Xy:\n",
    "        assert (not examples_X) and (not examples_y)\n",
    "        examples_X = [pair[0] for pair in pairs_Xy]\n",
    "        examples_y = [pair[1] for pair in pairs_Xy]\n",
    "    elif examples_X and not examples_y: \n",
    "        raise ValueError(\"Provided examples_X but not examples_y\")\n",
    "    elif examples_y and not examples_X:\n",
    "        raise ValueError(\"Provided examples_y but not examples_X\")\n",
    "    else:\n",
    "        # we know that examples_X and examples_y are both truthy, hence non-empty lists\n",
    "        assert (len(examples_X) == len(examples_y))\n",
    "\n",
    "\n",
    "    Xs = np.concatenate(examples_X, axis=0)\n",
    "    ys = np.concatenate(examples_y, axis=0)\n",
    "    print(f\"Training on {len(Xs)} examples\")\n",
    "\n",
    "    selector = ys >= 0\n",
    "\n",
    "    # adds \"model__sample_weight\" to train_kwargs, but if it already exists, it will not overwrite it\n",
    "    # the order of the dictionaries in | important, as the rightmost dictionary will overwrite the leftmost\n",
    "    train_kwargs = {classifier.model_pipeline_name + '__sample_weight': selector} \\\n",
    "        | train_kwargs\n",
    "\n",
    "    loss_list = []\n",
    "    old_stdout = sys.stdout\n",
    "    sys.stdout = mystdout = StringIO()\n",
    "    # multiply ys by selector, to zero-out the \"-1\" masked values but leave the others unchanged (where selector == 1)\n",
    "    classifier.pipeline.fit(Xs, ys * selector, **train_kwargs) # Fit the model\n",
    "    print(\"Done fitting\")\n",
    "    sys.stdout = old_stdout\n",
    "    loss_history = mystdout.getvalue()\n",
    "    print(loss_history)\n",
    "    # Get loss\n",
    "    try:\n",
    "        for line in loss_history.split('\\n'):\n",
    "            if(len(line.split(\"loss: \")) == 1):\n",
    "                continue\n",
    "            loss_list.append(float(line.split(\"loss: \")[-1]))\n",
    "    except:\n",
    "        warnings.warn(\"Failed to fetch loss history. Returning empty list.\")\n",
    "\n",
    "    return loss_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch_to(SleepWakeClassifier)\n",
    "def train(self, examples_X: List[np.ndarray]=[], \n",
    "          examples_y: List[np.ndarray]=[], \n",
    "          pairs_Xy: List[Tuple[np.ndarray, np.ndarray]]=[],\n",
    "          **training_kwargs\n",
    "          ):\n",
    "    \"\"\"\n",
    "    Assumes data is already preprocessed using `get_needed_X_y` \n",
    "    and ready to be passed to the model.\n",
    "\n",
    "    Returns the loss history of the model.\n",
    "    \"\"\"\n",
    "    loss_list = train_pipeline(self, examples_X, examples_y, pairs_Xy, **training_kwargs)\n",
    "    return loss_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SGD Linear Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class LinearModel(Enum):\n",
    "    \"\"\"Defines the loss used in sklearn's SGDClassifier which defines the linear model used for classification.\"\"\"\n",
    "    LOGISTIC_REGRESSION = 'log_loss'\n",
    "    PERCEPTRON = 'perceptron'\n",
    "    SVM = 'hinge'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class SGDLinearClassifier(SleepWakeClassifier):\n",
    "    \"\"\"Uses Sk-Learn's `SGDCLassifier` to train a model. Possible models are logistic regression, perceptron, and SVM.\n",
    "    The SGD aspect allows for online learning, or custom training regimes through the `partial_fit` method.\n",
    "    The model is trained with a balanced class weight, and uses L1 regularization. The input data is scaled with a `StandardScaler` before being passed to the model.\n",
    "    \"\"\"\n",
    "    def __init__(self, \n",
    "                 data_processor: DataProcessor | None = None, \n",
    "                 linear_model: LinearModel=LinearModel.LOGISTIC_REGRESSION,\n",
    "                 **kwargs):\n",
    "        if data_processor is not None:\n",
    "            if not isinstance(data_processor.model_input, ModelInput1D):\n",
    "                raise ValueError(\"Model input must be set to 1D on the data processor\")\n",
    "            if not data_processor.output_type == ModelOutputType.SLEEP_WAKE:\n",
    "                raise ValueError(\"Model output must be set to SleepWake on the data processor\")\n",
    "        super().__init__(\n",
    "            model=SGDClassifier(loss=linear_model.value, **kwargs),\n",
    "            data_processor=data_processor\n",
    "        )\n",
    "\n",
    "    def get_needed_X_y(self, id: str) -> Tuple[np.ndarray, np.ndarray] | None:\n",
    "        return self.data_processor.get_1D_X_y(id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class RandomForest(SleepWakeClassifier):\n",
    "    \"\"\"Interface for sklearn's RandomForestClassifier\"\"\"\n",
    "    def __init__(self,\n",
    "                 data_processor: DataProcessor | None = None,\n",
    "                 class_weight: str = 'balanced',\n",
    "                 **kwargs):\n",
    "        if data_processor is not None:\n",
    "            if not isinstance(data_processor.model_input, ModelInput1D):\n",
    "                raise ValueError(\"Model input must be set to 1D on the data processor\")\n",
    "        super().__init__(\n",
    "            model=RandomForestClassifier(class_weight=class_weight, **kwargs),\n",
    "            data_processor=data_processor\n",
    "        )\n",
    "\n",
    "    def get_needed_X_y(self, id: str) -> Tuple[np.ndarray, np.ndarray] | None:\n",
    "        return self.data_processor.get_1D_X_y(id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mads Olsen et all classifier\n",
    "\n",
    "We have downloaded the saved model weights from a [research repository from Mads Olsen's group](https://github.com/MADSOLSEN/SleepStagePrediction), and converted those into a saved Keras model to remove the need to re-define all of the layers. This conversion process is shown in `../analyses/convert_mads_olsen_model_to_keras.ipynb`.\n",
    "\n",
    "Thus, we have a TensorFlow model that we can run inference on, and we could train it if we wanted to.\n",
    "\n",
    "For simplicity, we are just going to run inference. One twist of our method is that the classifier is expecting two high-resolution spectrograms for inputs:\n",
    "1. 3-axis Accelerometer data\n",
    "2. PPG (photoplethysmogram) data\n",
    "\n",
    "Based on visually inspecting examples from the paper, we are going to hack together an input by flipping the accelerometer data along the frequencies axis. The paper images seem to show a similarity between high-frequency accelerometer data and low-frequency PPG data. Surprisingly, this seems to work well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class MOResUNetPretrained(SleepWakeClassifier):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        data_processor: DataProcessor | None = None,\n",
    "        model: keras.Model | None = None,\n",
    "        lazy_model_loading: bool = True,\n",
    "        initial_lr: float = 1e-5,\n",
    "        validation_split: float = 0.1,\n",
    "        epochs: int = 10,\n",
    "        batch_size: int = 1,\n",
    "        **kwargs) -> None:\n",
    "        \"\"\"\n",
    "        Initialize the MOResUNetPretrained classifier.\n",
    "\n",
    "        Args:\n",
    "            data_processor (DataProcessor, optional): The data processor to use.\n",
    "            model (keras.Model, optional): The TensorFlow model to use. Defaults to None, in which case the model is loaded from disk.\n",
    "        \"\"\"\n",
    "        if data_processor is not None:\n",
    "            if not isinstance(data_processor.model_input, ModelInputSpectrogram):\n",
    "                raise ValueError(\"Model input must be set to Spectrogram on the data processor\")\n",
    "\n",
    "        if model is None and not lazy_model_loading:\n",
    "            tf_model = load_saved_keras()\n",
    "        else:\n",
    "            tf_model = model\n",
    "\n",
    "\n",
    "        super().__init__(\n",
    "            model=tf_model,\n",
    "            data_processor=data_processor,\n",
    "        )\n",
    "\n",
    "        if self.model is not None:\n",
    "            self.model.compile(\n",
    "                optimizer=keras.optimizers.RMSprop(learning_rate=initial_lr), \n",
    "                loss=keras.losses.SparseCategoricalCrossentropy(),\n",
    "                metrics=[keras.metrics.SparseCategoricalAccuracy()],\n",
    "                weighted_metrics=[])\n",
    "        \n",
    "        # set up training params using the named step format of a pipeline.fit **kwargs\n",
    "        self.training_params = {\n",
    "            f'{self.model_pipeline_name}__validation_split': validation_split,\n",
    "            f'{self.model_pipeline_name}__epochs': epochs,\n",
    "            f'{self.model_pipeline_name}__batch_size': batch_size\n",
    "        }\n",
    "\n",
    "        self.pipeline = Pipeline([\n",
    "            (self.model_pipeline_name, self.model)\n",
    "        ])\n",
    "\n",
    "    def prepare_set_for_training(self, \n",
    "                                 ids: List[str],\n",
    "                                 max_workers: int | None = None \n",
    "                                 ) -> List[Tuple[np.ndarray, np.ndarray] | None]:\n",
    "        \"\"\"\n",
    "        Prepare the data set for training.\n",
    "\n",
    "        Args:\n",
    "            ids (List[str], optional): The IDs to prepare. Defaults to None.\n",
    "            max_workers (int, optional): The number of workers to use for parallel processing. Defaults to None, which uses all available cores. Setting to a negative number leaves that many cores unused. For example, if my machine has 4 cores and I set max_workers to -1, then 3 = 4 - 1 cores will be used; if max_workers=-3 then 1 = 4 - 3 cores are used.\n",
    "\n",
    "        Returns:\n",
    "            List[Tuple[np.ndarray, np.ndarray] | None]: A list of tuples, where each tuple is the result of `get_needed_X_y` for a given ID. An empty list indicates an error occurred during processing.\n",
    "        \"\"\"\n",
    "        results = []\n",
    "        \n",
    "        # Get the number of available CPU cores\n",
    "        num_cores = multiprocessing.cpu_count()\n",
    "        workers_to_use = max_workers if max_workers is not None else num_cores\n",
    "        if (workers_to_use > num_cores):\n",
    "            warnings.warn(f\"Attempting to use {max_workers} but only have {num_cores}. Running with {num_cores} workers.\")\n",
    "            workers_to_use = num_cores\n",
    "        if workers_to_use <= 0:\n",
    "            workers_to_use = num_cores + max_workers\n",
    "        if workers_to_use < 1:\n",
    "            # do this check second, NOT with elif, to verify we're still in a valid state\n",
    "            raise ValueError(f\"With `max_workers` == {max_workers}, we end up with f{max_workers + num_cores} ({max_workers} + {num_cores}) which is less than 1. This is an error.\")\n",
    "\n",
    "        print(f\"Using {workers_to_use} of {num_cores} cores ({int(100 * workers_to_use / num_cores)}%) for parallel preprocessing.\")\n",
    "        print(f\"This can cause memory or heat issues if  is too high; if you run into problems, call prepare_set_for_training() again with max_workers = -1, going more negative if needed. (See the docstring for more info.)\")\n",
    "        # Create a pool of workers\n",
    "        with ProcessPoolExecutor(max_workers=workers_to_use) as executor:\n",
    "            results = list(\n",
    "                tqdm(\n",
    "                    executor.map(\n",
    "                        self.get_needed_X_y,\n",
    "                        ids,\n",
    "                        repeat(self.data_processor),\n",
    "                    ), total=len(ids), desc=\"Preparing data...\"\n",
    "                ))\n",
    "\n",
    "        return results\n",
    "\n",
    "    def get_needed_X_y(self, id: str, data_processor: DataProcessor) -> Tuple[np.ndarray, np.ndarray] | None:\n",
    "        return data_processor.get_spectrogram_X_y(id)\n",
    "\n",
    "    def predict(self, sample_X: np.ndarray | pl.DataFrame) -> np.ndarray:\n",
    "        return np.argmax(self.predict_probabilities(sample_X), axis=1)\n",
    "\n",
    "    def predict_probabilities(self, sample_X: np.ndarray | pl.DataFrame) -> np.ndarray:\n",
    "        if isinstance(sample_X, pl.DataFrame):\n",
    "            sample_X = sample_X.to_numpy()\n",
    "        return softmax(self._evaluate_tf_model(sample_X)[0], axis=1)\n",
    "\n",
    "    def _evaluate_tf_model(self, inputs: np.ndarray) -> np.ndarray:\n",
    "        inputs = inputs.astype(np.float32)\n",
    "        preds = self.model.predict(inputs)\n",
    "        return preds\n",
    "\n",
    "    def evaluate_data_set(self, \n",
    "                          exclude: List[str] = [], \n",
    "                          max_workers: int = None) -> Tuple[Dict[str, dict], list]:\n",
    "        data_set = self.data_processor.data_set\n",
    "        filtered_ids = [id for id in data_set.ids if id not in exclude]\n",
    "        # Prepare the data\n",
    "        print(\"Preprocessing data...\")\n",
    "        mo_preprocessed_data = [\n",
    "            (d, i) \n",
    "            for (d, i) in zip(\n",
    "                self.prepare_set_for_training(filtered_ids, max_workers=max_workers),\n",
    "                filtered_ids) \n",
    "            if d is not None\n",
    "        ]\n",
    "\n",
    "        print(\"Evaluating data set...\")\n",
    "        evaluations: Dict[str, dict] = {}\n",
    "        for _, ((X, y_true), id) in tqdm(enumerate(mo_preprocessed_data)):\n",
    "            y_prob = self.predict_probabilities(X)\n",
    "            m = keras.metrics.SparseCategoricalAccuracy()\n",
    "            # Remove masked values\n",
    "            selector = y_true >= 0\n",
    "            y_true_filtered = y_true[selector]\n",
    "            y_prob_filtered = y_prob[selector]\n",
    "            # Calculate sample weights\n",
    "            unique, counts = np.unique(y_true_filtered, return_counts=True)\n",
    "            class_weights = dict(zip(unique, counts))\n",
    "            inv_class_weights = {k: 1.0 / v for k, v in class_weights.items()}\n",
    "            min_weight = min(inv_class_weights.values())\n",
    "            normalized_weights = {k: v / min_weight for k, v in inv_class_weights.items()}\n",
    "            sample_weights = np.array([normalized_weights[class_id] for class_id in y_true_filtered])\n",
    "            # Sparse categorical accuracy\n",
    "            y_true_reshaped = y_true_filtered.reshape(-1, 1)\n",
    "            m.update_state(y_true_reshaped, y_prob_filtered, sample_weight=sample_weights)\n",
    "            accuracy = m.result().numpy()\n",
    "            evaluations[id] = {\n",
    "                'sparse_categorical_accuracy': accuracy,\n",
    "            }\n",
    "\n",
    "        return evaluations, mo_preprocessed_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class SplitMaker:\n",
    "    def split(self, ids: List[str]) -> Tuple[List[int], List[int]]:\n",
    "        raise NotImplementedError\n",
    "    \n",
    "class LeaveOneOutSplitter(SplitMaker):\n",
    "    def split(self, ids: List[str]) -> Tuple[List[int], List[int]]:\n",
    "        loo = LeaveOneOut()\n",
    "        return loo.split(ids)\n",
    "\n",
    "\n",
    "def run_split(train_indices, \n",
    "              preprocessed_data_set: List[Tuple[np.ndarray, np.ndarray]], \n",
    "              swc: SleepWakeClassifier,\n",
    "              epochs: int) -> SleepWakeClassifier:\n",
    "    training_pairs = [\n",
    "        [preprocessed_data_set[i][0][0], preprocessed_data_set[i][0][1].reshape(1, -1)]\n",
    "        for i in train_indices\n",
    "        if preprocessed_data_set[i][0] is not None\n",
    "    ]\n",
    "    if isinstance(swc, MOResUNetPretrained):\n",
    "        extra_params = {\n",
    "            f'{swc.model_pipeline_name}__epochs': epochs,\n",
    "            f'{swc.model_pipeline_name}__batch_size': 1,\n",
    "            f'{swc.model_pipeline_name}__validation_split': 0.1\n",
    "        }\n",
    "        result = swc.train(pairs_Xy=training_pairs, **extra_params)\n",
    "    else:\n",
    "        result = swc.train(pairs_Xy=training_pairs)\n",
    "\n",
    "    return swc, result\n",
    "\n",
    "\n",
    "def run_splits(split_maker: SplitMaker, \n",
    "               data_processor: DataProcessor, \n",
    "               swc_class: Type[SleepWakeClassifier], \n",
    "               epochs: int | None,\n",
    "               exclude: List[str] = [],\n",
    "               linear_model: LinearModel | None = None,\n",
    "               ) -> Tuple[\n",
    "                   List[SleepWakeClassifier], \n",
    "                   List[np.ndarray], \n",
    "                   List[List[List[int]]] \n",
    "                   ]:\n",
    "    split_models: List[swc_class] = []\n",
    "    test_indices = []\n",
    "    split_results = []\n",
    "    splits = []\n",
    "\n",
    "    if swc_class == SGDLinearClassifier and not linear_model:\n",
    "        raise ValueError(\"Must provide a linear model for SGDLinearClassifier\")\n",
    "    elif not swc_class == SGDLinearClassifier and linear_model:\n",
    "        raise ValueError(\"Linear model provided but not using SGDLinearClassifier\")\n",
    "\n",
    "    swc = swc_class(data_processor, linear_model=linear_model, epochs=epochs)\n",
    "\n",
    "    ids_to_split = [id for id in data_processor.data_set.ids if id not in exclude]\n",
    "    tqdm_message_preprocess = f\"Preparing data for {len(ids_to_split)} IDs\"\n",
    "    preprocessed_data = [(swc.get_needed_X_y(id), id) for id in tqdm(ids_to_split, desc=tqdm_message_preprocess)]\n",
    "\n",
    "    tqdm_message_train = f\"Training {len(ids_to_split)} splits\"\n",
    "    all_splits = split_maker.split(ids_to_split)\n",
    "    for train_index, test_index in tqdm(all_splits, desc=tqdm_message_train, total=len(ids_to_split)):\n",
    "        if preprocessed_data[test_index[0]][0] is None:\n",
    "            continue\n",
    "        if swc_class == SGDLinearClassifier:\n",
    "            swc = swc_class(data_processor, linear_model, \n",
    "                            epochs=epochs)\n",
    "        elif swc_class == RandomForest:\n",
    "            swc = swc_class(data_processor)\n",
    "        else:\n",
    "            swc = swc_class(data_processor, epochs=epochs)\n",
    "\n",
    "        model, result = run_split(train_indices=train_index, \n",
    "                                  preprocessed_data_set=preprocessed_data, \n",
    "                                  swc=swc,\n",
    "                                  epochs=epochs)\n",
    "        split_models.append(model)\n",
    "        split_results.append(result)\n",
    "        test_indices.append(test_index[0])\n",
    "        splits.append([train_index, test_index])\n",
    "    \n",
    "    return split_models, split_results, preprocessed_data, splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev\n",
    "nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
