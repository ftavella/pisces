{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data sets and models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide \n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import polars as pl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifier models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import abc\n",
    "from enum import Enum\n",
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.pipeline import make_pipeline\n",
    "import numpy as np\n",
    "\n",
    "from pisces.data_sets import DataSetObject\n",
    "\n",
    "class SleepClassifierMode(Enum):\n",
    "    BINARY = 1\n",
    "    MULTICLASS = 2\n",
    "\n",
    "\n",
    "class SleepWakeClassifier(abc.ABC):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    @abc.abstractmethod\n",
    "    def get_needed_X_y(self, data_set: DataSetObject, id: str) -> Tuple[np.ndarray, np.ndarray] | None:\n",
    "        pass\n",
    "    def train(self, examples_X: List[pl.DataFrame] = [], examples_y: List[pl.DataFrame] = [], \n",
    "              pairs_Xy: List[Tuple[pl.DataFrame, pl.DataFrame]] = [], \n",
    "              epochs: int = 10, batch_size: int = 32):\n",
    "        pass\n",
    "    def predict(self, sample_X: np.ndarray | pl.DataFrame) -> np.ndarray:\n",
    "        pass\n",
    "    def predict_probabilities(self, sample_X: np.ndarray | pl.DataFrame) -> np.ndarray:\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SGD Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "from pisces.data_sets import get_activity_X_PSG_y, rolling_window\n",
    "\n",
    "\n",
    "class SGDLogisticRegression(SleepWakeClassifier):\n",
    "    \"\"\"Uses Sk-Learn's `SGDCLassifier` to train a logistic regression model. The SGD aspect allows for online learning, or custom training regimes through the `partial_fit` method.\n",
    "     \n",
    "    The model is trained with a balanced class weight, and uses L1 regularization. The input data is scaled with a `StandardScaler` before being passed to the model.\n",
    "    \"\"\"\n",
    "    def __init__(self, lr: float = 0.15, input_dim: int = 11, output_dim: int = 1):\n",
    "        self.model = SGDClassifier(loss='log_loss',\n",
    "                                   learning_rate='adaptive',\n",
    "                                   penalty='l2',\n",
    "                                   eta0=lr,\n",
    "                                   class_weight='balanced',\n",
    "                                   warm_start=True)\n",
    "        self.scaler = StandardScaler()\n",
    "        self.pipeline = make_pipeline(self.scaler, self.model)\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.window_step = 1\n",
    "\n",
    "    def get_needed_X_y(self, data_set: DataSetObject, id: str) -> Tuple[np.ndarray, np.ndarray] | None:\n",
    "        return get_activity_X_PSG_y(data_set, id)\n",
    "    \n",
    "    def _prepare_labels(self, y: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        y_trimmed = self._trim_labels(y)\n",
    "        n_sleep = np.sum(y_trimmed > 0)\n",
    "        n_wake = np.sum(y_trimmed == 0)\n",
    "        N = n_sleep + n_wake\n",
    "        # Want to make a balanced weight loss, along with giving 0.0 * loss for masked values (y < 0)\n",
    "        mask_weights_zero = np.where(y_trimmed < 0, 0.0, 1.0)\n",
    "        # balancing_weights_ignore_mask = np.where(y_trimmed > 0, n_wake / N, n_sleep / N)\n",
    "        balancing_weights_ignore_mask = np.where(y_trimmed > 0, N / n_sleep, N / n_wake)\n",
    "        sample_weights = mask_weights_zero * balancing_weights_ignore_mask\n",
    "\n",
    "        y_demasked = np.where(y_trimmed < 0, 0, y_trimmed)\n",
    "\n",
    "        return y_demasked, sample_weights\n",
    "\n",
    "    def train(self, examples_X: List[pl.DataFrame] = [], examples_y: List[pl.DataFrame] = [], \n",
    "              pairs_Xy: List[Tuple[pl.DataFrame, pl.DataFrame]] = [], \n",
    "              epochs: int = 10, batch_size: int = 32):\n",
    "        if examples_X or examples_y:\n",
    "            assert len(examples_X) == len(examples_y)\n",
    "        if pairs_Xy:\n",
    "            assert not examples_X\n",
    "        \n",
    "        training = []\n",
    "        training_iterator = iter(pairs_Xy) if pairs_Xy else zip(examples_X, examples_y)\n",
    "        for X, y in training_iterator:\n",
    "            try:\n",
    "                X_folded = self._fold(X)\n",
    "                (y_prepped, sample_weights) = self._prepare_labels(y)\n",
    "                if (X_folded.shape[0] == 0) \\\n",
    "                    or (y_prepped.shape[0] == 0):\n",
    "                    continue\n",
    "                if (X_folded.shape[0] != y_prepped.shape[0]):\n",
    "                    # trim to match\n",
    "                    smaller = min(X_folded.shape[0], y_prepped.shape[0])\n",
    "                    X_folded = X_folded[:smaller]\n",
    "                    y_prepped = y_prepped[:smaller]\n",
    "                    sample_weights = sample_weights[:smaller]\n",
    "                training.append((X_folded, y_prepped, sample_weights))\n",
    "            except Exception as e:\n",
    "                print(f\"Error folding or trimming data: {e}\")\n",
    "                continue\n",
    "        \n",
    "        Xs = [X for X, _, _ in training]\n",
    "        ys = [y for _, y, _ in training]\n",
    "        weights = [w for _, _, w in training]\n",
    "        Xs = np.concatenate(Xs, axis=0)\n",
    "        ys = np.concatenate(ys, axis=0)\n",
    "        weights = np.concatenate(weights, axis=0)\n",
    "\n",
    "        selector = ys >= 0\n",
    "        Xs = Xs[selector]\n",
    "        ys = ys[selector]\n",
    "        weights = weights[selector]\n",
    "\n",
    "        # self.pipeline.fit(Xs, ys, sgdclassifier__sample_weight=weights)\n",
    "        # balance weights is on for \"sgdclassifier\" step\n",
    "        self.pipeline.fit(Xs, ys)\n",
    "    \n",
    "    def _input_preprocessing(self, X: np.ndarray) -> np.ndarray:\n",
    "        return self.scaler.transform(self._fold(X))\n",
    "    \n",
    "    def predict(self, sample_X: np.ndarray | pl.DataFrame) -> np.ndarray:\n",
    "        return self.model.predict(self._input_preprocessing(sample_X))\n",
    "    \n",
    "    def predict_probabilities(self, sample_X: np.ndarray | pl.DataFrame) -> np.ndarray:\n",
    "        return self.model.predict_proba(self._input_preprocessing(sample_X))\n",
    "    \n",
    "    def _fold(self, input_X: np.ndarray | pl.DataFrame) -> np.ndarray:\n",
    "        if isinstance(input_X, pl.DataFrame):\n",
    "            input_X = input_X.to_numpy()\n",
    "        return rolling_window(input_X, self.input_dim)\n",
    "    \n",
    "    def _trim_labels(self, labels_y: pl.DataFrame) -> np.ndarray:\n",
    "        start, end = self._indices_to_trim()\n",
    "        # return labels_y[self.input_dim:]\n",
    "        return labels_y[start:-end]\n",
    "        \n",
    "    def _indices_to_trim(self) -> Tuple[int, int]:\n",
    "        # ex: input_dim = 8 => (4, 3)\n",
    "        # ex: input_dim = 7 => (3, 3)\n",
    "        # ex: input_dim = 6 => (3, 2)\n",
    "        return (self.input_dim // 2, self.input_dim - (self.input_dim // 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mads Olsen et all classifier\n",
    "\n",
    "We have downloaded the saved model weights from a [research repository from Mads Olsen's group](https://github.com/MADSOLSEN/SleepStagePrediction), and converted those into a saved Keras model to remove the need to re-define all of the layers. This conversion process is shown in `../analyses/convert_mads_olsen_model_to_keras.ipynb`.\n",
    "\n",
    "Thus, we have a TensorFlow model that we can run inference on, and we could train it if we wanted to.\n",
    "\n",
    "For simplicity, we are just going to run inference. One twist of our method is that the classifier is expecting two high-resolution spectrograms for inputs:\n",
    "1. 3-axis Accelerometer data\n",
    "2. PPG (photoplethysmogram) data\n",
    "\n",
    "Based on visually inspecting examples from the paper, we are going to hack together an input by flipping the accelerometer data along the frequencies axis. The paper images seem to show a similarity between high-frequency accelerometer data and low-frequency PPG data. Surprisingly, this seems to work well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from functools import partial\n",
    "import multiprocessing\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "import warnings\n",
    "\n",
    "import keras\n",
    "\n",
    "from pisces.mads_olsen_support import *\n",
    "from pisces.data_sets import fill_gaps_in_accelerometer_data, mask_psg_from_accel, psg_to_WLDM, psg_to_sleep_wake\n",
    "from pisces.utils import split_analysis\n",
    "\n",
    "\n",
    "class MOResUNetPretrained(SleepWakeClassifier):\n",
    "    config = MO_PREPROCESSING_CONFIG\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        sampling_hz: int = FS,\n",
    "        tf_model: keras.Model | None = None,\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Initialize the MOResUNetPretrained classifier.\n",
    "\n",
    "        Args:\n",
    "            sampling_hz (int, optional): The sampling frequency in Hz. Defaults to FS.\n",
    "            tf_model (keras.Model | None, optional): The TensorFlow model to use. Defaults to None, which will load the saved model.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self._tf_model = tf_model\n",
    "        self.sampling_hz = sampling_hz\n",
    "    \n",
    "    @property\n",
    "    def tf_model(self) -> keras.Model:\n",
    "        if self._tf_model is None:\n",
    "            self._tf_model = load_saved_keras()\n",
    "        return self._tf_model\n",
    "\n",
    "    def prepare_set_for_training(self, \n",
    "                                 data_set: DataSetObject, ids: List[str] | None = None,\n",
    "                                 max_workers: int | None = None,\n",
    "                                 N4: bool = True,\n",
    "                                 ) -> List[Tuple[np.ndarray, np.ndarray] | None]:\n",
    "        \"\"\"\n",
    "        Prepare the data set for training.\n",
    "\n",
    "        Args:\n",
    "            data_set (DataSetObject): The data set to prepare for training.\n",
    "            ids (List[str], optional): The IDs to prepare. Defaults to None.\n",
    "            max_workers (int, optional): The number of workers to use for parallel processing. Defaults to None, which uses all available cores. Setting to a negative number leaves that many cores unused. For example, if my machine has 4 cores and I set max_workers to -1, then 3 = 4 - 1 cores will be used; if max_workers=-3 then 1 = 4 - 3 cores are used.\n",
    "            N4 (bool, optional): Whether the data contains N4 stage or not. Defaults to True.\n",
    "\n",
    "        Returns:\n",
    "            List[Tuple[np.ndarray, np.ndarray] | None]: A list of tuples, where each tuple is the result of `get_needed_X_y` for a given ID. An empty list indicates an error occurred during processing.\n",
    "        \"\"\"\n",
    "        if ids is None:\n",
    "            ids = data_set.ids\n",
    "        results = []\n",
    "        \n",
    "        if ids:\n",
    "            data_tuples = [(data_set, id, N4) for id in ids]\n",
    "            # Get the number of available CPU cores\n",
    "            num_cores = multiprocessing.cpu_count()\n",
    "            workers_to_use = max_workers if max_workers is not None else num_cores\n",
    "            if (workers_to_use > num_cores):\n",
    "                warnings.warn(f\"Attempting to use {max_workers} but only have {num_cores}. Running with {num_cores} workers.\")\n",
    "                workers_to_use = num_cores\n",
    "            if workers_to_use <= 0:\n",
    "                workers_to_use = num_cores + max_workers\n",
    "            if workers_to_use < 1:\n",
    "                # do this check second, NOT with elif, to verify we're still in a valid state\n",
    "                raise ValueError(f\"With `max_workers` == {max_workers}, we end up with max_workers + num_cores ({max_workers} + {num_cores}) which is less than 1. This is an error.\")\n",
    "\n",
    "            print(f\"Using {workers_to_use} of {num_cores} cores ({int(100 * workers_to_use / num_cores)}%) for parallel preprocessing.\")\n",
    "            print(f\"This can cause memory or heat issues if  is too high; if you run into problems, call prepare_set_for_training() again with max_workers = -1, going more negative if needed. (See the docstring for more info.)\")\n",
    "\n",
    "            # Create a pool of workers\n",
    "            with ProcessPoolExecutor(max_workers=workers_to_use) as executor:\n",
    "                results = list(\n",
    "                    executor.map(\n",
    "                        self.get_needed_X_y_from_tuple, \n",
    "                        data_tuples\n",
    "                    ))\n",
    "        else:\n",
    "            warnings.warn(\"No IDs found in the data set.\")\n",
    "            return results\n",
    "        return results\n",
    "    \n",
    "    def get_needed_X_y_from_tuple(self, data_tuple: Tuple[DataSetObject, str, bool]) -> Tuple[np.ndarray, np.ndarray] | None:\n",
    "        \"\"\"\n",
    "        Get the needed X and y data from a pair of data set and ID.\n",
    "\n",
    "        Args:\n",
    "            data_tuple (Tuple[DataSetObject, str]): A tuple containing the data set, id, and N4 flag.\n",
    "\n",
    "        Returns:\n",
    "            Tuple[np.ndarray, np.ndarray] | None: The X and y data as a tuple, or None if an error occurred.\n",
    "        \"\"\"\n",
    "        data_set, id, N4 = data_tuple\n",
    "        print(f\"getting needed X, y for {id}\")\n",
    "        return self.get_needed_X_y(data_set, id, N4)\n",
    "    \n",
    "    def get_needed_X_y(self, data_set: DataSetObject, id: str, N4: bool = True) -> Tuple[np.ndarray, np.ndarray] | None:\n",
    "        accelerometer = data_set.get_feature_data(\"accelerometer\", id)\n",
    "        psg = data_set.get_feature_data(\"psg\", id)\n",
    "\n",
    "        if accelerometer is None or psg is None:\n",
    "            print(f\"ID {id} {'psg' if psg is None else 'accelerometer'} not found in {data_set.name}\")\n",
    "            return None\n",
    "        \n",
    "        print(\"sampling hz:\", self.sampling_hz)\n",
    "        psg = mask_psg_from_accel(psg, accelerometer)\n",
    "        accelerometer = fill_gaps_in_accelerometer_data(accelerometer, smooth=False, final_sampling_rate_hz=self.sampling_hz)\n",
    "        stop_time = min(accelerometer[:, 0].max(), psg[:, 0].max())\n",
    "        accelerometer = accelerometer.filter(accelerometer[:, 0] <= stop_time)\n",
    "        psg = psg.filter(psg[:, 0] <= stop_time)\n",
    "        start_time = max(accelerometer[:, 0].min(), psg[:, 0].min())\n",
    "        accelerometer = accelerometer.filter(accelerometer[:, 0] >= start_time)\n",
    "        psg = psg.filter(psg[:, 0] >= start_time)\n",
    "\n",
    "        mirrored_spectro = self._input_preprocessing(accelerometer)\n",
    "\n",
    "        # return mirrored_spectro, psg_to_sleep_wake(psg)\n",
    "        return mirrored_spectro, psg_to_WLDM(psg, N4)\n",
    "\n",
    "    def train(self, \n",
    "              examples_X: List[pl.DataFrame] = [], \n",
    "              examples_y: List[pl.DataFrame] = [], \n",
    "              pairs_Xy: List[Tuple[pl.DataFrame, pl.DataFrame]] = [], \n",
    "              epochs: int = 10, \n",
    "              batch_size: int = 1):\n",
    "        \"\"\"\n",
    "        Trains the associated Keras model.\n",
    "        \"\"\"\n",
    "        if examples_X or examples_y:\n",
    "            assert len(examples_X) == len(examples_y)\n",
    "        if pairs_Xy:\n",
    "            assert not examples_X\n",
    "        \n",
    "        training = []\n",
    "        training_iterator = iter(pairs_Xy) if pairs_Xy else zip(examples_X, examples_y)\n",
    "        for X, y in training_iterator:\n",
    "            try:\n",
    "                y_reshaped = np.pad(\n",
    "                    y.reshape(1, -1), \n",
    "                    pad_width=[\n",
    "                        (0, 0), # axis 0, no padding\n",
    "                        (0, N_OUT - y.shape[0]), # axis 1, pad to N_OUT from mads_olsen_support\n",
    "                    ],\n",
    "                    mode='constant', \n",
    "                    constant_values=0) \n",
    "                sample_weights = y_reshaped >= 0\n",
    "                training.append((X, y_reshaped, sample_weights))\n",
    "            except Exception as e:\n",
    "                print(f\"Error folding or trimming data: {e}\")\n",
    "                continue\n",
    "        \n",
    "        Xs = [X for X, _, _ in training]\n",
    "        ys = [y for _, y, _ in training]\n",
    "        weights = [w for _, _, w in training]\n",
    "        Xs_c = np.concatenate(Xs, axis=0)\n",
    "        ys_c = np.concatenate(ys, axis=0)\n",
    "        weights = np.concatenate(weights, axis=0)\n",
    "\n",
    "        self.tf_model.compile(\n",
    "            optimizer=keras.optimizers.RMSprop(learning_rate=1e-5), \n",
    "            loss=keras.losses.SparseCategoricalCrossentropy(),\n",
    "            metrics=[keras.metrics.SparseCategoricalAccuracy()],\n",
    "            weighted_metrics=[])\n",
    "        fit_result = self.tf_model.fit(\n",
    "            Xs_c, \n",
    "            ys_c * weights,\n",
    "            batch_size=batch_size,\n",
    "            epochs=epochs,\n",
    "            sample_weight=weights,\n",
    "            validation_split=0.1)\n",
    "        return fit_result\n",
    "\n",
    "    def predict(self, sample_X: np.ndarray | pl.DataFrame) -> np.ndarray:\n",
    "        return np.argmax(self.predict_probabilities(sample_X), axis=1)\n",
    "\n",
    "    def predict_probabilities(self, sample_X: np.ndarray | pl.DataFrame) -> np.ndarray:\n",
    "        if isinstance(sample_X, pl.DataFrame):\n",
    "            sample_X = sample_X.to_numpy()\n",
    "        return self._evaluate_tf_model(sample_X)\n",
    "\n",
    "    def roc_curve(self, examples_X_y: Tuple[np.ndarray, np.ndarray]) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        raise NotImplementedError\n",
    "    def roc_auc(self, examples_X_y: Tuple[np.ndarray, np.ndarray]) -> float:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    @classmethod\n",
    "    def _spectrogram_preprocessing(cls, acc_xyz: np.ndarray) -> np.ndarray:\n",
    "        return cls._preprocessing(acc_xyz)\n",
    "\n",
    "    @classmethod\n",
    "    def _input_preprocessing(\n",
    "        cls,\n",
    "        acc_xyz: pl.DataFrame | np.ndarray\n",
    "    ) -> np.ndarray:\n",
    "\n",
    "        spec = cls._spectrogram_preprocessing(acc_xyz)\n",
    "\n",
    "        # We will copy the spectrogram to both channels, flipping it on channel 1\n",
    "        input_shape = (1, *MO_UNET_CONFIG['input_shape'])\n",
    "        inputs_len = input_shape[1]\n",
    "\n",
    "        inputs = np.zeros(shape=input_shape, dtype=np.float32)\n",
    "        # We must do some careful work with indices to not overflow arrays\n",
    "        spec = spec[:inputs_len].astype(np.float32) # protect agains spec.len > input_shape\n",
    "\n",
    "        #! careful, order matters here. We first trim spec to make sure it'll fit into inputs,\n",
    "        # then compute the new length which we KNOW is <= inputs_len\n",
    "        spec_len = spec.shape[0]\n",
    "        # THEN we assign only as much inputs as spec covers\n",
    "        inputs[0, : spec_len, :, 0] = spec # protect agains spec_len < input_shape\n",
    "        inputs[0, : spec_len, :, 1] = spec[:, ::-1]\n",
    "\n",
    "        return inputs\n",
    "\n",
    "    def _evaluate_tf_model(self, inputs: np.ndarray) -> np.ndarray:\n",
    "        # set input tensor to FLOAT32\n",
    "        inputs = inputs.astype(np.float32)\n",
    "\n",
    "        # run inference\n",
    "        preds = self.tf_model.predict(inputs)\n",
    "\n",
    "        return preds\n",
    "    \n",
    "    @classmethod\n",
    "    def _preprocessing(\n",
    "        cls,\n",
    "        acc: pl.DataFrame | np.ndarray\n",
    "    ) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        The Mads Olsen repo uses a list of transformations\n",
    "        \"\"\"\n",
    "        if isinstance(acc, pl.DataFrame):\n",
    "            acc = acc.to_numpy()\n",
    "        x_ = acc[:, 0]\n",
    "        y_ = acc[:, 1]\n",
    "        z_ = acc[:, 2]\n",
    "        for step in cls.config[\"preprocessing\"]:\n",
    "            fn = eval(step[\"type\"])  # convert string version to function in environment\n",
    "            fn_args = partial(\n",
    "                fn, **step[\"args\"]\n",
    "            )  # fill in the args given, which must be everything besides numerical input\n",
    "\n",
    "            # apply\n",
    "            x_ = fn_args(x_)\n",
    "            y_ = fn_args(y_)\n",
    "            z_ = fn_args(z_)\n",
    "\n",
    "        spec = x_ + y_ + z_\n",
    "        spec /= 3.0\n",
    "\n",
    "        return spec\n",
    "\n",
    "    def evaluate_data_set(self, data_set: DataSetObject, exclude: List[str] = [], max_workers: int = None) -> Tuple[Dict[str, dict], list]:\n",
    "        filtered_ids = [id for id in data_set.ids if id not in exclude]\n",
    "        mo_preprocessed_data = [\n",
    "            (d, i) \n",
    "            for (d, i) in zip(\n",
    "                self.prepare_set_for_training(data_set, filtered_ids, max_workers=max_workers),\n",
    "                filtered_ids) \n",
    "            if d is not None\n",
    "        ]\n",
    "\n",
    "        evaluations: Dict[str, dict] = {}\n",
    "        # TODO: fix for staging evaluation\n",
    "        # for i, ((X, y), id) in enumerate(mo_preprocessed_data):\n",
    "        #     y_hat_proba = self.predict_probabilities(X)\n",
    "        #     y_hat_sleep_proba = (1 - y_hat_proba[:, :, 0]).reshape(-1,)\n",
    "        #     analysis = split_analysis(y, y_hat_sleep_proba)\n",
    "        #     evaluations[id] = analysis\n",
    "        #     print(f\"Processing {i+1} of {len(mo_preprocessed_data)} ({id})... AUROC: {analysis['auc']}\")\n",
    "        return evaluations, mo_preprocessed_data\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from typing import Type\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import LeaveOneOut\n",
    "\n",
    "\n",
    "class SplitMaker:\n",
    "    def split(self, ids: List[str]) -> Tuple[List[int], List[int]]:\n",
    "        raise NotImplementedError\n",
    "    \n",
    "class LeaveOneOutSplitter(SplitMaker):\n",
    "    def split(self, ids: List[str]) -> Tuple[List[int], List[int]]:\n",
    "        loo = LeaveOneOut()\n",
    "        return loo.split(ids)\n",
    "\n",
    "def run_split(train_indices, \n",
    "              preprocessed_data_set: List[Tuple[np.ndarray, np.ndarray]], \n",
    "              swc: SleepWakeClassifier,\n",
    "              epochs: int) -> SleepWakeClassifier:\n",
    "    training_pairs = [\n",
    "        preprocessed_data_set[i][0]\n",
    "        for i in train_indices\n",
    "        if preprocessed_data_set[i][0] is not None\n",
    "    ]\n",
    "    train_result = swc.train(pairs_Xy=training_pairs, epochs=epochs)\n",
    "\n",
    "    return swc, train_result\n",
    "\n",
    "def run_splits(split_maker: SplitMaker, w: DataSetObject, \n",
    "               swc_class: Type[SleepWakeClassifier], \n",
    "               exclude: List[str] = [],\n",
    "               preprocessed_data: List[np.ndarray] | None = None,\n",
    "               epochs: int = 10) -> Tuple[\n",
    "        List[SleepWakeClassifier], \n",
    "        List[np.ndarray],\n",
    "        List[List[List[int]]]]:\n",
    "    split_models: List[swc_class] = []\n",
    "    test_indices = []\n",
    "    splits = []\n",
    "    ids_to_split = [\n",
    "        i for i in w.ids if i not in exclude\n",
    "    ]\n",
    "    train_results = []\n",
    "\n",
    "    preprocessed_data = [(swc_class().get_needed_X_y(w, i), i) for i in ids_to_split] \\\n",
    "        if preprocessed_data is None else preprocessed_data\n",
    "\n",
    "    # for train_index, test_index in tqdm(split_maker.split(ids_to_split)):\n",
    "    for train_index, test_index in tqdm(split_maker.split(preprocessed_data)):\n",
    "        if preprocessed_data[test_index[0]][0] is None:\n",
    "            continue\n",
    "        model, train_result = run_split(train_indices=train_index, \n",
    "                                        preprocessed_data_set=preprocessed_data, \n",
    "                                        swc=swc_class(), \n",
    "                                        epochs=epochs)\n",
    "        split_models.append(model)\n",
    "        test_indices.append(test_index[0])\n",
    "        splits.append([train_index, test_index])\n",
    "        train_results.append(train_result)\n",
    "        # try:\n",
    "        #     model = run_split(train_indices=train_index,\n",
    "        #                     preprocessed_data_set=preprocessed_data,\n",
    "        #                     swc=swc_class())\n",
    "        #     split_models.append(model)\n",
    "        #     test_indices.append(test_index[0])\n",
    "        #     splits.append([train_index, test_index])\n",
    "        # except Exception as e:\n",
    "        #     print(f\"Training failed for {ids_to_split[test_index[0]]}\")\n",
    "    \n",
    "    return split_models, preprocessed_data, splits, train_results\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev\n",
    "nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
