[
  {
    "objectID": "models.html",
    "href": "models.html",
    "title": "Data sets and models",
    "section": "",
    "text": "source\n\n\n\n SleepWakeClassifier (model=None, data_processor=None,\n                      scaler_pipeline_name:str='scaler',\n                      model_pipeline_name:str='model')\n\nAbstract class for sleep/wake classifiers.\n\nsource\n\n\n\n\n train_pipeline (classifier:__main__.SleepWakeClassifier,\n                 examples_X:List[numpy.ndarray]=[],\n                 examples_y:List[numpy.ndarray]=[],\n                 pairs_Xy:List[Tuple[numpy.ndarray,numpy.ndarray]]=[],\n                 **train_kwargs)\n\n*Assumes data is already preprocessed using get_needed_X_y and ready to be passed to the classifier.\nReturns the loss history of the model.*\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nclassifier\nSleepWakeClassifier\n\nThe classifier object.\n\n\nexamples_X\nList\n[]\nList of input examples. If non-empty, then examples_y must also be provided and must have the same length.\n\n\nexamples_y\nList\n[]\nList of target labels. If non-empty, then examples_X must also be provided and must have the same length.\n\n\npairs_Xy\nList\n[]\nList of input-target pairs. If non-empty, then examples_X and examples_y must not be provided.\n\n\ntrain_kwargs\n\n\n\n\n\nReturns\nList\n\nThe loss history of the model.\n\n\n\n\nsource\n\n\n\n\n SleepWakeClassifier.train (examples_X:List[numpy.ndarray]=[],\n                            examples_y:List[numpy.ndarray]=[], pairs_Xy:Li\n                            st[Tuple[numpy.ndarray,numpy.ndarray]]=[],\n                            **training_kwargs)\n\n*Assumes data is already preprocessed using get_needed_X_y and ready to be passed to the model.\nReturns the loss history of the model.*\n\n\n\n\nsource\n\n\n\n\n LinearModel (value, names=None, module=None, qualname=None, type=None,\n              start=1)\n\nDefines the loss used in sklearn’s SGDClassifier which defines the linear model used for classification.\n\nsource\n\n\n\n\n SGDLinearClassifier\n                      (data_processor:pisces.data_sets.DataProcessor|None=\n                      None, linear_model:__main__.LinearModel=&lt;LinearModel\n                      .LOGISTIC_REGRESSION: 'log_loss'&gt;, **kwargs)\n\nUses Sk-Learn’s SGDCLassifier to train a model. Possible models are logistic regression, perceptron, and SVM. The SGD aspect allows for online learning, or custom training regimes through the partial_fit method. The model is trained with a balanced class weight, and uses L1 regularization. The input data is scaled with a StandardScaler before being passed to the model.\n\n\n\n\nsource\n\n\n\n\n RandomForest (data_processor:pisces.data_sets.DataProcessor|None=None,\n               class_weight:str='balanced', **kwargs)\n\nInterface for sklearn’s RandomForestClassifier\n\n\n\nWe have downloaded the saved model weights from a research repository from Mads Olsen’s group, and converted those into a saved Keras model to remove the need to re-define all of the layers. This conversion process is shown in ../analyses/convert_mads_olsen_model_to_keras.ipynb.\nThus, we have a TensorFlow model that we can run inference on, and we could train it if we wanted to.\nFor simplicity, we are just going to run inference. One twist of our method is that the classifier is expecting two high-resolution spectrograms for inputs: 1. 3-axis Accelerometer data 2. PPG (photoplethysmogram) data\nBased on visually inspecting examples from the paper, we are going to hack together an input by flipping the accelerometer data along the frequencies axis. The paper images seem to show a similarity between high-frequency accelerometer data and low-frequency PPG data. Surprisingly, this seems to work well.\n\nsource\n\n\n\n\n MOResUNetPretrained\n                      (data_processor:pisces.data_sets.DataProcessor|None=\n                      None, model:keras.src.models.model.Model|None=None,\n                      lazy_model_loading:bool=True,\n                      initial_lr:float=1e-05, validation_split:float=0.1,\n                      epochs:int=10, batch_size:int=1, **kwargs)\n\nAbstract class for sleep/wake classifiers.",
    "crumbs": [
      "Data sets and models"
    ]
  },
  {
    "objectID": "models.html#classifier-models",
    "href": "models.html#classifier-models",
    "title": "Data sets and models",
    "section": "",
    "text": "source\n\n\n\n SleepWakeClassifier (model=None, data_processor=None,\n                      scaler_pipeline_name:str='scaler',\n                      model_pipeline_name:str='model')\n\nAbstract class for sleep/wake classifiers.\n\nsource\n\n\n\n\n train_pipeline (classifier:__main__.SleepWakeClassifier,\n                 examples_X:List[numpy.ndarray]=[],\n                 examples_y:List[numpy.ndarray]=[],\n                 pairs_Xy:List[Tuple[numpy.ndarray,numpy.ndarray]]=[],\n                 **train_kwargs)\n\n*Assumes data is already preprocessed using get_needed_X_y and ready to be passed to the classifier.\nReturns the loss history of the model.*\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nclassifier\nSleepWakeClassifier\n\nThe classifier object.\n\n\nexamples_X\nList\n[]\nList of input examples. If non-empty, then examples_y must also be provided and must have the same length.\n\n\nexamples_y\nList\n[]\nList of target labels. If non-empty, then examples_X must also be provided and must have the same length.\n\n\npairs_Xy\nList\n[]\nList of input-target pairs. If non-empty, then examples_X and examples_y must not be provided.\n\n\ntrain_kwargs\n\n\n\n\n\nReturns\nList\n\nThe loss history of the model.\n\n\n\n\nsource\n\n\n\n\n SleepWakeClassifier.train (examples_X:List[numpy.ndarray]=[],\n                            examples_y:List[numpy.ndarray]=[], pairs_Xy:Li\n                            st[Tuple[numpy.ndarray,numpy.ndarray]]=[],\n                            **training_kwargs)\n\n*Assumes data is already preprocessed using get_needed_X_y and ready to be passed to the model.\nReturns the loss history of the model.*\n\n\n\n\nsource\n\n\n\n\n LinearModel (value, names=None, module=None, qualname=None, type=None,\n              start=1)\n\nDefines the loss used in sklearn’s SGDClassifier which defines the linear model used for classification.\n\nsource\n\n\n\n\n SGDLinearClassifier\n                      (data_processor:pisces.data_sets.DataProcessor|None=\n                      None, linear_model:__main__.LinearModel=&lt;LinearModel\n                      .LOGISTIC_REGRESSION: 'log_loss'&gt;, **kwargs)\n\nUses Sk-Learn’s SGDCLassifier to train a model. Possible models are logistic regression, perceptron, and SVM. The SGD aspect allows for online learning, or custom training regimes through the partial_fit method. The model is trained with a balanced class weight, and uses L1 regularization. The input data is scaled with a StandardScaler before being passed to the model.\n\n\n\n\nsource\n\n\n\n\n RandomForest (data_processor:pisces.data_sets.DataProcessor|None=None,\n               class_weight:str='balanced', **kwargs)\n\nInterface for sklearn’s RandomForestClassifier\n\n\n\nWe have downloaded the saved model weights from a research repository from Mads Olsen’s group, and converted those into a saved Keras model to remove the need to re-define all of the layers. This conversion process is shown in ../analyses/convert_mads_olsen_model_to_keras.ipynb.\nThus, we have a TensorFlow model that we can run inference on, and we could train it if we wanted to.\nFor simplicity, we are just going to run inference. One twist of our method is that the classifier is expecting two high-resolution spectrograms for inputs: 1. 3-axis Accelerometer data 2. PPG (photoplethysmogram) data\nBased on visually inspecting examples from the paper, we are going to hack together an input by flipping the accelerometer data along the frequencies axis. The paper images seem to show a similarity between high-frequency accelerometer data and low-frequency PPG data. Surprisingly, this seems to work well.\n\nsource\n\n\n\n\n MOResUNetPretrained\n                      (data_processor:pisces.data_sets.DataProcessor|None=\n                      None, model:keras.src.models.model.Model|None=None,\n                      lazy_model_loading:bool=True,\n                      initial_lr:float=1e-05, validation_split:float=0.1,\n                      epochs:int=10, batch_size:int=1, **kwargs)\n\nAbstract class for sleep/wake classifiers.",
    "crumbs": [
      "Data sets and models"
    ]
  },
  {
    "objectID": "models.html#training-tools",
    "href": "models.html#training-tools",
    "title": "Data sets and models",
    "section": "Training tools",
    "text": "Training tools\n\nsource\n\nrun_splits\n\n run_splits (split_maker:__main__.SplitMaker,\n             data_processor:pisces.data_sets.DataProcessor,\n             swc_class:Type[__main__.SleepWakeClassifier],\n             epochs:int|None, exclude:List[str]=[],\n             linear_model:__main__.LinearModel|None=None)\n\n\nsource\n\n\nrun_split\n\n run_split (train_indices,\n            preprocessed_data_set:List[Tuple[numpy.ndarray,numpy.ndarray]]\n            , swc:__main__.SleepWakeClassifier, epochs:int)\n\n\nsource\n\n\nLeaveOneOutSplitter\n\n LeaveOneOutSplitter ()\n\nInitialize self. See help(type(self)) for accurate signature.\n\nsource\n\n\nSplitMaker\n\n SplitMaker ()\n\nInitialize self. See help(type(self)) for accurate signature.",
    "crumbs": [
      "Data sets and models"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "pisces",
    "section": "",
    "text": "This package provides a framework for running machine learning experiments in the sleep classification space. The core of the package is SleepWakeClassifier class that formalizes an API for pre-processing raw sensor data into model-specified formats and scoring methods, automated data set and subject/feature discovery based on a light folder structure.\nAlso included is an example notebook showing pisces being used for analysis in a forthcoming paper from Arcascope. That studies the potential impact to scoring accuracy that may occur when a sleep classifier is trained on stationary subjects in a sleep study, then is deployed for inference on a Naval vessel with lots of ambient mechanical vibrations which affect the accelerometer recordings essential to many approaches to sleep classification.\nThe pipeline is designed to be flexible and can be easily extended to include new models, datasets, and evaluation metrics.",
    "crumbs": [
      "pisces"
    ]
  },
  {
    "objectID": "index.html#installation",
    "href": "index.html#installation",
    "title": "pisces",
    "section": "Installation",
    "text": "Installation\nWe will soon make Pisces available on PyPi, but for the time being you clone this repository and install the package locally.\nStart by making a python or conda environment with Python 3.11 and installing the requirements from file, replacing {env_name} with the name you’d like to give it, such as pisces_env:\nconda create -n {env_name} python=3.11\nconda activate {env_name}\nIn the same terminal (so that your new conda environment is active), navigate to the directory where you’d like to clone the package and run the following commands to clone it and use pip to install the package in an editable way with -e .\ngit clone https://github.com/Arcascope/pisces.git\ncd pisces\npip install -e .\n\nCommon issues\nYou may end up with a version of Keras incompatible with the marshalled data in pisces/cached_models. In that case, re-run the generation notebook &lt;pisces&gt;/analyses/convert_mads_olsen_model_to_keras.ipynb",
    "crumbs": [
      "pisces"
    ]
  },
  {
    "objectID": "index.html#usage",
    "href": "index.html#usage",
    "title": "pisces",
    "section": "Usage",
    "text": "Usage\nThe primary module to import is pisces.experiments which contains classes used for discovering and providing access to data sets in your chosen folder, as well as (trainable) classifiers.\nPisces is designed to be extended to support new models, datasets, and evaluation metrics. The analyses folder contains example notebooks that demonstrate how to use this code for comparing classifier performance on in-distribution and out-of-distribution accelerometer data.",
    "crumbs": [
      "pisces"
    ]
  },
  {
    "objectID": "index.html#data-sets",
    "href": "index.html#data-sets",
    "title": "pisces",
    "section": "Data Sets",
    "text": "Data Sets\nPisces automatically discovers data sets that match a simple, flexible format inside a given directory. analyses/stationary_vs_hybrid.ipynb finds data contained in the data_sets folder of the Pisces repository. The code is simple:\nfrom pisces.experiments import DataSetObject\n\nsets = DataSetObject.find_data_sets(\"../data_sets\")\nwalch = sets['walch_et_al']\nhybrid = sets['hybrid_motion']\nNow we have 2 DataSetObjects, walch and hybrid, that can be queried for their subjects and features. These were discovered because these are folders inside of data_sets that have a compatible structure.\nThese two sets were discovered because of the presence of at least one subdirectory matching the glob expression cleaned_*. Every subdirectory that matches this pattern is considered a feature, so based on the example below, Pisces discovers that hybrid_motion and walch_et_al both have psg, accelerometer, and activity features, in addition to other folders they may have not listed.\nThe data_sets directory looks like:\ndata_sets\n├── walch_et_al\n│   ├── cleaned_accelerometer\n│   │   ├── 46343_cleaned_motion.out\n│   │   ├── 759667_cleaned_motion.out\n│   │   ├── ...\n│   ├── cleaned_activity\n│   │   ├── 46343_cleaned_counts.out\n│   │   ├── 759667_cleaned_counts.out\n│   │   ├── ...\n│   ├── cleaned_psg\n│   │   ├── 46343_cleaned_psg.out\n│   │   ├── 759667_cleaned_psg.out\n│   │   ├── ...\n├── hybrid_motion\n│   ├── cleaned_accelerometer\n│   │   ├── 46343.csv\n│   │   ├── 759667.csv\n│   │   ├── ...\n│   ├── cleaned_activity\n│   │   ├── 46343.csv\n│   │   ├── 759667.csv\n│   │   ├── ...\n│   ├── cleaned_psg\n│   │   ├── 46343_labeled_sleep.txt\n│   │   ├── 759667_labeled_sleep.txt\n│   │   ├── ...\n\nKey takeaways for data set discovery:\n\nThe data set is discovered based on the presence of a subdirectory matching the glob expression cleaned_*.\nEvery subdirectory that matches this pattern is considered a feature; these features are named after the part matching *.\nSubjects within a feature are computed per-feature, based on variadic and constant parts of the filenames within each feature directory. Said in a less fancy way, because the walch_et_al accelerometer folders contain the files 46343_cleaned_motion.out and 759667_cleaned_motion.out which have _cleaned_motion.out in common, Pisces identifies 46343 and 759667 as subject IDs that have accelerometer feature data for walch_et_al.\n\nIt is no problem if some subjects are missing a certain feature. When the feature data for an existing subject, without that feature in their data, is requested, the feature will return None for that subject.\nThe naming scheme can vary greatly between features. However, the subject id MUST be the prefix on the filenames. For example, 46343_labeled_sleep.txt are both for the same subject, 46343. If instead we named those final_46343_cleaned_motion.out and 46343_labeled_sleep.txt then the subject’s data would be broken into two subjects, 46343 and final_46343.\n\n\n\n\nAdvanced features of data set discovery:\n\nThere is no a-priori rule about what features in a data set give the labels and which are model inputs. This allows you to call the label feature whatever you want, or use a mixture of features (psg + …) as labels for complex models supporting rich outputs.\nYou can have other folders inside data set directories that do NOT match cleaned_*, and these are totally ignored. This allows you to store other data, like raw data or metadata, in the same directory as the cleaned data.\nYou can have other folders whose sub-structure does not match the subject/feature structure, and these are totally ignored.",
    "crumbs": [
      "pisces"
    ]
  },
  {
    "objectID": "utils.html",
    "href": "utils.html",
    "title": "Utils",
    "section": "",
    "text": "source\n\n\n\n determine_header_rows_and_delimiter (filename:pathlib.Path|str)\n\nGiven a filename pointing at a CSV files, decides:  how many header lines there are (based on first line starting with a digit) * the delimiter– right now tries whitespace and comma\nReturns one of: - (number of header rows, column delimiter), - (number of header rows, None) if the delimiter could not be inferred, - (None, None) if CSV has no numerical rows,\n:param filename: CSV Path or filepath literal :return: header and delimiter information, if possible.*",
    "crumbs": [
      "Utils"
    ]
  },
  {
    "objectID": "utils.html#csv-utilities",
    "href": "utils.html#csv-utilities",
    "title": "Utils",
    "section": "",
    "text": "source\n\n\n\n determine_header_rows_and_delimiter (filename:pathlib.Path|str)\n\nGiven a filename pointing at a CSV files, decides:  how many header lines there are (based on first line starting with a digit) * the delimiter– right now tries whitespace and comma\nReturns one of: - (number of header rows, column delimiter), - (number of header rows, None) if the delimiter could not be inferred, - (None, None) if CSV has no numerical rows,\n:param filename: CSV Path or filepath literal :return: header and delimiter information, if possible.*",
    "crumbs": [
      "Utils"
    ]
  },
  {
    "objectID": "utils.html#activity-counts",
    "href": "utils.html#activity-counts",
    "title": "Utils",
    "section": "Activity counts",
    "text": "Activity counts\n/home/runner/.local/lib/python3.10/site-packages/fastcore/docscrape.py:230: UserWarning: potentially wrong underline length... \nParameters \n--- in \nADS algorithm for activity counts, developed by Arcascope with support from the NHRC.\n...\n  else: warn(msg)\n\nsource\n\nbuild_ADS\n\n build_ADS (time_xyz:numpy.ndarray, sampling_hz:float=50.0,\n            bin_size_seconds:float=15, prefix:str='')\n\n*ADS algorithm for activity counts, developed by Arcascope with support from the NHRC.",
    "crumbs": [
      "Utils"
    ]
  },
  {
    "objectID": "utils.html#parameters",
    "href": "utils.html#parameters",
    "title": "Utils",
    "section": "Parameters",
    "text": "Parameters\n\ntime_xyz: numpy array with shape (N_samples, 4) where the 4 coordinates are: [time, x, y, z]\nsampling_hz: float sampling frequency of thetime_xyz*\n\n\nsource\n\nbuild_activity_counts_te_Lindert_et_al\n\n build_activity_counts_te_Lindert_et_al (time_xyz, axis:int=3,\n                                         prefix:str='')\n\n*Implementation of the reverse-engineered activity count algorithm from te Lindert BH, Van Someren EJ. Sleep. 2013 Sleep estimates using microelectromechanical systems (MEMS). doi: 10.5665/sleep.2648\n:param time_xyz: np.ndarray loaded from timestamped triaxial accelerometer CSV. Shape (N, 4) :return: (time, activity counts with 15 second epoch)*\n\nsource\n\n\nbuild_activity_counts\n\n build_activity_counts (data, axis:int=3, prefix:str='',\n                        algorithm:__main__.ActivityCountAlgorithm=&lt;Activit\n                        yCountAlgorithm.ADS: 2&gt;)\n\n\nsource\n\n\nActivityCountAlgorithm\n\n ActivityCountAlgorithm (value, names=None, module=None, qualname=None,\n                         type=None, start=1)\n\nAn enumeration.",
    "crumbs": [
      "Utils"
    ]
  },
  {
    "objectID": "utils.html#plotting-utilities",
    "href": "utils.html#plotting-utilities",
    "title": "Utils",
    "section": "Plotting utilities",
    "text": "Plotting utilities\n\nsource\n\nplot_scores_PDF\n\n plot_scores_PDF (scores:List[float], ax:matplotlib.axes._axes.Axes=None)\n\nPlot the probability dist function (PDF) of the scores.\n\nsource\n\n\nplot_scores_CDF\n\n plot_scores_CDF (scores:List[float], ax:matplotlib.axes._axes.Axes=None)\n\nPlot the cumulative dist function (CDF) of the scores.\n/home/runner/.local/lib/python3.10/site-packages/fastcore/docscrape.py:230: UserWarning: potentially wrong underline length... \nReturns \n--- in \nComputes average of step functions.\n...\n  else: warn(msg)\n\nsource\n\n\navg_steps\n\n avg_steps (xs:List[List[float]], ys:List[List[float]])\n\n*Computes average of step functions.\nEach ys[j] is thought of as a right-continuous step function given by\nys[j](x) = xs[j][i] for xs[j][i] &lt;= x &lt; xs[j][i+1]\nThis function returns two NumPy arrays, (inputs, outputs), giving the pointwise average (see below) of these functions, one for inputs and one for outputs. These output arrays can be considered to give another step function.\nFor a list of functions [f_1, f_2, ..., f_n], their pointwise average is the function f_bar defined by\nf_bar(x) = (1/n)(f_1(x) + f_2(x) + ... + f_n(x))",
    "crumbs": [
      "Utils"
    ]
  },
  {
    "objectID": "utils.html#returns",
    "href": "utils.html#returns",
    "title": "Utils",
    "section": "Returns",
    "text": "Returns\ninputs: np.ndaray The union of all elements of all vectors in xs; this is the mutual domain of the average function. outputs: np.ndarray The pointwise average of the ys[j]s, considered as step functions extended to the full real line by assuming constant values for x &lt; min(xs[j]) or x &gt; max(xs[j])*\n\nsource\n\nconstant_interp\n\n constant_interp (x:numpy.ndarray, xp:numpy.ndarray, yp:numpy.ndarray,\n                  side:str='right')\n\n\nsource\n\n\nadd_rocs\n\n add_rocs (fprs:List[numpy.ndarray], tprs:List[numpy.ndarray],\n           x_class:str='SLEEP', y_class:str='WAKE', min_auc:float=0.0,\n           avg_curve_color:str='tab:blue',\n           specific_curve_color:str='tab:orange', roc_group_name:str='',\n           ax:matplotlib.axes._axes.Axes|None=None)\n\n*Adds ROC curves to the given plot, or makes a new plot if ax is None.\nif ax is None, we are making a new plot. We do additional formatting in this case, such as adding the legend and showing the plot.\nWhen ax is provided, we expect the call site to do formatting.*",
    "crumbs": [
      "Utils"
    ]
  },
  {
    "objectID": "utils.html#data-utilities",
    "href": "utils.html#data-utilities",
    "title": "Utils",
    "section": "Data utilities",
    "text": "Data utilities\n\nsource\n\npad_to_hat\n\n pad_to_hat (y:numpy.ndarray, y_hat:numpy.ndarray)\n\n*Adds zeros to the end of y to match the length of y_hat.\nUseful when the inputs had to be padded with zeros to match shape requirements for dense layers.*\n/home/runner/.local/lib/python3.10/site-packages/fastcore/docscrape.py:230: UserWarning: potentially wrong underline length... \nParameters \n--- in \nComputes Mean Absolute Error (MAE) for the numerical function `func` on the given lists.\n...\n  else: warn(msg)\n/home/runner/.local/lib/python3.10/site-packages/fastcore/docscrape.py:230: UserWarning: potentially wrong underline length... \nReturns \n--- in \nComputes Mean Absolute Error (MAE) for the numerical function `func` on the given lists.\n...\n  else: warn(msg)\n\nsource\n\n\nmae_func\n\n mae_func (func:Callable[[numpy.ndarray],float],\n           trues:List[numpy.ndarray], preds:List[numpy.ndarray])\n\n*Computes Mean Absolute Error (MAE) for the numerical function func on the given lists.\nThis function is useful for computing MAE of statistical functions giving a single float for every NumPy array.",
    "crumbs": [
      "Utils"
    ]
  },
  {
    "objectID": "utils.html#parameters-1",
    "href": "utils.html#parameters-1",
    "title": "Utils",
    "section": "Parameters",
    "text": "Parameters\nfunc: callable (np.ndarray) -&gt; float The statistic we are computing for truth/prediction arrays. It is called on each element of the lists of NumPy arrays, then MAE of the resulting statistic lists is computed. trues: list of np.ndarray The “True” labels, eg. This function is symmetric in trues and preds, and isn’t specific to classifiers, so the argument names are just mnemonics. preds: list of np.ndarray The “Predicted” labels, eg.\nReturns\n\n\n\n\n\n\nMAE of func applied to elements of trues and preds.*\n\n\n## Sleep metrics\n\n\n\nsource\n\nSleepMetricsCalculator\n\n SleepMetricsCalculator ()\n\nInitialize self. See help(type(self)) for accurate signature.\n\nsource\n\n\nConstants\n\n Constants ()\n\nInitialize self. See help(type(self)) for accurate signature.",
    "crumbs": [
      "Utils"
    ]
  },
  {
    "objectID": "utils.html#split-analysis-tools",
    "href": "utils.html#split-analysis-tools",
    "title": "Utils",
    "section": "Split analysis tools",
    "text": "Split analysis tools\n\nsource\n\nsplit_analysis\n\n split_analysis (y, y_hat_sleep_proba, sleep_accuracy:float=0.93,\n                 balancing:bool=True)",
    "crumbs": [
      "Utils"
    ]
  },
  {
    "objectID": "data_sets.html",
    "href": "data_sets.html",
    "title": "Data sets",
    "section": "",
    "text": "Data sets are discovered based on being folders within the provided data set root directory which contain subdirectories that start with cleaned_.\nOnce the data sets are discovered, we take the cleaned_&lt;feature&gt; subdirectories and use the &lt;feature&gt; as the feature name.\nThen we take the files within the cleaned_&lt;feature&gt; subdirectories and discover the ids that data set has for that feature. These do not need to be the same across features, hence all of our data getters might also return None.\nAutomagic ID discovery is done using a prefix tree, which is a data structure that allows for efficient searching of strings based on their prefixes.\n\nsource\n\n\n\n IdExtractor (delimiter:str='', key:str='')\n\n*Class extending the prefix trees that incorporates the algorithm for extracting IDs from a list of file names. The algorithm is somewhat oblique, so it’s better to just use the extract_ids method versus trying to use the prfix trees directly at the call site.\nThe algorithm is based on the assumption that the IDs are the same across all file names, but that the file names may have different suffixes. The algorithm reverses the file names, inserts them into the tree, and then simplifes and flattens that tree in order to find the IDs as leaves of that simplified tree.\n\nInsert the file name string into the tree, but with each string reversed.\nSimplify the tree, combining nodes with only one child.\nThere may be unexpected suffix matches for these IDs, so we flatten the tree to depth 1, meaning all children of the root are combined to make leaves.\nThe leaves are the IDs we want to extract. However, we must reverse these leaf keys to get the original IDs, since we reversed the file names in step 1.\n\nTODO: * If we want to find IDs for files with differing prefixes instead, we should instead insert the file names NOT reversed and then NOT reverse in the last step.\n\nTo handle IDs that appear in the middle of file names, we can use both methods to come up with a list of potential IDs based on prefix and suffix, then figure out the “intersection” of those lists. (Maybe using another prefix tree?)*\n\n\nsource\n\n\n\n\n SimplifiablePrefixTree (delimiter:str='', key:str='')\n\nA standard prefix tree with the ability to “simplify” itself by combining nodes with only one child. These also have the ability to “flatten” themselves, which means to convert all nodes at and below a certain depth into leaves on the most recent ancestor of that depth.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ndelimiter\nstr\n\nThe delimiter to use when splitting words into characters. If empty, the words are treated as sequences of characters.\n\n\nkey\nstr\n\nThe key of the current node in its parent’s .children dictionary. If empty, the node is (likely) the root of the tree.\n\n\n\n\nsource\n\n\n\n\n DataSetObject (name:str, path:pathlib.Path)\n\nInitialize self. See help(type(self)) for accurate signature.\n\nsource\n\n\n\n\n psg_to_WLDM (psg:polars.dataframe.frame.DataFrame, N4:bool=True)\n\n** map all positive classes as follows: If N4 is True: - 1, 2 =&gt; 1 (light sleep) - 3, 4 =&gt; 2 (deep sleep) - 5 =&gt; 3 (REM) If N4 is False: - 1, 2 =&gt; 1 (light sleep) - 3 =&gt; 2 (deep sleep) - 4 =&gt; 3 (REM) * retain all 0 (wake) and -1 (mask) classes*\n\nsource\n\n\n\n\n to_WLDM (x:float, N4:bool=True)\n\nMap sleep stages to wake, light, deep, and REM sleep. Retain masked values. If N4 stage is not present, PSG=4 is mapped to REM. Otherwise it is mapped to deep sleep.\n\nsource\n\n\n\n\n psg_to_sleep_wake (psg:polars.dataframe.frame.DataFrame)\n\n** map all positive classes to 1 (sleep) * retain all 0 (wake) and -1 (mask) classes*\n\nsource\n\n\n\n\n ModelInputSpectrogram (input_features:Union[List[str],str],\n                        input_sampling_hz:int|float, spectrogram_preproces\n                        sing_config:Dict={'preprocessing': [{'args':\n                        {'window_size': 30, 'fs': 32}, 'type': 'median'},\n                        {'args': {'iqr_window': 300, 'median_window': 300,\n                        'fs': 32}, 'type': 'iqr_normalization_adaptive'},\n                        {'args': {'threshold': 20, 'fs': 32}, 'type':\n                        'clip_by_iqr'}, {'args': {'fs': 32, 'nfft': 512,\n                        'f_max': 6, 'f_min': 0, 'f_sub': 3, 'window': 320,\n                        'noverlap': 256}, 'type': 'cal_psd'}]})\n\nInitialize self. See help(type(self)) for accurate signature.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ninput_features\nUnion\n\n\n\n\ninput_sampling_hz\nint | float\n\nSampling rate of the input data (1/s)\n\n\nspectrogram_preprocessing_config\nDict\n{‘preprocessing’: [{‘args’: {‘window_size’: 30, ‘fs’: 32}, ‘type’: ‘median’}, {‘args’: {‘iqr_window’: 300, ‘median_window’: 300, ‘fs’: 32}, ‘type’: ‘iqr_normalization_adaptive’}, {‘args’: {‘threshold’: 20, ‘fs’: 32}, ‘type’: ‘clip_by_iqr’}, {‘args’: {‘fs’: 32, ‘nfft’: 512, ‘f_max’: 6, ‘f_min’: 0, ‘f_sub’: 3, ‘window’: 320, ‘noverlap’: 256}, ‘type’: ‘cal_psd’}]}\nSteps in the preprocessing pipeline for getting a spectrogram from acceleration\n\n\n\n\nsource\n\n\n\n\n ModelInput1D (input_features:Union[List[str],str],\n               input_sampling_hz:int|float, input_window_time:int|float)\n\nInitialize self. See help(type(self)) for accurate signature.\n\n\n\n\n\n\n\n\n\nType\nDetails\n\n\n\n\ninput_features\nUnion\n\n\n\ninput_sampling_hz\nint | float\nSampling rate of the input data (1/s)\n\n\ninput_window_time\nint | float\nWindow size (in seconds) for the input data. Window will be centered around the time point for which the model is making a prediction\n\n\n\n\nsource\n\n\n\n\n ModelInput (input_features:Union[List[str],str],\n             input_sampling_hz:int|float)\n\nInitialize self. See help(type(self)) for accurate signature.\n\n\n\n\n\n\n\n\n\nType\nDetails\n\n\n\n\ninput_features\nUnion\n\n\n\ninput_sampling_hz\nint | float\nSampling rate of the input data (1/s)\n\n\n\n\nsource\n\n\n\n\n PSGType (value, names=None, module=None, qualname=None, type=None,\n          start=1)\n\nAn enumeration.\n\nsource\n\n\n\n\n ModelOutputType (value, names=None, module=None, qualname=None,\n                  type=None, start=1)\n\nAn enumeration.\n\nsource\n\n\n\n\n psg_to_WLDM (psg:polars.dataframe.frame.DataFrame, N4:bool=True)\n\n** map all positive classes as follows: If N4 is True: - 1, 2 =&gt; 1 (light sleep) - 3, 4 =&gt; 2 (deep sleep) - 5 =&gt; 3 (REM) If N4 is False: - 1, 2 =&gt; 1 (light sleep) - 3 =&gt; 2 (deep sleep) - 5 =&gt; 3 (REM) * retain all 0 (wake) and -1 (mask) classes*\n\nsource\n\n\n\n\n to_WLDM (x:float, N4:bool=True)\n\nMap sleep stages to wake, light, deep, and REM sleep. Retain masked values. If N4 stage is not present, PSG=4 is mapped to REM. Otherwise it is mapped to deep sleep.\n\nsource\n\n\n\n\n psg_to_sleep_wake (psg:polars.dataframe.frame.DataFrame)\n\n** map all positive classes to 1 (sleep) * retain all 0 (wake) and -1 (mask) classes*\n\nsource\n\n\n\n\n ModelInputSpectrogram (input_features:Union[List[str],str],\n                        input_sampling_hz:int|float, spectrogram_preproces\n                        sing_config:Dict={'preprocessing': [{'args':\n                        {'window_size': 30, 'fs': 32}, 'type': 'median'},\n                        {'args': {'iqr_window': 300, 'median_window': 300,\n                        'fs': 32}, 'type': 'iqr_normalization_adaptive'},\n                        {'args': {'threshold': 20, 'fs': 32}, 'type':\n                        'clip_by_iqr'}, {'args': {'fs': 32, 'nfft': 512,\n                        'f_max': 6, 'f_min': 0, 'f_sub': 3, 'window': 320,\n                        'noverlap': 256}, 'type': 'cal_psd'}]})\n\nInitialize self. See help(type(self)) for accurate signature.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ninput_features\nUnion\n\n\n\n\ninput_sampling_hz\nint | float\n\nSampling rate of the input data (1/s)\n\n\nspectrogram_preprocessing_config\nDict\n{‘preprocessing’: [{‘args’: {‘window_size’: 30, ‘fs’: 32}, ‘type’: ‘median’}, {‘args’: {‘iqr_window’: 300, ‘median_window’: 300, ‘fs’: 32}, ‘type’: ‘iqr_normalization_adaptive’}, {‘args’: {‘threshold’: 20, ‘fs’: 32}, ‘type’: ‘clip_by_iqr’}, {‘args’: {‘fs’: 32, ‘nfft’: 512, ‘f_max’: 6, ‘f_min’: 0, ‘f_sub’: 3, ‘window’: 320, ‘noverlap’: 256}, ‘type’: ‘cal_psd’}]}\nSteps in the preprocessing pipeline for getting a spectrogram from acceleration\n\n\n\n\nsource\n\n\n\n\n ModelInput1D (input_features:Union[List[str],str],\n               input_sampling_hz:int|float, input_window_time:int|float)\n\nInitialize self. See help(type(self)) for accurate signature.\n\n\n\n\n\n\n\n\n\nType\nDetails\n\n\n\n\ninput_features\nUnion\n\n\n\ninput_sampling_hz\nint | float\nSampling rate of the input data (1/s)\n\n\ninput_window_time\nint | float\nWindow size (in seconds) for the input data. Window will be centered around the time point for which the model is making a prediction\n\n\n\n\nsource\n\n\n\n\n ModelInput (input_features:Union[List[str],str],\n             input_sampling_hz:int|float)\n\nInitialize self. See help(type(self)) for accurate signature.\n\n\n\n\n\n\n\n\n\nType\nDetails\n\n\n\n\ninput_features\nUnion\n\n\n\ninput_sampling_hz\nint | float\nSampling rate of the input data (1/s)\n\n\n\n\nsource\n\n\n\n\n PSGType (value, names=None, module=None, qualname=None, type=None,\n          start=1)\n\nAn enumeration.\n\nsource\n\n\n\n\n ModelOutputType (value, names=None, module=None, qualname=None,\n                  type=None, start=1)\n\nAn enumeration.\n\nsource\n\n\n\n\n fill_gaps_in_accelerometer_data (acc:polars.dataframe.frame.DataFrame,\n                                  smooth:bool=False,\n                                  final_sampling_rate_hz:int|None=None)\n\n\nsource\n\n\n\n\n apply_gausian_filter (df:polars.dataframe.frame.DataFrame,\n                       sigma:float=1.0, overwrite:bool=False)\n\n\nsource\n\n\n\n\n mask_psg_from_accel (psg:numpy.ndarray, accel:numpy.ndarray,\n                      psg_epoch:int=30, accel_sample_rate:float|None=None,\n                      min_epoch_fraction_covered:float=0.5)\n\n\nsource\n\n\n\n\n get_sample_weights (y:numpy.ndarray)\n\nCalculate sample weights based on the distribution of classes in the data. Doesn’t count masked values (-1) in the class distribution.\n\nsource\n\n\n\n\n DataProcessor (data_set:__main__.DataSetObject,\n                model_input:__main__.ModelInput, output_feature:str='psg',\n                output_type:__main__.ModelOutputType=&lt;ModelOutputType.WAKE\n                _LIGHT_DEEP_REM: 2&gt;,\n                psg_type:__main__.PSGType=&lt;PSGType.NO_N4: 1&gt;)\n\nInitialize self. See help(type(self)) for accurate signature.\n\n# not exported\ndef _create_mock_data():\n    np.random.seed(42) # by Deep Thought\n    mock_data_location = \"../mock_data_sets\"\n    if not os.path.exists(mock_data_location):\n        os.makedirs(mock_data_location)\n    total_time_hrs = 1.0\n    total_subjects = 3\n    total_data_sets = 2\n    accelerometer_sampling_hz = 1.0 \n    activity_dt_seconds = 15.0\n    max_activity = 50\n    psg_dt_seconds = 30.0\n    for data_set in range(total_data_sets):\n        data_set_path = f\"{mock_data_location}/data_set_{data_set}\"\n        if not os.path.exists(data_set_path):\n            os.makedirs(data_set_path)\n        # Accelerometer data\n        accelerometer_path = f\"{data_set_path}/cleaned_accelerometer\"\n        if not os.path.exists(accelerometer_path):\n            os.makedirs(accelerometer_path)\n        accelerometer_time = np.arange(0, total_time_hrs * 3600, 1.0 / accelerometer_sampling_hz)\n        for i in range(total_subjects):\n            accelerometer_data = np.random.randn(len(accelerometer_time), 3)\n            accelerometer = pl.DataFrame({\n                'timestamp': accelerometer_time,\n                'x': accelerometer_data[:, 0],\n                'y': accelerometer_data[:, 1],\n                'z': accelerometer_data[:, 2],\n            })\n            subject_path = f\"{accelerometer_path}/id00{i}_cleaned_motion.out\"\n            accelerometer.write_csv(subject_path, include_header=False, separator=' ')\n        # Activity data\n        activity_path = f\"{data_set_path}/cleaned_activity\"\n        if not os.path.exists(activity_path):\n            os.makedirs(activity_path)\n        activity_time = np.arange(0, total_time_hrs * 3600, activity_dt_seconds)\n        for i in range(total_subjects):\n            activity_data = np.random.randint(0, max_activity, len(activity_time))\n            activity = pl.DataFrame({\n                'timestamp': activity_time,\n                'activity': activity_data,\n            })\n            subject_path = f\"{activity_path}/id00{i}_cleaned_counts.out\"\n            activity.write_csv(subject_path, include_header=False, separator=' ')\n        # Heart rate data\n        hr_path = f\"{data_set_path}/cleaned_heartrate\"\n        if not os.path.exists(hr_path):\n            os.makedirs(hr_path)\n        ## Irregular sampling rate\n        hr_time = np.random.choice(accelerometer_time, len(activity_time), replace=False)\n        for i in range(total_subjects):\n            hr_data = np.random.randint(60, 120, len(activity_time))\n            hr = pl.DataFrame({\n                'timestamp': hr_time,\n                'hr': hr_data,\n            })\n            subject_path = f\"{hr_path}/id00{i}_cleaned_hr.out\"\n            hr.write_csv(subject_path, include_header=False, separator=' ')\n        # PSG data\n        psg_path = f\"{data_set_path}/cleaned_psg\"\n        if not os.path.exists(psg_path):\n            os.makedirs(psg_path)\n        psg_time = np.arange(0, total_time_hrs * 3600, psg_dt_seconds)\n        for i in range(total_subjects):\n            psg_data = np.random.randint(-1, 5, len(psg_time))\n            psg = pl.DataFrame({\n                'timestamp': psg_time,\n                'stage': psg_data,\n            })\n            subject_path = f\"{psg_path}/id00{i}_cleaned_psg.out\"\n            psg.write_csv(subject_path, include_header=False, separator=' ')",
    "crumbs": [
      "Data sets"
    ]
  },
  {
    "objectID": "data_sets.html#data-set-discovery-using-prefix-trees",
    "href": "data_sets.html#data-set-discovery-using-prefix-trees",
    "title": "Data sets",
    "section": "",
    "text": "Data sets are discovered based on being folders within the provided data set root directory which contain subdirectories that start with cleaned_.\nOnce the data sets are discovered, we take the cleaned_&lt;feature&gt; subdirectories and use the &lt;feature&gt; as the feature name.\nThen we take the files within the cleaned_&lt;feature&gt; subdirectories and discover the ids that data set has for that feature. These do not need to be the same across features, hence all of our data getters might also return None.\nAutomagic ID discovery is done using a prefix tree, which is a data structure that allows for efficient searching of strings based on their prefixes.\n\nsource\n\n\n\n IdExtractor (delimiter:str='', key:str='')\n\n*Class extending the prefix trees that incorporates the algorithm for extracting IDs from a list of file names. The algorithm is somewhat oblique, so it’s better to just use the extract_ids method versus trying to use the prfix trees directly at the call site.\nThe algorithm is based on the assumption that the IDs are the same across all file names, but that the file names may have different suffixes. The algorithm reverses the file names, inserts them into the tree, and then simplifes and flattens that tree in order to find the IDs as leaves of that simplified tree.\n\nInsert the file name string into the tree, but with each string reversed.\nSimplify the tree, combining nodes with only one child.\nThere may be unexpected suffix matches for these IDs, so we flatten the tree to depth 1, meaning all children of the root are combined to make leaves.\nThe leaves are the IDs we want to extract. However, we must reverse these leaf keys to get the original IDs, since we reversed the file names in step 1.\n\nTODO: * If we want to find IDs for files with differing prefixes instead, we should instead insert the file names NOT reversed and then NOT reverse in the last step.\n\nTo handle IDs that appear in the middle of file names, we can use both methods to come up with a list of potential IDs based on prefix and suffix, then figure out the “intersection” of those lists. (Maybe using another prefix tree?)*\n\n\nsource\n\n\n\n\n SimplifiablePrefixTree (delimiter:str='', key:str='')\n\nA standard prefix tree with the ability to “simplify” itself by combining nodes with only one child. These also have the ability to “flatten” themselves, which means to convert all nodes at and below a certain depth into leaves on the most recent ancestor of that depth.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ndelimiter\nstr\n\nThe delimiter to use when splitting words into characters. If empty, the words are treated as sequences of characters.\n\n\nkey\nstr\n\nThe key of the current node in its parent’s .children dictionary. If empty, the node is (likely) the root of the tree.\n\n\n\n\nsource\n\n\n\n\n DataSetObject (name:str, path:pathlib.Path)\n\nInitialize self. See help(type(self)) for accurate signature.\n\nsource\n\n\n\n\n psg_to_WLDM (psg:polars.dataframe.frame.DataFrame, N4:bool=True)\n\n** map all positive classes as follows: If N4 is True: - 1, 2 =&gt; 1 (light sleep) - 3, 4 =&gt; 2 (deep sleep) - 5 =&gt; 3 (REM) If N4 is False: - 1, 2 =&gt; 1 (light sleep) - 3 =&gt; 2 (deep sleep) - 4 =&gt; 3 (REM) * retain all 0 (wake) and -1 (mask) classes*\n\nsource\n\n\n\n\n to_WLDM (x:float, N4:bool=True)\n\nMap sleep stages to wake, light, deep, and REM sleep. Retain masked values. If N4 stage is not present, PSG=4 is mapped to REM. Otherwise it is mapped to deep sleep.\n\nsource\n\n\n\n\n psg_to_sleep_wake (psg:polars.dataframe.frame.DataFrame)\n\n** map all positive classes to 1 (sleep) * retain all 0 (wake) and -1 (mask) classes*\n\nsource\n\n\n\n\n ModelInputSpectrogram (input_features:Union[List[str],str],\n                        input_sampling_hz:int|float, spectrogram_preproces\n                        sing_config:Dict={'preprocessing': [{'args':\n                        {'window_size': 30, 'fs': 32}, 'type': 'median'},\n                        {'args': {'iqr_window': 300, 'median_window': 300,\n                        'fs': 32}, 'type': 'iqr_normalization_adaptive'},\n                        {'args': {'threshold': 20, 'fs': 32}, 'type':\n                        'clip_by_iqr'}, {'args': {'fs': 32, 'nfft': 512,\n                        'f_max': 6, 'f_min': 0, 'f_sub': 3, 'window': 320,\n                        'noverlap': 256}, 'type': 'cal_psd'}]})\n\nInitialize self. See help(type(self)) for accurate signature.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ninput_features\nUnion\n\n\n\n\ninput_sampling_hz\nint | float\n\nSampling rate of the input data (1/s)\n\n\nspectrogram_preprocessing_config\nDict\n{‘preprocessing’: [{‘args’: {‘window_size’: 30, ‘fs’: 32}, ‘type’: ‘median’}, {‘args’: {‘iqr_window’: 300, ‘median_window’: 300, ‘fs’: 32}, ‘type’: ‘iqr_normalization_adaptive’}, {‘args’: {‘threshold’: 20, ‘fs’: 32}, ‘type’: ‘clip_by_iqr’}, {‘args’: {‘fs’: 32, ‘nfft’: 512, ‘f_max’: 6, ‘f_min’: 0, ‘f_sub’: 3, ‘window’: 320, ‘noverlap’: 256}, ‘type’: ‘cal_psd’}]}\nSteps in the preprocessing pipeline for getting a spectrogram from acceleration\n\n\n\n\nsource\n\n\n\n\n ModelInput1D (input_features:Union[List[str],str],\n               input_sampling_hz:int|float, input_window_time:int|float)\n\nInitialize self. See help(type(self)) for accurate signature.\n\n\n\n\n\n\n\n\n\nType\nDetails\n\n\n\n\ninput_features\nUnion\n\n\n\ninput_sampling_hz\nint | float\nSampling rate of the input data (1/s)\n\n\ninput_window_time\nint | float\nWindow size (in seconds) for the input data. Window will be centered around the time point for which the model is making a prediction\n\n\n\n\nsource\n\n\n\n\n ModelInput (input_features:Union[List[str],str],\n             input_sampling_hz:int|float)\n\nInitialize self. See help(type(self)) for accurate signature.\n\n\n\n\n\n\n\n\n\nType\nDetails\n\n\n\n\ninput_features\nUnion\n\n\n\ninput_sampling_hz\nint | float\nSampling rate of the input data (1/s)\n\n\n\n\nsource\n\n\n\n\n PSGType (value, names=None, module=None, qualname=None, type=None,\n          start=1)\n\nAn enumeration.\n\nsource\n\n\n\n\n ModelOutputType (value, names=None, module=None, qualname=None,\n                  type=None, start=1)\n\nAn enumeration.\n\nsource\n\n\n\n\n psg_to_WLDM (psg:polars.dataframe.frame.DataFrame, N4:bool=True)\n\n** map all positive classes as follows: If N4 is True: - 1, 2 =&gt; 1 (light sleep) - 3, 4 =&gt; 2 (deep sleep) - 5 =&gt; 3 (REM) If N4 is False: - 1, 2 =&gt; 1 (light sleep) - 3 =&gt; 2 (deep sleep) - 5 =&gt; 3 (REM) * retain all 0 (wake) and -1 (mask) classes*\n\nsource\n\n\n\n\n to_WLDM (x:float, N4:bool=True)\n\nMap sleep stages to wake, light, deep, and REM sleep. Retain masked values. If N4 stage is not present, PSG=4 is mapped to REM. Otherwise it is mapped to deep sleep.\n\nsource\n\n\n\n\n psg_to_sleep_wake (psg:polars.dataframe.frame.DataFrame)\n\n** map all positive classes to 1 (sleep) * retain all 0 (wake) and -1 (mask) classes*\n\nsource\n\n\n\n\n ModelInputSpectrogram (input_features:Union[List[str],str],\n                        input_sampling_hz:int|float, spectrogram_preproces\n                        sing_config:Dict={'preprocessing': [{'args':\n                        {'window_size': 30, 'fs': 32}, 'type': 'median'},\n                        {'args': {'iqr_window': 300, 'median_window': 300,\n                        'fs': 32}, 'type': 'iqr_normalization_adaptive'},\n                        {'args': {'threshold': 20, 'fs': 32}, 'type':\n                        'clip_by_iqr'}, {'args': {'fs': 32, 'nfft': 512,\n                        'f_max': 6, 'f_min': 0, 'f_sub': 3, 'window': 320,\n                        'noverlap': 256}, 'type': 'cal_psd'}]})\n\nInitialize self. See help(type(self)) for accurate signature.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ninput_features\nUnion\n\n\n\n\ninput_sampling_hz\nint | float\n\nSampling rate of the input data (1/s)\n\n\nspectrogram_preprocessing_config\nDict\n{‘preprocessing’: [{‘args’: {‘window_size’: 30, ‘fs’: 32}, ‘type’: ‘median’}, {‘args’: {‘iqr_window’: 300, ‘median_window’: 300, ‘fs’: 32}, ‘type’: ‘iqr_normalization_adaptive’}, {‘args’: {‘threshold’: 20, ‘fs’: 32}, ‘type’: ‘clip_by_iqr’}, {‘args’: {‘fs’: 32, ‘nfft’: 512, ‘f_max’: 6, ‘f_min’: 0, ‘f_sub’: 3, ‘window’: 320, ‘noverlap’: 256}, ‘type’: ‘cal_psd’}]}\nSteps in the preprocessing pipeline for getting a spectrogram from acceleration\n\n\n\n\nsource\n\n\n\n\n ModelInput1D (input_features:Union[List[str],str],\n               input_sampling_hz:int|float, input_window_time:int|float)\n\nInitialize self. See help(type(self)) for accurate signature.\n\n\n\n\n\n\n\n\n\nType\nDetails\n\n\n\n\ninput_features\nUnion\n\n\n\ninput_sampling_hz\nint | float\nSampling rate of the input data (1/s)\n\n\ninput_window_time\nint | float\nWindow size (in seconds) for the input data. Window will be centered around the time point for which the model is making a prediction\n\n\n\n\nsource\n\n\n\n\n ModelInput (input_features:Union[List[str],str],\n             input_sampling_hz:int|float)\n\nInitialize self. See help(type(self)) for accurate signature.\n\n\n\n\n\n\n\n\n\nType\nDetails\n\n\n\n\ninput_features\nUnion\n\n\n\ninput_sampling_hz\nint | float\nSampling rate of the input data (1/s)\n\n\n\n\nsource\n\n\n\n\n PSGType (value, names=None, module=None, qualname=None, type=None,\n          start=1)\n\nAn enumeration.\n\nsource\n\n\n\n\n ModelOutputType (value, names=None, module=None, qualname=None,\n                  type=None, start=1)\n\nAn enumeration.\n\nsource\n\n\n\n\n fill_gaps_in_accelerometer_data (acc:polars.dataframe.frame.DataFrame,\n                                  smooth:bool=False,\n                                  final_sampling_rate_hz:int|None=None)\n\n\nsource\n\n\n\n\n apply_gausian_filter (df:polars.dataframe.frame.DataFrame,\n                       sigma:float=1.0, overwrite:bool=False)\n\n\nsource\n\n\n\n\n mask_psg_from_accel (psg:numpy.ndarray, accel:numpy.ndarray,\n                      psg_epoch:int=30, accel_sample_rate:float|None=None,\n                      min_epoch_fraction_covered:float=0.5)\n\n\nsource\n\n\n\n\n get_sample_weights (y:numpy.ndarray)\n\nCalculate sample weights based on the distribution of classes in the data. Doesn’t count masked values (-1) in the class distribution.\n\nsource\n\n\n\n\n DataProcessor (data_set:__main__.DataSetObject,\n                model_input:__main__.ModelInput, output_feature:str='psg',\n                output_type:__main__.ModelOutputType=&lt;ModelOutputType.WAKE\n                _LIGHT_DEEP_REM: 2&gt;,\n                psg_type:__main__.PSGType=&lt;PSGType.NO_N4: 1&gt;)\n\nInitialize self. See help(type(self)) for accurate signature.\n\n# not exported\ndef _create_mock_data():\n    np.random.seed(42) # by Deep Thought\n    mock_data_location = \"../mock_data_sets\"\n    if not os.path.exists(mock_data_location):\n        os.makedirs(mock_data_location)\n    total_time_hrs = 1.0\n    total_subjects = 3\n    total_data_sets = 2\n    accelerometer_sampling_hz = 1.0 \n    activity_dt_seconds = 15.0\n    max_activity = 50\n    psg_dt_seconds = 30.0\n    for data_set in range(total_data_sets):\n        data_set_path = f\"{mock_data_location}/data_set_{data_set}\"\n        if not os.path.exists(data_set_path):\n            os.makedirs(data_set_path)\n        # Accelerometer data\n        accelerometer_path = f\"{data_set_path}/cleaned_accelerometer\"\n        if not os.path.exists(accelerometer_path):\n            os.makedirs(accelerometer_path)\n        accelerometer_time = np.arange(0, total_time_hrs * 3600, 1.0 / accelerometer_sampling_hz)\n        for i in range(total_subjects):\n            accelerometer_data = np.random.randn(len(accelerometer_time), 3)\n            accelerometer = pl.DataFrame({\n                'timestamp': accelerometer_time,\n                'x': accelerometer_data[:, 0],\n                'y': accelerometer_data[:, 1],\n                'z': accelerometer_data[:, 2],\n            })\n            subject_path = f\"{accelerometer_path}/id00{i}_cleaned_motion.out\"\n            accelerometer.write_csv(subject_path, include_header=False, separator=' ')\n        # Activity data\n        activity_path = f\"{data_set_path}/cleaned_activity\"\n        if not os.path.exists(activity_path):\n            os.makedirs(activity_path)\n        activity_time = np.arange(0, total_time_hrs * 3600, activity_dt_seconds)\n        for i in range(total_subjects):\n            activity_data = np.random.randint(0, max_activity, len(activity_time))\n            activity = pl.DataFrame({\n                'timestamp': activity_time,\n                'activity': activity_data,\n            })\n            subject_path = f\"{activity_path}/id00{i}_cleaned_counts.out\"\n            activity.write_csv(subject_path, include_header=False, separator=' ')\n        # Heart rate data\n        hr_path = f\"{data_set_path}/cleaned_heartrate\"\n        if not os.path.exists(hr_path):\n            os.makedirs(hr_path)\n        ## Irregular sampling rate\n        hr_time = np.random.choice(accelerometer_time, len(activity_time), replace=False)\n        for i in range(total_subjects):\n            hr_data = np.random.randint(60, 120, len(activity_time))\n            hr = pl.DataFrame({\n                'timestamp': hr_time,\n                'hr': hr_data,\n            })\n            subject_path = f\"{hr_path}/id00{i}_cleaned_hr.out\"\n            hr.write_csv(subject_path, include_header=False, separator=' ')\n        # PSG data\n        psg_path = f\"{data_set_path}/cleaned_psg\"\n        if not os.path.exists(psg_path):\n            os.makedirs(psg_path)\n        psg_time = np.arange(0, total_time_hrs * 3600, psg_dt_seconds)\n        for i in range(total_subjects):\n            psg_data = np.random.randint(-1, 5, len(psg_time))\n            psg = pl.DataFrame({\n                'timestamp': psg_time,\n                'stage': psg_data,\n            })\n            subject_path = f\"{psg_path}/id00{i}_cleaned_psg.out\"\n            psg.write_csv(subject_path, include_header=False, separator=' ')",
    "crumbs": [
      "Data sets"
    ]
  },
  {
    "objectID": "mads_olsen_support.html",
    "href": "mads_olsen_support.html",
    "title": "Support code for Mads Olsen’s group’s model",
    "section": "",
    "text": "source\n\nload_saved_keras\n\n load_saved_keras ()\n\n\nsource\n\n\ncal_psd\n\n cal_psd (x, fs, window, noverlap, nfft, f_min, f_max, f_sub=1)\n\nhttps://github.com/MADSOLSEN/SleepStagePrediction/blob/d47ff488f5cedd3b0459593a53fc4f92fc3660a2/signal_processing/spectrogram.py#L91\n\nsource\n\n\niqr_normalization_adaptive\n\n iqr_normalization_adaptive (x, fs, median_window, iqr_window)\n\n\nsource\n\n\nclip_by_iqr\n\n clip_by_iqr (x, fs, threshold=20)\n\n\nsource\n\n\nmedian\n\n median (x, fs, window_size)\n\n\nsource\n\n\ndetermine_depth\n\n determine_depth (temporal_shape, temporal_max_pool_size)\n\n\nsource\n\n\nconv_block\n\n conv_block (x, features, kernel_size, data_format='channels_last',\n             weight_decay=0.0, residual=True, stochastic_depth=True,\n             activation='gelu')\n\n\nsource\n\n\nfactory_ResUNet\n\n factory_ResUNet (input_shape, num_classes, num_outputs, depth=None,\n                  init_filter_num=8,\n                  filter_increment_factor=1.2599210498948732,\n                  kernel_size=(16, 1), max_pool_size=(2, 1),\n                  activation='gelu', output_layer='sigmoid',\n                  weight_decay=0.0, residual=False,\n                  stochastic_depth=False, data_format='channels_last')\n\n\nevents = ['wake', 'light', 'deep', 'rem']\nevents_format = [\n    {\n        'name': 'wake', \n        'h5_path': 'wake',\n        'probability': 1 / len(events)\n    },\n    {\n        'name': 'light', \n        'h5_path': 'light',\n        'probability': 1 / len(events)\n    },\n    {\n        'name': 'deep', \n        'h5_path': 'deep',\n        'probability': 1 / len(events)\n    },\n    {\n        'name': 'rem', \n        'h5_path': 'rem',\n        'probability': 1 / len(events)\n    }\n]\nsignals_format =  {\n    \"ACC_merge_fft_spec\": {\n        \"add\": True,\n        \"fs_post\": 0.5,\n        \"h5_path\": \"acc_signal\",\n        \"dimensions\": [\n            32,\n            1\n        ],\n        \"channel_idx\": [\n            0,\n            1,\n            2\n        ],\n        \"preprocessing\": [\n            {\n                \"args\": {\n                    \"window_size\": 30\n                },\n                \"type\": \"median\"\n            },\n            {\n                \"args\": {\n                    \"iqr_window\": 300,\n                    \"median_window\": 300\n                },\n                \"type\": \"iqr_normalization_adaptive\"\n            },\n            {\n                \"args\": {\n                    \"threshold\": 20\n                },\n                \"type\": \"clip_by_iqr\"\n            },\n            {\n                \"args\": {\n                    \"nfft\": 512,\n                    \"f_max\": 6,\n                    \"f_min\": 0,\n                    \"f_sub\": 3,\n                    \"window\": 320,\n                    \"noverlap\": 256\n                },\n                \"type\": \"cal_psd\"\n            }\n        ],\n        \"transformations\": {\n            \"freq_mask\": {},\n            \"time_mask\": {},\n            \"image_translation\": {}\n        },\n        \"batch_normalization\": {}\n    },\n    \"PPG_fft_spec\": {\n        \"add\": False,\n        \"fs_post\": 0.5,\n        \"h5_path\": \"ppg_signal\",\n        \"dimensions\": [\n            32,\n            1\n        ],\n        \"channel_idx\": [\n            0\n        ],\n        \"preprocessing\": [\n            {\n                \"args\": {},\n                \"type\": \"zscore\"\n            },\n            {\n                \"args\": {},\n                \"type\": \"change_PPG_direction\"\n            },\n            {\n                \"args\": {\n                    \"iqr_window\": 301,\n                    \"median_window\": 301\n                },\n                \"type\": \"iqr_normalization_adaptive\"\n            },\n            {\n                \"args\": {\n                    \"threshold\": 20\n                },\n                \"type\": \"clip_by_iqr\"\n            },\n            {\n                \"args\": {\n                    \"nfft\": 512,\n                    \"f_max\": 2.1,\n                    \"f_min\": 0.1,\n                    \"f_sub\": 1,\n                    \"window\": 320,\n                    \"noverlap\": 256\n                },\n                \"type\": \"cal_psd\"\n            }\n        ],\n        \"transformations\": {\n            \"freq_mask\": {},\n            \"time_mask\": {},\n            \"image_translation\": {}\n        },\n        \"batch_normalization\": {}\n    }\n}\ndata_directory = '.'\n\ndataset_params = {\n    \"h5_directory\": data_directory, \n    \"signals_format\": signals_format,\n    \"window\": 30 * 2 ** 10, \n    \"number_of_channels\": len(signals_format), \n    \"events_format\": events_format,\n    \"prediction_resolution\": 30,\n    \"overlap\": 0.25,\n    \"minimum_overlap\": 0.1,\n    \"batch_size\": 2,\n    \"cache_data\": True,\n    \"n_jobs\": 4,\n    \"use_mask\": True,\n    \"load_signal_in_RAM\": True\n}\n\n@dataclass\nclass DSLite:\n    fsTime: float\n    nSpace: int\n    nChannels: int\n    window: int\n    prediction_resolution: int\n\n    def __init__(self, h5_directory, signals_format, window, overlap, batch_size, minimum_overlap, events_format, number_of_channels, prediction_resolution, load_signal_in_RAM, use_mask, cache_data, n_jobs):\n# datasets\n        self.h5_directory = h5_directory\n\n        # signal modalities\n        self.signals_format = signals_format\n        self.window = window\n        self.number_of_channels = number_of_channels\n        self.prediction_resolution = prediction_resolution\n        self.overlap = overlap\n        self.batch_size = batch_size\n        self.predictions_per_window = window // prediction_resolution\n        self.nChannels = sum([sf['dimensions'][-1] for sf in signals_format.values()])\n        self.nSpace = [sf['dimensions'][0] for sf in signals_format.values()][0] # assumes same space resolution\n        self.fsTime = [sf['fs_post'] for sf in signals_format.values()][0] # assumes same temporal resolution\n\n        # events\n        self.events_format = events_format\n        self.minimum_overlap = minimum_overlap\n        self.number_of_events = len(events_format)\n        self.number_of_classes = len(events_format)\n        self.event_probabilities = [event['probability'] for event in events_format]\n        self.event_labels = [event['name'] for event in events_format]\n        assert sum(self.event_probabilities) &lt;= 1\n\n        # training\n        self.load_signal_in_RAM = load_signal_in_RAM\n        self.use_mask = use_mask\n\n\nds_train = DSLite(**dataset_params)\n\n# model creation\nmodel_params = {\n    'input_shape': [int(ds_train.fsTime * ds_train.window), ds_train.nSpace, ds_train.nChannels], \n    'num_classes': len(events),\n    'num_outputs': ds_train.window // ds_train.prediction_resolution,\n    'depth': 9,\n    'init_filter_num': 16,\n    'filter_increment_factor': 2 ** (1 / 3),\n    'max_pool_size': (2, 2),\n    'kernel_size': (16, 3)\n}\n\nresunet = factory_ResUNet(**model_params)\nresunet.summary() # print summary.\n\nWARNING:tensorflow:From c:\\Users\\tavel\\Desktop\\Internship-Arcascope\\2024_internship\\.venv\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\core.py:187: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n\n\n\nModel: \"functional_1\"\n\n\n\n┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n┃ Layer (type)        ┃ Output Shape      ┃    Param # ┃ Connected to      ┃\n┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n│ input_layer         │ (None, 15360, 32, │          0 │ -                 │\n│ (InputLayer)        │ 2)                │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ zero_padding2d      │ (None, 16384, 32, │          0 │ input_layer[0][0] │\n│ (ZeroPadding2D)     │ 2)                │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv2d (Conv2D)     │ (None, 16384, 32, │      1,552 │ zero_padding2d[0… │\n│                     │ 16)               │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ batch_normalization │ (None, 16384, 32, │         64 │ conv2d[0][0]      │\n│ (BatchNormalizatio… │ 16)               │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv2d_1 (Conv2D)   │ (None, 8192, 16,  │      1,300 │ batch_normalizat… │\n│                     │ 20)               │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ batch_normalizatio… │ (None, 8192, 16,  │         80 │ conv2d_1[0][0]    │\n│ (BatchNormalizatio… │ 20)               │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv2d_2 (Conv2D)   │ (None, 8192, 16,  │     19,220 │ batch_normalizat… │\n│                     │ 20)               │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ batch_normalizatio… │ (None, 8192, 16,  │         80 │ conv2d_2[0][0]    │\n│ (BatchNormalizatio… │ 20)               │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv2d_3 (Conv2D)   │ (None, 4096, 8,   │      2,025 │ batch_normalizat… │\n│                     │ 25)               │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ batch_normalizatio… │ (None, 4096, 8,   │        100 │ conv2d_3[0][0]    │\n│ (BatchNormalizatio… │ 25)               │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv2d_4 (Conv2D)   │ (None, 4096, 8,   │     30,025 │ batch_normalizat… │\n│                     │ 25)               │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ batch_normalizatio… │ (None, 4096, 8,   │        100 │ conv2d_4[0][0]    │\n│ (BatchNormalizatio… │ 25)               │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv2d_5 (Conv2D)   │ (None, 2048, 4,   │      3,232 │ batch_normalizat… │\n│                     │ 32)               │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ batch_normalizatio… │ (None, 2048, 4,   │        128 │ conv2d_5[0][0]    │\n│ (BatchNormalizatio… │ 32)               │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv2d_6 (Conv2D)   │ (None, 2048, 4,   │     49,184 │ batch_normalizat… │\n│                     │ 32)               │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ batch_normalizatio… │ (None, 2048, 4,   │        128 │ conv2d_6[0][0]    │\n│ (BatchNormalizatio… │ 32)               │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv2d_7 (Conv2D)   │ (None, 1024, 2,   │      5,160 │ batch_normalizat… │\n│                     │ 40)               │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ batch_normalizatio… │ (None, 1024, 2,   │        160 │ conv2d_7[0][0]    │\n│ (BatchNormalizatio… │ 40)               │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv2d_8 (Conv2D)   │ (None, 1024, 2,   │     51,240 │ batch_normalizat… │\n│                     │ 40)               │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ batch_normalizatio… │ (None, 1024, 2,   │        160 │ conv2d_8[0][0]    │\n│ (BatchNormalizatio… │ 40)               │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv2d_9 (Conv2D)   │ (None, 512, 1,    │      8,050 │ batch_normalizat… │\n│                     │ 50)               │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ batch_normalizatio… │ (None, 512, 1,    │        200 │ conv2d_9[0][0]    │\n│ (BatchNormalizatio… │ 50)               │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv2d_10 (Conv2D)  │ (None, 512, 1,    │     40,050 │ batch_normalizat… │\n│                     │ 50)               │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ batch_normalizatio… │ (None, 512, 1,    │        200 │ conv2d_10[0][0]   │\n│ (BatchNormalizatio… │ 50)               │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv2d_11 (Conv2D)  │ (None, 256, 1,    │      6,464 │ batch_normalizat… │\n│                     │ 64)               │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ batch_normalizatio… │ (None, 256, 1,    │        256 │ conv2d_11[0][0]   │\n│ (BatchNormalizatio… │ 64)               │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv2d_12 (Conv2D)  │ (None, 256, 1,    │     65,600 │ batch_normalizat… │\n│                     │ 64)               │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ batch_normalizatio… │ (None, 256, 1,    │        256 │ conv2d_12[0][0]   │\n│ (BatchNormalizatio… │ 64)               │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv2d_13 (Conv2D)  │ (None, 128, 1,    │     10,320 │ batch_normalizat… │\n│                     │ 80)               │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ batch_normalizatio… │ (None, 128, 1,    │        320 │ conv2d_13[0][0]   │\n│ (BatchNormalizatio… │ 80)               │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv2d_14 (Conv2D)  │ (None, 128, 1,    │    102,480 │ batch_normalizat… │\n│                     │ 80)               │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ batch_normalizatio… │ (None, 128, 1,    │        320 │ conv2d_14[0][0]   │\n│ (BatchNormalizatio… │ 80)               │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv2d_15 (Conv2D)  │ (None, 64, 1,     │     16,261 │ batch_normalizat… │\n│                     │ 101)              │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ batch_normalizatio… │ (None, 64, 1,     │        404 │ conv2d_15[0][0]   │\n│ (BatchNormalizatio… │ 101)              │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv2d_16 (Conv2D)  │ (None, 64, 1,     │    163,317 │ batch_normalizat… │\n│                     │ 101)              │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ batch_normalizatio… │ (None, 64, 1,     │        404 │ conv2d_16[0][0]   │\n│ (BatchNormalizatio… │ 101)              │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv2d_17 (Conv2D)  │ (None, 32, 1,     │     25,984 │ batch_normalizat… │\n│                     │ 128)              │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ batch_normalizatio… │ (None, 32, 1,     │        512 │ conv2d_17[0][0]   │\n│ (BatchNormalizatio… │ 128)              │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv2d_18 (Conv2D)  │ (None, 32, 1,     │    262,272 │ batch_normalizat… │\n│                     │ 128)              │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ batch_normalizatio… │ (None, 32, 1,     │        512 │ conv2d_18[0][0]   │\n│ (BatchNormalizatio… │ 128)              │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv2d_transpose    │ (None, 64, 1,     │     25,957 │ batch_normalizat… │\n│ (Conv2DTranspose)   │ 101)              │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ batch_normalizatio… │ (None, 64, 1,     │        404 │ conv2d_transpose… │\n│ (BatchNormalizatio… │ 101)              │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ concatenate         │ (None, 64, 1,     │          0 │ batch_normalizat… │\n│ (Concatenate)       │ 202)              │            │ batch_normalizat… │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv2d_19 (Conv2D)  │ (None, 64, 1,     │    326,533 │ concatenate[0][0] │\n│                     │ 101)              │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ batch_normalizatio… │ (None, 64, 1,     │        404 │ conv2d_19[0][0]   │\n│ (BatchNormalizatio… │ 101)              │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv2d_transpose_1  │ (None, 128, 1,    │     16,240 │ batch_normalizat… │\n│ (Conv2DTranspose)   │ 80)               │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ batch_normalizatio… │ (None, 128, 1,    │        320 │ conv2d_transpose… │\n│ (BatchNormalizatio… │ 80)               │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ concatenate_1       │ (None, 128, 1,    │          0 │ batch_normalizat… │\n│ (Concatenate)       │ 160)              │            │ batch_normalizat… │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv2d_20 (Conv2D)  │ (None, 128, 1,    │    204,880 │ concatenate_1[0]… │\n│                     │ 80)               │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ batch_normalizatio… │ (None, 128, 1,    │        320 │ conv2d_20[0][0]   │\n│ (BatchNormalizatio… │ 80)               │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv2d_transpose_2  │ (None, 256, 1,    │     10,304 │ batch_normalizat… │\n│ (Conv2DTranspose)   │ 64)               │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ batch_normalizatio… │ (None, 256, 1,    │        256 │ conv2d_transpose… │\n│ (BatchNormalizatio… │ 64)               │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ concatenate_2       │ (None, 256, 1,    │          0 │ batch_normalizat… │\n│ (Concatenate)       │ 128)              │            │ batch_normalizat… │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv2d_21 (Conv2D)  │ (None, 256, 1,    │    131,136 │ concatenate_2[0]… │\n│                     │ 64)               │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ batch_normalizatio… │ (None, 256, 1,    │        256 │ conv2d_21[0][0]   │\n│ (BatchNormalizatio… │ 64)               │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv2d_transpose_3  │ (None, 512, 1,    │      6,450 │ batch_normalizat… │\n│ (Conv2DTranspose)   │ 50)               │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ batch_normalizatio… │ (None, 512, 1,    │        200 │ conv2d_transpose… │\n│ (BatchNormalizatio… │ 50)               │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ concatenate_3       │ (None, 512, 1,    │          0 │ batch_normalizat… │\n│ (Concatenate)       │ 100)              │            │ batch_normalizat… │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv2d_22 (Conv2D)  │ (None, 512, 1,    │     80,050 │ concatenate_3[0]… │\n│                     │ 50)               │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ batch_normalizatio… │ (None, 512, 1,    │        200 │ conv2d_22[0][0]   │\n│ (BatchNormalizatio… │ 50)               │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv2d_transpose_4  │ (None, 1024, 2,   │      8,040 │ batch_normalizat… │\n│ (Conv2DTranspose)   │ 40)               │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ batch_normalizatio… │ (None, 1024, 2,   │        160 │ conv2d_transpose… │\n│ (BatchNormalizatio… │ 40)               │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ concatenate_4       │ (None, 1024, 2,   │          0 │ batch_normalizat… │\n│ (Concatenate)       │ 80)               │            │ batch_normalizat… │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv2d_23 (Conv2D)  │ (None, 1024, 2,   │    102,440 │ concatenate_4[0]… │\n│                     │ 40)               │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ batch_normalizatio… │ (None, 1024, 2,   │        160 │ conv2d_23[0][0]   │\n│ (BatchNormalizatio… │ 40)               │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv2d_transpose_5  │ (None, 2048, 4,   │      5,152 │ batch_normalizat… │\n│ (Conv2DTranspose)   │ 32)               │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ batch_normalizatio… │ (None, 2048, 4,   │        128 │ conv2d_transpose… │\n│ (BatchNormalizatio… │ 32)               │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ concatenate_5       │ (None, 2048, 4,   │          0 │ batch_normalizat… │\n│ (Concatenate)       │ 64)               │            │ batch_normalizat… │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv2d_24 (Conv2D)  │ (None, 2048, 4,   │     98,336 │ concatenate_5[0]… │\n│                     │ 32)               │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ batch_normalizatio… │ (None, 2048, 4,   │        128 │ conv2d_24[0][0]   │\n│ (BatchNormalizatio… │ 32)               │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv2d_transpose_6  │ (None, 4096, 8,   │      3,225 │ batch_normalizat… │\n│ (Conv2DTranspose)   │ 25)               │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ batch_normalizatio… │ (None, 4096, 8,   │        100 │ conv2d_transpose… │\n│ (BatchNormalizatio… │ 25)               │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ concatenate_6       │ (None, 4096, 8,   │          0 │ batch_normalizat… │\n│ (Concatenate)       │ 50)               │            │ batch_normalizat… │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv2d_25 (Conv2D)  │ (None, 4096, 8,   │     60,025 │ concatenate_6[0]… │\n│                     │ 25)               │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ batch_normalizatio… │ (None, 4096, 8,   │        100 │ conv2d_25[0][0]   │\n│ (BatchNormalizatio… │ 25)               │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv2d_transpose_7  │ (None, 8192, 16,  │      2,020 │ batch_normalizat… │\n│ (Conv2DTranspose)   │ 20)               │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ batch_normalizatio… │ (None, 8192, 16,  │         80 │ conv2d_transpose… │\n│ (BatchNormalizatio… │ 20)               │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ concatenate_7       │ (None, 8192, 16,  │          0 │ batch_normalizat… │\n│ (Concatenate)       │ 40)               │            │ batch_normalizat… │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv2d_26 (Conv2D)  │ (None, 8192, 16,  │     38,420 │ concatenate_7[0]… │\n│                     │ 20)               │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ batch_normalizatio… │ (None, 8192, 16,  │         80 │ conv2d_26[0][0]   │\n│ (BatchNormalizatio… │ 20)               │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv2d_transpose_8  │ (None, 16384, 32, │      1,296 │ batch_normalizat… │\n│ (Conv2DTranspose)   │ 16)               │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ batch_normalizatio… │ (None, 16384, 32, │         64 │ conv2d_transpose… │\n│ (BatchNormalizatio… │ 16)               │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ concatenate_8       │ (None, 16384, 32, │          0 │ batch_normalizat… │\n│ (Concatenate)       │ 32)               │            │ batch_normalizat… │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv2d_27 (Conv2D)  │ (None, 16384, 32, │     24,592 │ concatenate_8[0]… │\n│                     │ 16)               │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ batch_normalizatio… │ (None, 16384, 32, │         64 │ conv2d_27[0][0]   │\n│ (BatchNormalizatio… │ 16)               │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ lambda (Lambda)     │ (None, 15360, 32, │          0 │ batch_normalizat… │\n│                     │ 16)               │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ reshape (Reshape)   │ (None, 15360,     │          0 │ lambda[0][0]      │\n│                     │ 512)              │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv1d (Conv1D)     │ (None, 15360, 16) │      8,208 │ reshape[0][0]     │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ average_pooling1d   │ (None, 1024, 16)  │          0 │ conv1d[0][0]      │\n│ (AveragePooling1D)  │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv1d_1 (Conv1D)   │ (None, 1024, 4)   │         68 │ average_pooling1… │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dense (Dense)       │ (None, 1024, 4)   │         20 │ conv1d_1[0][0]    │\n└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n\n\n\n Total params: 2,024,936 (7.72 MB)\n\n\n\n Trainable params: 2,021,032 (7.71 MB)\n\n\n\n Non-trainable params: 3,904 (15.25 KB)",
    "crumbs": [
      "Support code for Mads Olsen's group's model"
    ]
  }
]